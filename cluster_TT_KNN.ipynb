{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12055345,"sourceType":"datasetVersion","datasetId":7587200}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =============================================================================\n# HUMOB / SIGSPATIAL Cup 2025\n# Improved Hybrid TT-KNN + Cluster + TimeProfile pipeline (minimal safe changes)\n# - Minimal, safe changes: incidental pruning, second-best rule, small hourly mixing\n# - Keeps modular layout and memory-safety from original script\n# =============================================================================\n\n# ----------------------------\n# Requirements (run once)\n# ----------------------------\n%pip install -q git+https://github.com/yahoojapan/geobleu.git tqdm scikit-learn matplotlib pandas\n\n# ----------------------------\n# Imports\n# ----------------------------\nimport os, gc, json, time, random\nfrom collections import defaultdict, Counter\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom sklearn.cluster import MiniBatchKMeans\n\n# geobleu\ntry:\n    from geobleu import calc_geobleu_single, calc_geobleu_bulk\nexcept Exception:\n    from geobleu import calc_geobleu_single\n    calc_geobleu_bulk = None\n\n# ----------------------------\n# Global configs\n# ----------------------------\nDATA_DIR = \"/kaggle/input/humob-data/15313913\"   # Kaggle path\nCITIES = [\"B\", \"C\"]                   # run subset as needed\nCOLUMNS = [\"uid\",\"d\",\"t\",\"x\",\"y\"]\nDTYPES = {\"uid\":\"int32\",\"d\":\"int16\",\"t\":\"int16\",\"x\":\"int16\",\"y\":\"int16\"}\n\nTRAIN_DAY_MAX = 60\nTEST_DAY_MIN = 61\nTEST_DAY_MAX = 75\nMASK_VALUE = 999\nDELTA = 30   # minutes per segment (kept as in original)\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED); random.seed(RANDOM_SEED)\n\nTARGET_RANGES = {\"A\":(147001,150000),\"B\":(27001,30000),\"C\":(22001,25000),\"D\":(17001,20000)}\n\n# grid candidates (unchanged)\nGRID_CANDIDATES = {\n    \"A\":{\"topL\":[200,300],\"ncl\":[200,300]},\n    \"B\":{\"topL\":[150,200],\"ncl\":[120,150,200]},\n    \"C\":{\"topL\":[150,200,250],\"ncl\":[100,150]},\n    \"D\":{\"topL\":[100,200,300],\"ncl\":[100,150,200]},\n}\n\n# per-city memory-performance tuning + new small params:\n# INCIDENTAL_THRESHOLD: minimum count for a cell to be considered in top_cells (prune global noise)\n# P_MIX: probability of mixing prediction with hourly profile at each timestep (reduces drift)\nCITY_CONFIG = {\n    \"A\": {\"TOP_L_MAX\":150, \"SAMPLE_FRAC_FOR_GRID\":0.01, \"MIN_TRANSITION_COUNT\":2, \"CLUSTER_BATCH\":1024, \"INCIDENTAL_THRESHOLD\":3, \"P_MIX\":0.03},\n    \"B\": {\"TOP_L_MAX\":250, \"SAMPLE_FRAC_FOR_GRID\":0.03, \"MIN_TRANSITION_COUNT\":1, \"CLUSTER_BATCH\":2048, \"INCIDENTAL_THRESHOLD\":2, \"P_MIX\":0.04},\n    \"C\": {\"TOP_L_MAX\":250, \"SAMPLE_FRAC_FOR_GRID\":0.03, \"MIN_TRANSITION_COUNT\":1, \"CLUSTER_BATCH\":2048, \"INCIDENTAL_THRESHOLD\":2, \"P_MIX\":0.05},\n    \"D\": {\"TOP_L_MAX\":300, \"SAMPLE_FRAC_FOR_GRID\":0.04, \"MIN_TRANSITION_COUNT\":1, \"CLUSTER_BATCH\":2048, \"INCIDENTAL_THRESHOLD\":1, \"P_MIX\":0.07},\n}\n\nMAX_USERS_FOR_GRID = 1500\nPRUNE_CLUSTERTT_IF_SMALL = 500\nCLUSTER_PREF_RATIO = 0.6\nN_JOBS = 1  # keep 1 on Kaggle free to avoid fork/pickle overhead\nOUT_DIR = \"./results\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nMAKE_SUBMISSION = True\nRUN_FULL_FINAL = True\nUSE_GEOLEU_BULK = True\n\n# ----------------------------\n# Helpers\n# ----------------------------\ndef load_city_df(city):\n    path = os.path.join(DATA_DIR, f\"city_{city}_challengedata.csv\")\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Missing data: {path}\")\n    return pd.read_csv(path, usecols=COLUMNS, dtype=DTYPES)\n\ndef to_flat_segment(d,t,delta=DELTA):\n    return int(d * ((24*60)//delta) + (t*60)//delta)\n\ndef chebyshev(a,b):\n    return max(abs(a[0]-b[0]), abs(a[1]-b[1]))\n\ndef prune_counter_dict(counter_obj, min_count=1):\n    return {k:int(v) for k,v in counter_obj.items() if int(v) >= min_count}\n\n# ----------------------------\n# Held-user sampling (user-level)\n# ----------------------------\ndef sample_heldout_users(df, frac, seed=RANDOM_SEED):\n    mask = (df[\"d\"].between(TEST_DAY_MIN, TEST_DAY_MAX)) & (df[\"x\"] != MASK_VALUE)\n    users = df.loc[mask, \"uid\"].unique().tolist()\n    if not users:\n        return []\n    rnd = random.Random(seed)\n    rnd.shuffle(users)\n    k = max(1, int(len(users) * frac))\n    return [int(u) for u in users[:k]]\n\n# ----------------------------\n# Build artifacts (user-level TT, hourly profiles, topK)\n# - CHANGED: incidental-location pruning for top_cells (min_global_cell_count)\n# ----------------------------\ndef build_base_artifacts(df, top_l_max, held_uids=None, min_trans=1, mask_targets=False, city=None, min_global_cell_count=1):\n    \"\"\"\n    Build artifacts from allowed unmasked rows.\n    If held_uids provided, exclude their days 61-75 from allowed training (simulate).\n    If mask_targets True and city provided, exclude official target users' days > TRAIN_DAY_MAX.\n    min_global_cell_count: prune global cells with fewer than this many occurrences before top-L selection.\n    \"\"\"\n    if held_uids is None: held_uids = []\n\n    allowed_mask = (df[\"x\"] != MASK_VALUE)\n    if held_uids:\n        held_mask = df[\"uid\"].isin(held_uids) & df[\"d\"].between(TEST_DAY_MIN, TEST_DAY_MAX)\n        allowed_mask = allowed_mask & ~held_mask\n    if mask_targets and city is not None:\n        lo,hi = TARGET_RANGES[city]\n        target_mask = df[\"uid\"].between(lo,hi) & (df[\"d\"] > TRAIN_DAY_MAX)\n        allowed_mask = allowed_mask & ~target_mask\n\n    allowed_train = df.loc[allowed_mask & (df[\"d\"] <= TRAIN_DAY_MAX)].copy()\n    # top cells with incidental pruning\n    cell_counts = Counter(zip(allowed_train[\"x\"], allowed_train[\"y\"]))\n    # prune globally incidental locations first\n    filtered_cells = [(loc,cnt) for loc,cnt in cell_counts.items() if cnt >= min_global_cell_count]\n    filtered_cells.sort(key=lambda kv:kv[1], reverse=True)\n    top_cells_full = [loc for loc,_ in filtered_cells[:top_l_max]]\n\n    # city hourly profile\n    hour_counts = defaultdict(Counter)\n    for t,x,y in zip(allowed_train[\"t\"], allowed_train[\"x\"], allowed_train[\"y\"]):\n        hour_counts[int(t)][(int(x),int(y))] += 1\n    city_profile = {h: max(c.items(), key=lambda kv:kv[1])[0] for h,c in hour_counts.items() if c}\n\n    # per-user artifacts (prune low-count transitions)\n    userTT = {}\n    user_hour = {}\n    user_topK = {}\n    last_known = {}\n\n    for uid, g in tqdm(allowed_train.groupby(\"uid\", sort=False), desc=\"build user artifacts\"):\n        uid0 = int(uid)\n        seq = [(to_flat_segment(int(d),int(t)), (int(x),int(y))) for d,t,x,y in zip(g[\"d\"],g[\"t\"],g[\"x\"],g[\"y\"])]\n        seq.sort()\n        tt = defaultdict(Counter)\n        for (s1,l1),(s2,l2) in zip(seq, seq[1:]):\n            if 0 < s2 - s1 <= 3:\n                tt[(s1,l1)][l2] += 1\n        # prune transitions\n        tt_pruned = {}\n        for k,v in tt.items():\n            dct = prune_counter_dict(v, min_trans)\n            if dct:\n                tt_pruned[k] = dct\n        if tt_pruned:\n            userTT[uid0] = tt_pruned\n\n        hc = defaultdict(Counter)\n        for t,x,y in zip(g[\"t\"], g[\"x\"], g[\"y\"]):\n            hc[int(t)][(int(x),int(y))] += 1\n        if hc:\n            user_hour[uid0] = {h:max(c.items(), key=lambda kv:kv[1])[0] for h,c in hc.items()}\n\n        locs = Counter(zip(g[\"x\"], g[\"y\"]))\n        user_topK[uid0] = [loc for loc,_ in locs.most_common(3)]\n\n        last_row = g.loc[g[\"d\"] <= TRAIN_DAY_MAX].sort_values([\"d\",\"t\"]).tail(1)\n        last_known[uid0] = None if last_row.empty else (int(last_row[\"x\"].iloc[0]), int(last_row[\"y\"].iloc[0]))\n\n    # test rows (days 61-75)\n    df_test = df.loc[df[\"d\"].between(TEST_DAY_MIN, TEST_DAY_MAX)].sort_values([\"uid\",\"d\",\"t\"])\n    user_test_rows = {}\n    for uid, g in df_test.groupby(\"uid\", sort=False):\n        user_test_rows[int(uid)] = [(int(idx), int(r.d), int(r.t), int(r.x), int(r.y)) for idx, r in g.iterrows()]\n\n    artifacts = {\n        \"top_cells_full\": top_cells_full,\n        \"city_profile\": city_profile,\n        \"userTT\": userTT,\n        \"user_hour\": user_hour,\n        \"user_topK\": user_topK,\n        \"last_known\": last_known,\n        \"user_test_rows\": user_test_rows\n    }\n    return artifacts\n\n# ----------------------------\n# Precompute per-user vectors for clustering\n# ----------------------------\ndef precompute_user_vectors(df, artifacts, top_l_max):\n    top_cells = artifacts[\"top_cells_full\"]\n    top_index = {loc:i for i,loc in enumerate(top_cells)}\n    mask = (df[\"d\"] <= TRAIN_DAY_MAX) & (df[\"x\"] != MASK_VALUE)\n    user_vecs = {}\n    for uid, g in tqdm(df.loc[mask].groupby(\"uid\", sort=False), desc=\"precompute vecs\"):\n        uid0 = int(uid)\n        vec = np.zeros(len(top_cells), dtype=np.float32)\n        for x,y in zip(g[\"x\"], g[\"y\"]):\n            k = (int(x), int(y))\n            if k in top_index:\n                vec[top_index[k]] += 1.0\n        if vec.sum() > 0:\n            vec /= (np.linalg.norm(vec) + 1e-9)\n        user_vecs[uid0] = vec\n    return user_vecs\n\n# ----------------------------\n# Clustering -> uid -> cluster label\n# ----------------------------\ndef cluster_for_params(user_vecs, topL, n_clusters, batch_size=1024):\n    uids = list(user_vecs.keys())\n    if not uids:\n        return {}\n    X = np.vstack([user_vecs[uid][:topL] for uid in uids])\n    n_clusters = max(1, min(n_clusters, max(1, len(uids)//5)))\n    # safe n_init integer for sklearn versions\n    km = MiniBatchKMeans(n_clusters=n_clusters, random_state=RANDOM_SEED, batch_size=batch_size, n_init=10)\n    labels = km.fit_predict(X)\n    return dict(zip(uids, labels))\n\n# ----------------------------\n# Build cluster-level auxiliaries (hour, TT)\n# ----------------------------\ndef build_cluster_aux(df_subset, uid_to_cluster, min_trans=1):\n    \"\"\"\n    df_subset: dataframe used for cluster stats (should be allowed training rows)\n    \"\"\"\n    mask = (df_subset[\"d\"] <= TRAIN_DAY_MAX) & (df_subset[\"x\"] != MASK_VALUE)\n    train = df_subset.loc[mask]\n    cl_hour_counts = defaultdict(lambda: defaultdict(Counter))\n    cl_tt_counts = defaultdict(lambda: defaultdict(Counter))\n\n    for uid, g in train.groupby(\"uid\", sort=False):\n        uid0 = int(uid); cl = uid_to_cluster.get(uid0)\n        if cl is None: continue\n        for t,x,y in zip(g[\"t\"], g[\"x\"], g[\"y\"]):\n            cl_hour_counts[cl][int(t)][(int(x),int(y))] += 1\n        seq = [(to_flat_segment(int(d),int(t)), (int(x),int(y))) for d,t,x,y in zip(g[\"d\"],g[\"t\"],g[\"x\"],g[\"y\"])]\n        seq.sort()\n        for (s1,l1),(s2,l2) in zip(seq, seq[1:]):\n            if 0 < s2 - s1 <= 3:\n                cl_tt_counts[cl][(s1,l1)][l2] += 1\n\n    cluster_hour = {}\n    cluster_hour_top_counts = {}\n    cluster_hour_total = {}\n    for cl, hr in cl_hour_counts.items():\n        cluster_hour[cl] = {}\n        cluster_hour_top_counts[cl] = {}\n        cluster_hour_total[cl] = {}\n        for h, counter in hr.items():\n            most_common = counter.most_common(1)\n            if most_common:\n                top_loc, top_count = most_common[0]\n                total = sum(counter.values())\n                cluster_hour[cl][h] = top_loc\n                cluster_hour_top_counts[cl][h] = int(top_count)\n                cluster_hour_total[cl][h] = int(total)\n\n    clusterTT = {}\n    for cl, tt in cl_tt_counts.items():\n        pruned = {}\n        for k, v in tt.items():\n            dct = prune_counter_dict(v, min_trans)\n            if dct:\n                pruned[k] = dct\n        if pruned:\n            clusterTT[cl] = pruned\n\n    total_trans = sum(sum(sum(v.values()) for v in tt.values()) for tt in cl_tt_counts.values())\n    return cluster_hour, cluster_hour_top_counts, cluster_hour_total, clusterTT, total_trans\n\n# ----------------------------\n# Predict user sequentially (used in eval and final)\n# - CHANGED: second-best-if-top-is-current; small probabilistic hourly mixing (P_MIX)\n# ----------------------------\ndef predict_user_seq(uid, rows, artifacts, uid_to_cluster, cluster_hour, cluster_hour_top, cluster_hour_total, clusterTT, held_uids_set, use_cluster_tt=True, cluster_pref_ratio=CLUSTER_PREF_RATIO, p_mix=0.05):\n    preds = []; gts = []\n    curr = artifacts.get(\"last_known\", {}).get(uid, (0,0))\n    stay_idx = 0\n    for idx, d, t, x, y in rows:\n        seg = to_flat_segment(d,t); hour = int(t)\n        if uid in held_uids_set:\n            # mixing: sometimes pick hourly-based prediction directly to reduce drift\n            if random.random() < p_mix:\n                # pick best hourly from user or cluster or city\n                cl = uid_to_cluster.get(uid, None)\n                picked = None\n                # cluster strong preference\n                if cl is not None and cl in cluster_hour_top and hour in cluster_hour_top[cl]:\n                    top_count = cluster_hour_top[cl][hour]\n                    total = cluster_hour_total[cl].get(hour,1)\n                    if total>0 and (top_count/float(total) >= cluster_pref_ratio):\n                        picked = cluster_hour[cl].get(hour, None)\n                if picked is None and uid in artifacts[\"user_hour\"] and hour in artifacts[\"user_hour\"][uid]:\n                    picked = artifacts[\"user_hour\"][uid][hour]\n                if picked is None and cl is not None and cl in cluster_hour and hour in cluster_hour[cl]:\n                    picked = cluster_hour[cl][hour]\n                if picked is None and uid in artifacts[\"user_topK\"] and artifacts[\"user_topK\"][uid]:\n                    picked = artifacts[\"user_topK\"][uid][stay_idx % len(artifacts[\"user_topK\"][uid])]\n                    stay_idx += 1\n                if picked is None:\n                    picked = artifacts[\"city_profile\"].get(hour, curr)\n                preds.append((int(d), int(t), int(picked[0]), int(picked[1])))\n                gts.append((int(d), int(t), int(x), int(y)))\n                curr = picked\n                continue\n\n            cand = Counter()\n            # userTT\n            if uid in artifacts[\"userTT\"]:\n                cand.update(artifacts[\"userTT\"][uid].get((seg,curr), {}))\n            # clusterTT fallback\n            cl = uid_to_cluster.get(uid, None)\n            if use_cluster_tt and cl is not None:\n                cand.update(clusterTT.get(cl, {}).get((seg,curr), {}))\n            if cand:\n                # pick the most frequent, but if top == curr, try second-best\n                sorted_cands = sorted(cand.items(), key=lambda kv: (-kv[1], chebyshev(kv[0], curr)))\n                top_loc, top_count = sorted_cands[0]\n                pred = top_loc\n                # if the top predicted location equals current location, try second entry\n                if top_loc == curr and len(sorted_cands) > 1:\n                    # only switch to second best if its count is reasonably high (>= 1 or >= 0.5*top_count)\n                    second_loc, second_count = sorted_cands[1]\n                    if (second_count >= 1) and (second_count >= 0.5 * top_count or top_count == 1):\n                        pred = second_loc\n                    else:\n                        # keep top_loc (it's legitimately dominant)\n                        pred = top_loc\n                preds.append((int(d), int(t), int(pred[0]), int(pred[1])))\n            else:\n                # cluster hourly strong preference or else user/cluster/hour/topK/city fallback (same as before)\n                cl_prefed = False\n                if cl is not None and cl in cluster_hour_top and hour in cluster_hour_top[cl]:\n                    top_count = cluster_hour_top[cl][hour]\n                    total = cluster_hour_total[cl].get(hour,1)\n                    if total>0 and (top_count/float(total) >= cluster_pref_ratio):\n                        cl_prefed = True\n                if cl_prefed and cl is not None and cl in cluster_hour and hour in cluster_hour[cl]:\n                    pred = cluster_hour[cl][hour]\n                elif uid in artifacts[\"user_hour\"] and hour in artifacts[\"user_hour\"][uid]:\n                    pred = artifacts[\"user_hour\"][uid][hour]\n                elif cl is not None and cl in cluster_hour and hour in cluster_hour[cl]:\n                    pred = cluster_hour[cl][hour]\n                elif uid in artifacts[\"user_topK\"] and artifacts[\"user_topK\"][uid]:\n                    pred = artifacts[\"user_topK\"][uid][stay_idx % len(artifacts[\"user_topK\"][uid])]\n                    stay_idx += 1\n                else:\n                    pred = artifacts[\"city_profile\"].get(hour, curr)\n                preds.append((int(d), int(t), int(pred[0]), int(pred[1])))\n            gts.append((int(d), int(t), int(x), int(y)))\n            curr = preds[-1][2:]  # update current to last predicted\n        else:\n            if int(x) != MASK_VALUE:\n                curr = (int(x), int(y))\n    return preds, gts\n\n# ----------------------------\n# Evaluate (sequential)\n# ----------------------------\ndef evaluate_with_cluster(df, artifacts, uid_to_cluster, cluster_hour, cluster_hour_top, cluster_hour_total, clusterTT, held_uids, users_to_eval, use_cluster_tt=True, p_mix=0.05):\n    tasks = users_to_eval\n    preds_map = {}; gts_map = {}\n    held_set = set(held_uids)\n    for uid in tqdm(tasks, desc=\"predict users (seq)\"):\n        rows = artifacts[\"user_test_rows\"].get(uid, [])\n        if not rows: continue\n        preds, gts = predict_user_seq(uid, rows, artifacts, uid_to_cluster, cluster_hour, cluster_hour_top, cluster_hour_total, clusterTT, held_set, use_cluster_tt, CLUSTER_PREF_RATIO, p_mix=p_mix)\n        if preds:\n            preds_map[uid] = preds; gts_map[uid] = gts\n\n    # scoring: try bulk then fallback\n    if calc_geobleu_bulk is not None and USE_GEOLEU_BULK:\n        gen_bulk = []\n        ref_bulk = []\n        for uid, seq in preds_map.items():\n            for d,t,x,y in seq:\n                gen_bulk.append((int(uid), int(d), int(t), int(x), int(y)))\n        for uid, seq in gts_map.items():\n            for d,t,x,y in seq:\n                ref_bulk.append((int(uid), int(d), int(t), int(x), int(y)))\n        try:\n            score = float(calc_geobleu_bulk(gen_bulk, ref_bulk, processes=1))\n            info = {\"num_users\": len(preds_map), \"num_preds\": sum(len(v) for v in preds_map.values())}\n            return score, info, preds_map\n        except Exception:\n            pass\n\n    # fallback per-user\n    scores = []\n    for uid in preds_map.keys():\n        try:\n            p = [pt[2:] for pt in preds_map[uid]]\n            g = [pt[2:] for pt in gts_map[uid]]\n            scores.append(calc_geobleu_single(p, g))\n        except Exception:\n            scores.append(0.0)\n    mean_score = float(np.mean(scores)) if scores else 0.0\n    info = {\"num_users\": len(preds_map), \"num_preds\": sum(len(v) for v in preds_map.values())}\n    return mean_score, info, preds_map\n\n# ----------------------------\n# Sampled grid search\n# - CHANGED: pass incidental threshold and P_MIX from city cfg (P_MIX used at evaluate time)\n# ----------------------------\ndef sampled_grid_search(df, city, grid_candidates, cfg):\n    print(\"[grid] sampling held users ...\")\n    held_uids = sample_heldout_users(df, frac=cfg[\"SAMPLE_FRAC_FOR_GRID\"])\n    print(f\"[grid] sampled held users: {len(held_uids)}\")\n    artifacts = build_base_artifacts(df, top_l_max=cfg[\"TOP_L_MAX\"], held_uids=held_uids, min_trans=cfg[\"MIN_TRANSITION_COUNT\"], min_global_cell_count=cfg.get(\"INCIDENTAL_THRESHOLD\",1))\n    user_vecs = precompute_user_vectors(df, artifacts, cfg[\"TOP_L_MAX\"])\n    users_for_grid = held_uids[:MAX_USERS_FOR_GRID]\n    grid_results = []\n\n    for topL in grid_candidates[\"topL\"]:\n        for ncl in grid_candidates[\"ncl\"]:\n            print(f\"[grid] try topL={topL}, n_clusters={ncl}\")\n            uid_to_cluster = cluster_for_params(user_vecs, topL, ncl, batch_size=cfg.get(\"CLUSTER_BATCH\", 1024))\n            # Build cluster aux from allowed training (we can pass df masked/allowed rows or artifacts)\n            # For speed/memory use same allowed training used in artifacts: rebuild df_subset quickly:\n            allowed_mask = (df[\"x\"] != MASK_VALUE)\n            if held_uids:\n                held_mask = df[\"uid\"].isin(held_uids) & df[\"d\"].between(TEST_DAY_MIN, TEST_DAY_MAX)\n                allowed_mask = allowed_mask & ~held_mask\n            df_subset = df.loc[allowed_mask & (df[\"d\"] <= TRAIN_DAY_MAX)].copy()\n            cluster_hour, cluster_hour_top, cluster_hour_total, clusterTT, total_trans = build_cluster_aux(df_subset, uid_to_cluster, min_trans=cfg[\"MIN_TRANSITION_COUNT\"])\n            use_cluster_tt = total_trans >= PRUNE_CLUSTERTT_IF_SMALL\n            if not use_cluster_tt: clusterTT = {}\n            score, info, _ = evaluate_with_cluster(df, artifacts, uid_to_cluster, cluster_hour, cluster_hour_top, cluster_hour_total, clusterTT, held_uids, users_for_grid, use_cluster_tt, p_mix=cfg.get(\"P_MIX\", 0.05))\n            print(f\" -> score={score:.5f}, users_eval={info['num_users']}, preds={info['num_preds']}\")\n            grid_results.append({\"topL\": topL, \"n_clusters\": ncl, \"score\": score, \"use_cluster_tt\": use_cluster_tt})\n            with open(os.path.join(OUT_DIR, f\"{city}_grid_progress.json\"), \"w\") as f:\n                json.dump(grid_results, f, indent=2)\n\n    best = max(grid_results, key=lambda r:r[\"score\"]) if grid_results else None\n    print(\"[grid] best:\", best)\n    return best, artifacts, held_uids, grid_results\n\n# ----------------------------\n# Final training & submission\n# - CHANGED: artifacts built with incidental pruning param; final eval uses p_mix\n# ----------------------------\ndef final_train_and_generate_submission(df, city, artifacts_from_grid, held_uids, topL, n_clusters, cfg, make_submission=MAKE_SUBMISSION):\n    print(\"[final] building final artifacts using allowed unmasked rows (may take time)...\")\n    artifacts_final = build_base_artifacts(df, top_l_max=cfg[\"TOP_L_MAX\"], held_uids=None, min_trans=cfg[\"MIN_TRANSITION_COUNT\"], mask_targets=True, city=city, min_global_cell_count=cfg.get(\"INCIDENTAL_THRESHOLD\",1))\n    # build user vectors on final_train\n    user_vecs_final = precompute_user_vectors(df, artifacts_final, cfg[\"TOP_L_MAX\"])\n    uid_to_cluster_final = cluster_for_params(user_vecs_final, topL, n_clusters, batch_size=cfg.get(\"CLUSTER_BATCH\",1024))\n    # build cluster aux using final allowed train (we can re-create df_subset for allowed rows)\n    lo,hi = TARGET_RANGES[city]\n    is_target = df[\"uid\"].between(lo,hi)\n    allowed_mask = (df[\"x\"] != MASK_VALUE) & ~(is_target & (df[\"d\"] > TRAIN_DAY_MAX))\n    df_allowed = df.loc[allowed_mask & (df[\"d\"] <= TRAIN_DAY_MAX)].copy()\n    cluster_hour_final, cluster_hour_top_final, cluster_hour_total_final, clusterTT_final, total_trans_final = build_cluster_aux(df_allowed, uid_to_cluster_final, min_trans=cfg[\"MIN_TRANSITION_COUNT\"])\n    use_cluster_tt_final = total_trans_final >= PRUNE_CLUSTERTT_IF_SMALL\n    if not use_cluster_tt_final: clusterTT_final = {}\n\n    print(\"[final] local evaluation on held users...\")\n    score_local, info_local, preds_local = evaluate_with_cluster(df, artifacts_final, uid_to_cluster_final, cluster_hour_final, cluster_hour_top_final, cluster_hour_total_final, clusterTT_final, held_uids, held_uids, use_cluster_tt_final, p_mix=cfg.get(\"P_MIX\",0.05))\n    print(f\"[final] local GeoBLEU (final-trained): {score_local:.5f} info: {info_local}\")\n    with open(os.path.join(OUT_DIR, f\"{city}_final_info.json\"), \"w\") as f:\n        json.dump({\"score_local\":score_local, \"info\":info_local, \"topL\":topL, \"n_clusters\":n_clusters}, f, indent=2)\n\n    submission_path = None\n    if make_submission:\n        print(\"[final] generating official predictions for masked cells...\")\n        lo,hi = TARGET_RANGES[city]\n        df_masked = df.loc[df[\"uid\"].between(lo,hi) & df[\"d\"].between(TEST_DAY_MIN, TEST_DAY_MAX) & (df[\"x\"] == MASK_VALUE)].sort_values([\"uid\",\"d\",\"t\"])\n        rows_out = []\n        held_set = set(held_uids)\n        grouped = df_masked.groupby(\"uid\", sort=False)\n        for uid, g in tqdm(grouped, desc=\"predict submission users\"):\n            # initialize curr from last_known or topK or city_profile\n            curr = artifacts_final[\"last_known\"].get(int(uid), None)\n            if curr is None:\n                if int(uid) in artifacts_final[\"user_topK\"] and artifacts_final[\"user_topK\"].get(int(uid)):\n                    curr = artifacts_final[\"user_topK\"][int(uid)][0]\n                else:\n                    # fallback hour from first row\n                    first_row = g.iloc[0]\n                    curr = artifacts_final[\"city_profile\"].get(int(first_row[\"t\"]), (0,0))\n            stay_idx = 0\n            # iterate rows using iterrows (safe unpack)\n            for _, row in g.iterrows():\n                d = int(row[\"d\"]); t = int(row[\"t\"])\n                seg = to_flat_segment(d,t); hour = int(t)\n                # mixing: small chance to use hourly profile (same behavior as eval)\n                if random.random() < cfg.get(\"P_MIX\", 0.05):\n                    cl = uid_to_cluster_final.get(int(uid), None)\n                    chosen = None\n                    if cl is not None and cl in cluster_hour_top_final and hour in cluster_hour_top_final[cl]:\n                        top_count = cluster_hour_top_final[cl][hour]\n                        total = cluster_hour_total_final[cl].get(hour,1)\n                        if total>0 and (top_count/float(total) >= CLUSTER_PREF_RATIO):\n                            chosen = cluster_hour_final[cl].get(hour, None)\n                    if chosen is None and int(uid) in artifacts_final[\"user_hour\"] and hour in artifacts_final[\"user_hour\"][int(uid)]:\n                        chosen = artifacts_final[\"user_hour\"][int(uid)][hour]\n                    if chosen is None and cl is not None and cl in cluster_hour_final and hour in cluster_hour_final[cl]:\n                        chosen = cluster_hour_final[cl][hour]\n                    if chosen is None and int(uid) in artifacts_final[\"user_topK\"] and artifacts_final[\"user_topK\"][int(uid)]:\n                        chosen = artifacts_final[\"user_topK\"][int(uid)][stay_idx % len(artifacts_final[\"user_topK\"][int(uid)])]\n                        stay_idx += 1\n                    if chosen is None:\n                        chosen = artifacts_final[\"city_profile\"].get(hour, curr)\n                    pred = chosen\n                else:\n                    cand = Counter()\n                    uid0 = int(uid)\n                    if uid0 in artifacts_final[\"userTT\"]:\n                        cand.update(artifacts_final[\"userTT\"][uid0].get((seg,curr), {}))\n                    cl = uid_to_cluster_final.get(uid0, None)\n                    if use_cluster_tt_final and cl is not None:\n                        cand.update(clusterTT_final.get(cl, {}).get((seg,curr), {}))\n                    if cand:\n                        sorted_cands = sorted(cand.items(), key=lambda kv: (-kv[1], chebyshev(kv[0], curr)))\n                        top_loc, top_count = sorted_cands[0]\n                        pred = top_loc\n                        if top_loc == curr and len(sorted_cands) > 1:\n                            second_loc, second_count = sorted_cands[1]\n                            if (second_count >= 1) and (second_count >= 0.5 * top_count or top_count == 1):\n                                pred = second_loc\n                    else:\n                        cl_prefed = False\n                        if cl is not None and cl in cluster_hour_top_final and hour in cluster_hour_top_final[cl]:\n                            top_count = cluster_hour_top_final[cl][hour]\n                            total = cluster_hour_total_final[cl].get(hour,1)\n                            if total>0 and (top_count/float(total) >= CLUSTER_PREF_RATIO):\n                                cl_prefed = True\n                        if cl_prefed and cl is not None and cl in cluster_hour_final and hour in cluster_hour_final[cl]:\n                            pred = cluster_hour_final[cl][hour]\n                        elif uid0 in artifacts_final[\"user_hour\"] and hour in artifacts_final[\"user_hour\"][uid0]:\n                            pred = artifacts_final[\"user_hour\"][uid0][hour]\n                        elif cl is not None and cl in cluster_hour_final and hour in cluster_hour_final[cl]:\n                            pred = cluster_hour_final[cl][hour]\n                        elif uid0 in artifacts_final[\"user_topK\"] and artifacts_final[\"user_topK\"][uid0]:\n                            pred = artifacts_final[\"user_topK\"][uid0][stay_idx % len(artifacts_final[\"user_topK\"][uid0])]\n                            stay_idx += 1\n                        else:\n                            pred = artifacts_final[\"city_profile\"].get(hour, curr)\n                rows_out.append({\"uid\": int(uid), \"d\": d, \"t\": t, \"x\": int(pred[0]), \"y\": int(pred[1])})\n                curr = pred\n        out_df = pd.DataFrame(rows_out)\n        submission_path = os.path.join(OUT_DIR, f\"{city}_submission.csv\")\n        out_df.to_csv(submission_path, index=False)\n        print(\"[final] wrote submission to\", submission_path)\n    return score_local, info_local, submission_path\n\n# ----------------------------\n# Smoke test: small run on fraction of City D (commented by default)\n# ----------------------------\n# To run quick smoke test before heavy jobs, uncomment and run this block.\n# It uses a tiny fraction of city D and does grid+final quick run.\n#\n# city = \"D\"\n# df = load_city_df(city)\n# df_small = df.sample(frac=0.01, random_state=RANDOM_SEED)  # 1% for smoke\n# cfg = CITY_CONFIG[city]\n# best, artifacts, held_uids, grid_results = sampled_grid_search(df_small, city, GRID_CANDIDATES[city], cfg)\n# print(\"SMOKE best:\", best)\n# final_train_and_generate_submission(df_small, city, artifacts, held_uids, best[\"topL\"], best[\"n_clusters\"], cfg, make_submission=False)\n# raise SystemExit(\"Smoke test done\")\n\n# ----------------------------\n# Main driver (per-city)\n# ----------------------------\nif __name__ == \"__main__\":\n    total_start = time.time()\n    for city in CITIES:\n        print(\"\\n\\n====================\")\n        print(\"RUNNING CITY:\", city)\n        print(\"====================\\n\")\n        cfg = CITY_CONFIG[city]\n        df = load_city_df(city)\n\n        # GRID SEARCH (sampled held users)\n        best, artifacts, held_uids, grid_results = sampled_grid_search(df, city, GRID_CANDIDATES[city], cfg)\n        print(\"Grid best:\", best)\n\n        # free memory used by grid artifacts/vectors before final\n        del artifacts\n        gc.collect()\n\n        # FINAL (train on allowed rows & generate submission)\n        if RUN_FULL_FINAL and best is not None:\n            score_local, info_local, submission_path = final_train_and_generate_submission(df, city, None, held_uids, best[\"topL\"], best[\"n_clusters\"], cfg, make_submission=MAKE_SUBMISSION)\n            print(f\"City {city} final local GeoBLEU: {score_local:.5f}, info: {info_local}, submission: {submission_path}\")\n        else:\n            print(f\"City {city} - grid only (no final).\")\n\n        # cleanup per city\n        del df, held_uids, grid_results\n        gc.collect()\n\n    print(\"Total elapsed (s):\", int(time.time() - total_start))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T13:43:54.794714Z","iopub.execute_input":"2025-09-09T13:43:54.795053Z","iopub.status.idle":"2025-09-09T13:46:42.104646Z","shell.execute_reply.started":"2025-09-09T13:43:54.795023Z","shell.execute_reply":"2025-09-09T13:46:42.103113Z"}},"outputs":[],"execution_count":null}]}