{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf3e5174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T14:37:33.537602Z",
     "iopub.status.busy": "2025-08-26T14:37:33.537269Z",
     "iopub.status.idle": "2025-08-26T14:37:33.541495Z",
     "shell.execute_reply": "2025-08-26T14:37:33.540853Z"
    },
    "papermill": {
     "duration": 0.009756,
     "end_time": "2025-08-26T14:37:33.542728",
     "exception": false,
     "start_time": "2025-08-26T14:37:33.532972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import math\n",
    "\n",
    "# # Placeholder for GeoBLEU and DTW functions\n",
    "# # In a real-world scenario, you would install the library and import them.\n",
    "# %pip install git+https://github.com/yahoojapan/geobleu.git\n",
    "# from geobleu import calc_geobleu_single\n",
    "# from geobleu import calc_dtw_single\n",
    "# # The functions are designed to handle (d,t,x,y) tuples for a single user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "302d3a3c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-26T14:37:33.548957Z",
     "iopub.status.busy": "2025-08-26T14:37:33.548734Z",
     "iopub.status.idle": "2025-08-26T14:37:33.555834Z",
     "shell.execute_reply": "2025-08-26T14:37:33.555264Z"
    },
    "papermill": {
     "duration": 0.011723,
     "end_time": "2025-08-26T14:37:33.557088",
     "exception": false,
     "start_time": "2025-08-26T14:37:33.545365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# # def calc_geobleu_single(generated, reference):\n",
    "# #     # This is a dummy implementation based on the repo's logic.\n",
    "# #     # It would be replaced by the actual function from the installed library.\n",
    "# #     gen_xy = np.array([(p[2], p[3]) for p in generated])\n",
    "# #     ref_xy = np.array([(p[2], p[3]) for p in reference])\n",
    "    \n",
    "# #     if len(gen_xy) == 0 or len(ref_xy) == 0:\n",
    "# #         return 0.0\n",
    "    \n",
    "# #     # Simulate a basic score calculation for GeoBLEU\n",
    "# #     distance = np.mean(np.sqrt(np.sum((gen_xy - ref_xy[:len(gen_xy)])**2, axis=1)))\n",
    "# #     score = max(0, 1.0 - distance / 100)\n",
    "# #     return score\n",
    "\n",
    "# # def calc_dtw_single(generated, reference):\n",
    "# #     # This is a dummy implementation based on the repo's logic.\n",
    "# #     # It would be replaced by the actual function from the installed library.\n",
    "# #     gen_xy = np.array([(p[2], p[3]) for p in generated])\n",
    "# #     ref_xy = np.array([(p[2], p[3]) for p in reference])\n",
    "    \n",
    "# #     # Simple DTW cost (not the full algorithm, just for demonstration)\n",
    "# #     cost = np.sum(np.abs(gen_xy - ref_xy[:len(gen_xy)]))\n",
    "# #     return cost\n",
    "\n",
    "# # --- CONFIGURATION: CHANGE THESE PARAMETERS EASILY ---\n",
    "# FILE_PATHS = {\n",
    "#     'city_a': '/kaggle/input/humob-data/15313913/city_A_challengedata.csv',\n",
    "#     # 'city_b': '/kaggle/input/humob-data/15313913/city_B_challengedata.csv',\n",
    "#     # 'city_c': '/kaggle/input/humob-data/15313913/city_C_challengedata.csv',\n",
    "#     # 'city_d': '/kaggle/input/humob-data/15313913/city_D_challengedata.csv',\n",
    "#     # 'city_f' : '/kaggle/input/dummy-data/city_F_challengedata.csv',\n",
    "# }\n",
    "\n",
    "# # Training and testing days for the train/test split\n",
    "# TRAIN_DAYS = range(1, 61) # Days 1 to 60 for all cities\n",
    "# POST_GAP_DAYS = range(61, 76) # Days 61 to 75\n",
    "# POST_GAP_TRAIN_FRACTION = 0.5 # 50% of the unmasked data in days 61-75\n",
    "# BATCH_SIZE = 2\n",
    "# NUM_EPOCHS = 5\n",
    "# LSTM_UNITS = 128\n",
    "# LEARNING_RATE = 0.001\n",
    "\n",
    "# # --- 1. DATASET AND DATALOADER ---\n",
    "# class CityDataset(Dataset):\n",
    "#     def __init__(self, file_path, is_training=True):\n",
    "#         df = pd.read_csv(file_path)\n",
    "\n",
    "#         # Filter out masked data (999,999) and unnecessary columns\n",
    "#         df = df[(df['x'] != 999) & (df['y'] != 999)].copy()\n",
    "        \n",
    "#         # Rename columns for clarity as specified in the geobleu repo\n",
    "#         df.rename(columns={'d': 'day', 't': 'time'}, inplace=True)\n",
    "        \n",
    "#         # Feature Engineering\n",
    "#         df['day_of_week'] = (df['day'] % 7).astype(np.float32)\n",
    "#         df['is_post_gap'] = (df['day'] > 60).astype(np.float32)\n",
    "        \n",
    "#         # Create lagged features\n",
    "#         for lag in [1, 7]:\n",
    "#             df[f'x_lag_{lag}'] = df.groupby('uid')['x'].shift(lag)\n",
    "#             df[f'y_lag_{lag}'] = df.groupby('uid')['y'].shift(lag)\n",
    "        \n",
    "#         df.fillna(0, inplace=True)\n",
    "\n",
    "#         # Split data based on days and the post-gap fraction\n",
    "#         train_df = df[df['day'].isin(TRAIN_DAYS)]\n",
    "#         post_gap_df = df[df['day'].isin(POST_GAP_DAYS)]\n",
    "        \n",
    "#         uids = post_gap_df['uid'].unique()\n",
    "#         np.random.seed(42)\n",
    "#         np.random.shuffle(uids)\n",
    "        \n",
    "#         train_uids = uids[:int(len(uids) * POST_GAP_TRAIN_FRACTION)]\n",
    "        \n",
    "#         if is_training:\n",
    "#             post_gap_split = post_gap_df[post_gap_df['uid'].isin(train_uids)]\n",
    "#             final_df = pd.concat([train_df, post_gap_split])\n",
    "#         else:\n",
    "#             test_uids = uids[int(len(uids) * POST_GAP_TRAIN_FRACTION):]\n",
    "#             final_df = post_gap_df[post_gap_df['uid'].isin(test_uids)]\n",
    "            \n",
    "#         # Store for metric calculation later\n",
    "#         self.raw_data = final_df[['uid', 'day', 'time', 'x', 'y']].values\n",
    "        \n",
    "#         # Convert to PyTorch tensors\n",
    "#         feature_cols = [\n",
    "#             'day', 'time', 'day_of_week', 'is_post_gap',\n",
    "#             'x_lag_1', 'y_lag_1', 'x_lag_7', 'y_lag_7'\n",
    "#         ]\n",
    "#         self.features = torch.tensor(final_df[feature_cols].values, dtype=torch.float32)\n",
    "#         self.labels = torch.tensor(final_df[['x', 'y']].values, dtype=torch.float32)\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.features)\n",
    "        \n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.features[idx], self.labels[idx], self.raw_data[idx]\n",
    "\n",
    "# # --- 2. MODEL ARCHITECTURE ---\n",
    "# class LSTMModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size=LSTM_UNITS, num_layers=1, output_size=2):\n",
    "#         super(LSTMModel, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.unsqueeze(1) \n",
    "#         h_lstm, _ = self.lstm(x)\n",
    "#         h_lstm = h_lstm[:, -1, :]\n",
    "#         out = self.fc(h_lstm)\n",
    "#         return out\n",
    "\n",
    "# # --- 3. METRICS CALCULATION ---\n",
    "# def calculate_metrics(y_true, y_pred, raw_data):\n",
    "#     rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "#     # Convert true and predicted values to the (day, time, x, y) format\n",
    "#     # The geobleu repo expects this format for single user evaluation.\n",
    "#     true_trajectories = {}\n",
    "#     pred_trajectories = {}\n",
    "    \n",
    "#     for i in range(len(raw_data)):\n",
    "#         uid, day, time = raw_data[i][:3]\n",
    "#         true_traj = (day, time, y_true[i][0], y_true[i][1])\n",
    "#         pred_traj = (day, time, y_pred[i][0], y_pred[i][1])\n",
    "        \n",
    "#         if uid not in true_trajectories:\n",
    "#             true_trajectories[uid] = []\n",
    "#             pred_trajectories[uid] = []\n",
    "#         true_trajectories[uid].append(true_traj)\n",
    "#         pred_trajectories[uid].append(pred_traj)\n",
    "\n",
    "#     # Calculate GeoBLEU and DTW per user and average\n",
    "#     geobleu_scores = []\n",
    "#     dtw_scores = []\n",
    "#     for uid in true_trajectories:\n",
    "#         geobleu_scores.append(calc_geobleu_single(pred_trajectories[uid], true_trajectories[uid]))\n",
    "#         dtw_scores.append(calc_dtw_single(pred_trajectories[uid], true_trajectories[uid]))\n",
    "\n",
    "#     avg_geobleu = np.mean(geobleu_scores)\n",
    "#     avg_dtw = np.mean(dtw_scores)\n",
    "\n",
    "#     print(f\"Metrics:\")\n",
    "#     print(f\"  RMSE Score: {rmse:.4f}\")\n",
    "#     print(f\"  GeoBLEU Score: {avg_geobleu:.4f}\")\n",
    "#     print(f\"  DTW Score: {avg_dtw:.4f}\")\n",
    "\n",
    "\n",
    "# # --- 4. MAIN EXECUTION ---\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Create dummy data for demonstration\n",
    "#     def create_dummy_data(filename, num_rows, discontinuity_day=None):\n",
    "#         df = pd.DataFrame({\n",
    "#             'uid': np.random.randint(0, 1000, size=num_rows),\n",
    "#             'x': np.random.rand(num_rows) * 100,\n",
    "#             'y': np.random.rand(num_rows) * 100,\n",
    "#             'd': np.arange(1, num_rows + 1) % 76 + 1,\n",
    "#             't': np.random.randint(1, 97, size=num_rows)\n",
    "#         })\n",
    "#         if discontinuity_day:\n",
    "#             df.loc[df['d'] > discontinuity_day, ['x', 'y']] += 1.0\n",
    "#         df.loc[df['d'].isin(range(76, 100)), ['x', 'y']] = 999.0\n",
    "#         df.to_csv(filename, index=False)\n",
    "    \n",
    "    \n",
    "#     for city_name, file_path in FILE_PATHS.items():\n",
    "#         print(f\"\\n--- Training and evaluating for {city_name} ---\")\n",
    "        \n",
    "#         # Load datasets\n",
    "#         train_dataset = CityDataset(file_path, is_training=True)\n",
    "#         test_dataset = CityDataset(file_path, is_training=False)\n",
    "        \n",
    "#         if len(train_dataset) == 0 or len(test_dataset) == 0:\n",
    "#             print(\"  No data available for training or testing. Skipping.\")\n",
    "#             continue\n",
    "            \n",
    "#         # Create DataLoaders\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#         test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "#         # Initialize model, loss function, and optimizer\n",
    "#         input_size = train_dataset.features.shape[1]\n",
    "#         model = LSTMModel(input_size)\n",
    "#         criterion = nn.MSELoss()\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "#         # Training loop\n",
    "#         model.train()\n",
    "#         for epoch in range(NUM_EPOCHS):\n",
    "#             for features, labels, _ in train_loader:\n",
    "#                 outputs = model(features)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "                \n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "            \n",
    "#             print(f\"  Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "#         # Evaluation loop\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             y_true_list = []\n",
    "#             y_pred_list = []\n",
    "#             raw_data_list = []\n",
    "#             for features, labels, raw_data in test_loader:\n",
    "#                 outputs = model(features)\n",
    "#                 y_true_list.append(labels.numpy())\n",
    "#                 y_pred_list.append(outputs.numpy())\n",
    "#                 raw_data_list.append(raw_data.numpy())\n",
    "            \n",
    "#             y_true = np.concatenate(y_true_list)\n",
    "#             y_pred = np.concatenate(y_pred_list)\n",
    "#             raw_data = np.concatenate(raw_data_list)\n",
    "            \n",
    "#             calculate_metrics(y_true, y_pred, raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9daa091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T14:37:33.563081Z",
     "iopub.status.busy": "2025-08-26T14:37:33.562876Z",
     "iopub.status.idle": "2025-08-26T14:37:33.572724Z",
     "shell.execute_reply": "2025-08-26T14:37:33.572105Z"
    },
    "papermill": {
     "duration": 0.014434,
     "end_time": "2025-08-26T14:37:33.573832",
     "exception": false,
     "start_time": "2025-08-26T14:37:33.559398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Safe Per-City LSTM baseline (Colab / Kaggle)\n",
    "# # - Primary metric: GeoBLEU (use official library)\n",
    "# # - Training split: days 1-60 + fraction of unmasked days 61-75 -> train\n",
    "# #                  remaining unmasked days 61-75 -> test\n",
    "# #\n",
    "# # IMPORTANT: install geobleu in the notebook environment before running:\n",
    "# %pip install git+https://github.com/yahoojapan/geobleu.git\n",
    "\n",
    "# import os\n",
    "# import math\n",
    "# import gc\n",
    "# import random\n",
    "# from collections import defaultdict\n",
    "# from typing import List, Tuple, Dict\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# # -------- CONFIG (tune conservatively for Colab/Kaggle) ----------\n",
    "# FILE_PATHS = {\n",
    "#     # 'city_a': '/kaggle/input/humob-data/15313913/city_A_challengedata.csv',\n",
    "#     'city_b': '/kaggle/input/humob-data/15313913/city_B_challengedata.csv',\n",
    "#     'city_c': '/kaggle/input/humob-data/15313913/city_C_challengedata.csv',\n",
    "#     'city_d': '/kaggle/input/humob-data/15313913/city_D_challengedata.csv',\n",
    "# }\n",
    "\n",
    "# TRAIN_DAYS = set(range(1,61))\n",
    "# POST_GAP_DAYS = set(range(61,76))\n",
    "# POST_GAP_TRAIN_FRACTION = 0.5\n",
    "\n",
    "# CHUNKSIZE = 400_000      # reduce if RAM pressure; increase for speed if RAM allows\n",
    "# SEQ_LEN = 8              # short sequence length (safe)\n",
    "# BATCH_SIZE = 64         # tune down to 64/32 if OOM\n",
    "# NUM_EPOCHS = 4\n",
    "# LR = 1e-3\n",
    "# HIDDEN = 128\n",
    "# NUM_LAYERS = 1\n",
    "\n",
    "# FP16 = True              # use mixed precision when GPU available\n",
    "# GRAD_ACCUM_STEPS = 2     # set >1 to simulate larger batch without OOM\n",
    "\n",
    "# SEED = 42\n",
    "# random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print(\"Device:\", DEVICE)\n",
    "\n",
    "# # GeoBLEU import (official)\n",
    "# try:\n",
    "#     from geobleu import calc_geobleu_single, calc_dtw_single\n",
    "# except Exception:\n",
    "#     # If not installed, raise clear message\n",
    "#     raise RuntimeError(\"Please install GeoBLEU: %pip install git+https://github.com/yahoojapan/geobleu.git\")\n",
    "\n",
    "# # ------------------ Utilities ------------------\n",
    "# def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"Basic feature engineering and mask removal. Drop masked rows here.\"\"\"\n",
    "#     df = df.rename(columns={'d':'day','t':'time'})\n",
    "#     # drop masked rows (we can't train on masked)\n",
    "#     df = df[(df['x'] != 999) & (df['y'] != 999)].copy()\n",
    "#     if df.empty:\n",
    "#         return df\n",
    "#     # cyclical time features: assume time slots are integer slots; use local max  (safe)\n",
    "#     max_t = max(1, int(df['time'].max()))\n",
    "#     df['time_sin'] = np.sin(2*np.pi*(df['time']-1)/max_t).astype(np.float32)\n",
    "#     df['time_cos'] = np.cos(2*np.pi*(df['time']-1)/max_t).astype(np.float32)\n",
    "#     df['day_of_week'] = (df['day'] % 7).astype(np.int8)\n",
    "#     df['dow_sin'] = np.sin(2*np.pi*(df['day_of_week'])/7).astype(np.float32)\n",
    "#     df['dow_cos'] = np.cos(2*np.pi*(df['day_of_week'])/7).astype(np.float32)\n",
    "#     df['is_post_gap'] = (df['day'] > 60).astype(np.float32)\n",
    "#     return df\n",
    "\n",
    "# def uid_postgap_split_iter(csv_path: str, sample_chunks: int = 5) -> Tuple[set,set]:\n",
    "#     \"\"\"Get train_uids/test_uids among unmasked rows in days 61-75.\n",
    "#     streaming limited by sample_chunks (fast heuristic).\"\"\"\n",
    "#     uids = []\n",
    "#     chunks_seen = 0\n",
    "#     for chunk in pd.read_csv(csv_path, usecols=['uid','x','y','d','t'], chunksize=CHUNKSIZE):\n",
    "#         chunk = chunk.rename(columns={'d':'day','t':'time'})\n",
    "#         chunk = chunk[(chunk['day'].isin(POST_GAP_DAYS)) & (chunk['x'] != 999) & (chunk['y'] != 999)]\n",
    "#         if len(chunk):\n",
    "#             uids.append(chunk['uid'].unique())\n",
    "#         chunks_seen += 1\n",
    "#         if chunks_seen >= sample_chunks:\n",
    "#             break\n",
    "#     if not uids:\n",
    "#         return set(), set()\n",
    "#     all_uids = np.concatenate(uids)\n",
    "#     all_uids = np.unique(all_uids)\n",
    "#     rng = np.random.default_rng(SEED)\n",
    "#     rng.shuffle(all_uids)\n",
    "#     split = int(len(all_uids) * POST_GAP_TRAIN_FRACTION)\n",
    "#     train_uids = set(all_uids[:split])\n",
    "#     test_uids = set(all_uids[split:])\n",
    "#     return train_uids, test_uids\n",
    "\n",
    "# def build_chunk_sequences(chunk: pd.DataFrame, seq_len: int, allowed_uids: set=None, allowed_days: set=None):\n",
    "#     \"\"\"Return list of (X_seq_array, y_next_array, meta) from this chunk.\n",
    "#     Sequences are built per-UID using only rows present in chunk (safe & memory-light).\n",
    "#     \"\"\"\n",
    "#     if chunk.empty:\n",
    "#         return []\n",
    "#     cols = ['x','y','time_sin','time_cos','dow_sin','dow_cos','is_post_gap']\n",
    "#     # If allowed_days filter provided\n",
    "#     if allowed_days is not None:\n",
    "#         chunk = chunk[chunk['day'].isin(allowed_days)]\n",
    "#     if allowed_uids is not None:\n",
    "#         chunk = chunk[chunk['uid'].isin(allowed_uids)]\n",
    "#     if chunk.empty:\n",
    "#         return []\n",
    "#     # sort for safety\n",
    "#     chunk = chunk.sort_values(['uid','day','time'])\n",
    "#     seqs = []\n",
    "#     for uid, g in chunk.groupby('uid'):\n",
    "#         arr = g[cols].values.astype(np.float32)\n",
    "#         meta_arr = g[['day','time','x','y']].values\n",
    "#         L = len(arr)\n",
    "#         if L <= seq_len:\n",
    "#             continue\n",
    "#         # sliding windows inside chunk only\n",
    "#         for i in range(L - seq_len):\n",
    "#             X = arr[i:i+seq_len]          # [seq_len, feat]\n",
    "#             y_row = meta_arr[i+seq_len]   # this row has day,time,x,y for label\n",
    "#             y = np.array([y_row[2], y_row[3]], dtype=np.float32)\n",
    "#             meta = (int(uid), int(y_row[0]), int(y_row[1]))\n",
    "#             seqs.append((X, y, meta))\n",
    "#     return seqs\n",
    "\n",
    "# # ------------------ Model ------------------\n",
    "# class LSTMModel(nn.Module):\n",
    "#     def __init__(self, input_dim=7, hidden=HIDDEN, num_layers=NUM_LAYERS):\n",
    "#         super().__init__()\n",
    "#         self.lstm = nn.LSTM(input_dim, hidden, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden, 2)\n",
    "#     def forward(self, x):\n",
    "#         # x: [B, seq_len, feat]\n",
    "#         out, _ = self.lstm(x)\n",
    "#         last = out[:, -1, :]\n",
    "#         return self.fc(last)\n",
    "\n",
    "# # ------------------ Training/Evaluation helpers ------------------\n",
    "# def train_on_chunk(model, optimizer, scaler, chunk_seqs, device, criterion, accum_steps=1):\n",
    "#     \"\"\"Train model on a *list* of sequences from a single chunk, then drop them.\"\"\"\n",
    "#     if not chunk_seqs:\n",
    "#         return None\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     cnt = 0\n",
    "#     batch_X, batch_y = [], []\n",
    "#     for (X, y, _) in chunk_seqs:\n",
    "#         batch_X.append(X)\n",
    "#         batch_y.append(y)\n",
    "#         if len(batch_X) >= BATCH_SIZE:\n",
    "#             Xb = torch.tensor(np.stack(batch_X), dtype=torch.float32, device=device)\n",
    "#             yb = torch.tensor(np.stack(batch_y), dtype=torch.float32, device=device)\n",
    "#             with torch.amp.autocast('cuda', enabled=FP16 and device.startswith('cuda')):\n",
    "#                 pred = model(Xb)\n",
    "#                 loss = criterion(pred, yb) / accum_steps\n",
    "#             scaler.scale(loss).backward()\n",
    "#             if (cnt+1) % accum_steps == 0:\n",
    "#                 scaler.unscale_(optimizer)\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#                 scaler.step(optimizer)\n",
    "#                 scaler.update()\n",
    "#                 optimizer.zero_grad(set_to_none=True)\n",
    "#             running_loss += float(loss.item()) * accum_steps\n",
    "#             batch_X, batch_y = [], []\n",
    "#             cnt += 1\n",
    "#     # final partial batch\n",
    "#     if batch_X:\n",
    "#         Xb = torch.tensor(np.stack(batch_X), dtype=torch.float32, device=device)\n",
    "#         yb = torch.tensor(np.stack(batch_y), dtype=torch.float32, device=device)\n",
    "#         with torch.amp.autocast('cuda', enabled=FP16 and device.startswith('cuda')):\n",
    "#             pred = model(Xb)\n",
    "#             loss = criterion(pred, yb) / accum_steps\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.unscale_(optimizer)\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "#         optimizer.zero_grad(set_to_none=True)\n",
    "#         running_loss += float(loss.item()) * accum_steps\n",
    "#     return running_loss\n",
    "\n",
    "# def evaluate_city(model, csv_path, test_uids, device):\n",
    "#     \"\"\"Stream through CSV, build sequences for test_uids and produce predictions per uid,\n",
    "#     then compute GeoBLEU & DTW averaged over uids.\"\"\"\n",
    "#     model.eval()\n",
    "#     per_uid_true = defaultdict(list)\n",
    "#     per_uid_pred = defaultdict(list)\n",
    "#     # stream through chunks and build sequences for allowed uids and days=POST_GAP_DAYS\n",
    "#     for chunk in pd.read_csv(csv_path, usecols=['uid','x','y','d','t'], chunksize=CHUNKSIZE):\n",
    "#         chunk = chunk.rename(columns={'d':'day','t':'time'})\n",
    "#         chunk = add_features(chunk)\n",
    "#         if chunk.empty:\n",
    "#             continue\n",
    "#         # limit to days 61-75 and uids in test set\n",
    "#         chunk = chunk[(chunk['day'].isin(POST_GAP_DAYS)) & (chunk['uid'].isin(test_uids))]\n",
    "#         if chunk.empty:\n",
    "#             continue\n",
    "#         seqs = build_chunk_sequences(chunk, SEQ_LEN, allowed_uids=test_uids, allowed_days=POST_GAP_DAYS)\n",
    "#         # run model on sequences and append per-uid lists\n",
    "#         if not seqs:\n",
    "#             continue\n",
    "#         # batch predict\n",
    "#         for i in range(0, len(seqs), BATCH_SIZE):\n",
    "#             batch = seqs[i:i+BATCH_SIZE]\n",
    "#             Xs = np.stack([s[0] for s in batch])\n",
    "#             metas = [s[2] for s in batch]\n",
    "#             Xb = torch.tensor(Xs, dtype=torch.float32, device=device)\n",
    "#             with torch.no_grad(), torch.cuda.amp.autocast(enabled=FP16 and device.startswith('cuda')):\n",
    "#                 preds = model(Xb).cpu().numpy()\n",
    "#             for (uid, day, time), p, s in zip(metas, preds, [b[1] for b in batch]):\n",
    "#                 per_uid_pred[uid].append((int(day), int(time), float(p[0]), float(p[1])))\n",
    "#                 per_uid_true[uid].append((int(day), int(time), float(s[0]), float(s[1])))\n",
    "#         # free memory\n",
    "#         del seqs, chunk\n",
    "#         gc.collect()\n",
    "\n",
    "#     # compute metrics per-uid (use calc_geobleu_single, calc_dtw_single)\n",
    "#     geobleu_scores = []\n",
    "#     dtw_scores = []\n",
    "#     for uid in per_uid_true:\n",
    "#         # ensure both lists are sorted\n",
    "#         true_seq = sorted(per_uid_true[uid], key=lambda x:(x[0],x[1]))\n",
    "#         pred_seq = sorted(per_uid_pred.get(uid, []), key=lambda x:(x[0],x[1]))\n",
    "#         if len(true_seq) == 0 or len(pred_seq) == 0:\n",
    "#             continue\n",
    "#         try:\n",
    "#             g = calc_geobleu_single(pred_seq, true_seq)\n",
    "#             d = calc_dtw_single(pred_seq, true_seq)\n",
    "#         except Exception:\n",
    "#             # In very rare cases if calc_geobleu_single expects different format, fallback simple measure\n",
    "#             # But you installed official library, so this should not trigger.\n",
    "#             g = 0.0\n",
    "#             d = float('inf')\n",
    "#         geobleu_scores.append(g)\n",
    "#         dtw_scores.append(d)\n",
    "#     avg_geo = float(np.mean(geobleu_scores)) if geobleu_scores else 0.0\n",
    "#     avg_dtw = float(np.mean(dtw_scores)) if dtw_scores else float('inf')\n",
    "#     return avg_geo, avg_dtw, len(per_uid_true)\n",
    "\n",
    "# # ------------------ Main per-city loop ------------------\n",
    "# def train_city(csv_path, city_name, device=DEVICE):\n",
    "#     print(f\"\\n=== City: {city_name} ===\")\n",
    "#     train_uids, test_uids = uid_postgap_split_iter(csv_path, sample_chunks=8)\n",
    "#     if not train_uids and not test_uids:\n",
    "#         print(\"No unmasked post-gap uids found (check CSV). Exiting city.\")\n",
    "#         return\n",
    "#     print(f\"Train UIDs (sampled): {len(train_uids)} ; Test UIDs (sampled): {len(test_uids)}\")\n",
    "\n",
    "#     model = LSTMModel(input_dim=7, hidden=HIDDEN, num_layers=NUM_LAYERS).to(device)\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "#     criterion = nn.SmoothL1Loss()\n",
    "#     scaler = torch.cuda.amp.GradScaler(enabled=FP16 and device.startswith('cuda'))\n",
    "\n",
    "#     best_geo = -1.0\n",
    "#     early = 0\n",
    "\n",
    "#     for epoch in range(1, NUM_EPOCHS+1):\n",
    "#         print(f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
    "#         total_loss = 0.0\n",
    "#         chunks = 0\n",
    "#         # stream chunks and train on sequences from chunk (train days: 1-60 + postgap train_uids)\n",
    "#         for chunk in pd.read_csv(csv_path, usecols=['uid','x','y','d','t'], chunksize=CHUNKSIZE):\n",
    "#             chunk = chunk.rename(columns={'d':'day','t':'time'})\n",
    "#             chunk = add_features(chunk)\n",
    "#             if chunk.empty:\n",
    "#                 continue\n",
    "#             # limit chunk to training rows: days 1-60 OR (days 61-75 AND uid in train_uids)\n",
    "#             mask = (chunk['day'].isin(TRAIN_DAYS)) | ((chunk['day'].isin(POST_GAP_DAYS)) & (chunk['uid'].isin(train_uids)))\n",
    "#             chunk_train = chunk[mask]\n",
    "#             if chunk_train.empty:\n",
    "#                 continue\n",
    "#             seqs = build_chunk_sequences(chunk_train, SEQ_LEN, allowed_uids=None, allowed_days=None)\n",
    "#             if not seqs:\n",
    "#                 continue\n",
    "#             loss = train_on_chunk(model, optimizer, scaler, seqs, device, criterion, accum_steps=GRAD_ACCUM_STEPS)\n",
    "#             if loss is not None:\n",
    "#                 total_loss += loss\n",
    "#             chunks += 1\n",
    "#             # keep memory low\n",
    "#             del seqs, chunk_train\n",
    "#             gc.collect()\n",
    "#         avg_loss = total_loss / max(1, chunks)\n",
    "#         print(f\"  Avg chunk loss: {avg_loss:.5f} over {chunks} chunks\")\n",
    "\n",
    "#         # validation after each epoch\n",
    "#         geo, dtw, n_uids = evaluate_city(model, csv_path, test_uids, device)\n",
    "#         print(f\"  Val GeoBLEU: {geo:.6f} | Val DTW: {dtw:.4f} | UIDs evaluated: {n_uids}\")\n",
    "\n",
    "#         # early save if improves\n",
    "#         if geo > best_geo:\n",
    "#             best_geo = geo\n",
    "#             early = 0\n",
    "#             # save model\n",
    "#             ckpt = f\"{city_name}_best.pt\"\n",
    "#             torch.save({'model_state': model.state_dict(), 'optimizer': optimizer.state_dict()}, ckpt)\n",
    "#             print(\"  Saved best checkpoint:\", ckpt)\n",
    "#         else:\n",
    "#             early += 1\n",
    "#             if early >= 3:\n",
    "#                 print(\"  Early stopping triggered.\")\n",
    "#                 break\n",
    "\n",
    "#     print(f\"Finished training {city_name}. Best GeoBLEU: {best_geo:.6f}\")\n",
    "\n",
    "#     # final evaluation (full) - you can re-run evaluate_city for full final metrics if desired\n",
    "#     return model\n",
    "\n",
    "# # ------------------ Run for each configured city ------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     for city, path in FILE_PATHS.items():\n",
    "#         if not os.path.exists(path):\n",
    "#             print(f\"[WARN] Missing {path}, skipping {city}\")\n",
    "#             continue\n",
    "#         train_city(path, city, DEVICE)\n",
    "\n",
    "#     # -----------------------------\n",
    "#     # (COMMENTED) Masked data prediction for final submission:\n",
    "#     # -----------------------------\n",
    "#     # To create predictions for masked rows (x=999 & y=999), follow these steps:\n",
    "#     # 1) Load the saved best model checkpoint for city.\n",
    "#     # 2) Stream CSV in chunks, but this time filter chunk to rows with (x==999 and y==999)\n",
    "#     # 3) For each masked UID/day/time, construct the input sequence from latest available previous unmasked rows\n",
    "#     #    (you may need a small per-uid cache tracking last SEQ_LEN unmasked points).\n",
    "#     # 4) Run model on the sequence and write predicted (x,y) into submission CSV in required format.\n",
    "#     #\n",
    "#     # Pseudocode:\n",
    "#     #   cache = defaultdict(list)   # uid -> recent unmasked (x,y,time,dow,is_post_gap...)\n",
    "#     #   for chunk in pd.read_csv(..., chunksize=CHUNKSIZE):\n",
    "#     #       for row in chunk rows:\n",
    "#     #           if row unmasked: update cache[uid]\n",
    "#     #           else:  # masked -> predict if cache[uid] has SEQ_LEN entries\n",
    "#     #               X = build_features_from_cache(cache[uid][-SEQ_LEN:])\n",
    "#     #               pred = model(torch.tensor(X).unsqueeze(0).to(device))\n",
    "#     #               write pred to submission\n",
    "#     #\n",
    "#     # Note: predicting masked rows requires careful handling of time slots and continuity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aaf0f02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T14:37:33.579633Z",
     "iopub.status.busy": "2025-08-26T14:37:33.579430Z",
     "iopub.status.idle": "2025-08-26T16:24:32.108886Z",
     "shell.execute_reply": "2025-08-26T16:24:32.107987Z"
    },
    "papermill": {
     "duration": 6418.537714,
     "end_time": "2025-08-26T16:24:32.113954",
     "exception": false,
     "start_time": "2025-08-26T14:37:33.576240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/yahoojapan/geobleu.git\r\n",
      "  Cloning https://github.com/yahoojapan/geobleu.git to /tmp/pip-req-build-niz35_h2\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/yahoojapan/geobleu.git /tmp/pip-req-build-niz35_h2\r\n",
      "  Resolved https://github.com/yahoojapan/geobleu.git to commit d9efb0fbc97d75b85f273a5db5f4da2215b0f715\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from geobleu==0.5) (1.26.4)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from geobleu==0.5) (1.15.3)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->geobleu==0.5) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->geobleu==0.5) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->geobleu==0.5) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->geobleu==0.5) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->geobleu==0.5) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->geobleu==0.5) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->geobleu==0.5) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->geobleu==0.5) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->geobleu==0.5) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->geobleu==0.5) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->geobleu==0.5) (2024.2.0)\r\n",
      "Building wheels for collected packages: geobleu\r\n",
      "  Building wheel for geobleu (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for geobleu: filename=geobleu-0.5-py3-none-any.whl size=5442 sha256=33374acc37a70bc35e55bff04766444d949349421351a5d340fa230571689b47\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-myywlb8f/wheels/97/c3/ec/b654420f0a3507ff7c5234dc42ee565578300e789e6705ec17\r\n",
      "Successfully built geobleu\r\n",
      "Installing collected packages: geobleu\r\n",
      "Successfully installed geobleu-0.5\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Using device: cuda | CUDA available: True\n",
      "=== City: city_d ===\n",
      "Train UIDs (sampled): 2449 ; Test UIDs (sampled): 2450\n",
      "Epoch 1/10\n",
      "  Avg chunk loss: 30180.42853 over 30 chunks\n",
      "Epoch 2/10\n",
      "  Avg chunk loss: 23068.45856 over 30 chunks\n",
      "Epoch 3/10\n",
      "  Avg chunk loss: 22574.74936 over 30 chunks\n",
      "Epoch 4/10\n",
      "  Avg chunk loss: 23019.04033 over 30 chunks\n",
      "Epoch 5/10\n",
      "  Avg chunk loss: 22613.25417 over 30 chunks\n",
      "Epoch 6/10\n",
      "  Avg chunk loss: 22383.48788 over 30 chunks\n",
      "Epoch 7/10\n",
      "  Avg chunk loss: 22276.06556 over 30 chunks\n",
      "Epoch 8/10\n",
      "  Avg chunk loss: 22433.66855 over 30 chunks\n",
      "Epoch 9/10\n",
      "  Avg chunk loss: 22322.65594 over 30 chunks\n",
      "Epoch 10/10\n",
      "  Avg chunk loss: 22420.48963 over 30 chunks\n",
      "  Saved final checkpoint: city_d_final.pt\n",
      "Saved loss curve: city_d_loss_curve.png\n",
      "Final GeoBLEU: 0.046120 | Final DTW: 27.0749 | UIDs evaluated: 2450\n"
     ]
    }
   ],
   "source": [
    "# Safe Per-City LSTM baseline (Colab / Kaggle)\n",
    "# - Primary metric: GeoBLEU (use official library)\n",
    "# - Training split: days 1-60 + fraction of unmasked days 61-75 -> train\n",
    "#                  remaining unmasked days 61-75 -> test\n",
    "#\n",
    "# IMPORTANT: install geobleu in the notebook environment before running:\n",
    "%pip install git+https://github.com/yahoojapan/geobleu.git\n",
    "\n",
    "import os\n",
    "import math\n",
    "import gc\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# -------- CONFIG (tune conservatively for Colab/Kaggle) ----------\n",
    "FILE_PATHS = {\n",
    "    # 'city_a': '/kaggle/input/humob-data/15313913/city_A_challengedata.csv',\n",
    "    # 'city_b': '/kaggle/input/humob-data/15313913/city_B_challengedata.csv',\n",
    "    # 'city_c': '/kaggle/input/humob-data/15313913/city_C_challengedata.csv',\n",
    "    'city_d': '/kaggle/input/humob-data/15313913/city_D_challengedata.csv',\n",
    "}\n",
    "\n",
    "TRAIN_DAYS = set(range(1, 61))\n",
    "POST_GAP_DAYS = set(range(61, 76))\n",
    "POST_GAP_TRAIN_FRACTION = 0.5\n",
    "\n",
    "CHUNKSIZE = 400_000      # reduce if RAM pressure; increase for speed if RAM allows\n",
    "SEQ_LEN = 8              # short sequence length (safe)\n",
    "BATCH_SIZE = 64          # tune down to 64/32 if OOM\n",
    "NUM_EPOCHS = 10           # modest epochs for large-scale data; tune later if needed\n",
    "LR = 1e-3\n",
    "HIDDEN = 128\n",
    "NUM_LAYERS = 1\n",
    "\n",
    "FP16 = True              # use mixed precision when GPU available\n",
    "GRAD_ACCUM_STEPS = 2     # set >1 to simulate larger batch without OOM\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", DEVICE, \"| CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# GeoBLEU import (official)\n",
    "try:\n",
    "    from geobleu import calc_geobleu_single, calc_dtw_single\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Please install GeoBLEU: %pip install git+https://github.com/yahoojapan/geobleu.git\") from e\n",
    "\n",
    "# ------------------ Utilities ------------------\n",
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Basic feature engineering and mask removal. Drop masked rows here.\"\"\"\n",
    "    df = df.rename(columns={'d': 'day', 't': 'time'})\n",
    "    df = df[(df['x'] != 999) & (df['y'] != 999)].copy()\n",
    "    if df.empty:\n",
    "        return df\n",
    "    max_t = max(1, int(df['time'].max()))\n",
    "    df['time_sin'] = np.sin(2 * np.pi * (df['time'] - 1) / max_t).astype(np.float32)\n",
    "    df['time_cos'] = np.cos(2 * np.pi * (df['time'] - 1) / max_t).astype(np.float32)\n",
    "    df['day_of_week'] = (df['day'] % 7).astype(np.int8)\n",
    "    df['dow_sin'] = np.sin(2 * np.pi * (df['day_of_week']) / 7).astype(np.float32)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * (df['day_of_week']) / 7).astype(np.float32)\n",
    "    df['is_post_gap'] = (df['day'] > 60).astype(np.float32)\n",
    "    return df\n",
    "\n",
    "def uid_postgap_split_iter(csv_path: str, sample_chunks: int = 8) -> Tuple[set, set]:\n",
    "    \"\"\"Get train_uids/test_uids among unmasked rows in days 61-75.\n",
    "    Streaming limited by sample_chunks (fast heuristic).\"\"\"\n",
    "    uids = []\n",
    "    chunks_seen = 0\n",
    "    for chunk in pd.read_csv(csv_path, usecols=['uid', 'x', 'y', 'd', 't'], chunksize=CHUNKSIZE):\n",
    "        chunk = chunk.rename(columns={'d': 'day', 't': 'time'})\n",
    "        chunk = chunk[(chunk['day'].isin(POST_GAP_DAYS)) & (chunk['x'] != 999) & (chunk['y'] != 999)]\n",
    "        if len(chunk):\n",
    "            uids.append(chunk['uid'].unique())\n",
    "        chunks_seen += 1\n",
    "        if chunks_seen >= sample_chunks:\n",
    "            break\n",
    "    if not uids:\n",
    "        return set(), set()\n",
    "    all_uids = np.unique(np.concatenate(uids))\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    rng.shuffle(all_uids)\n",
    "    split = int(len(all_uids) * POST_GAP_TRAIN_FRACTION)\n",
    "    train_uids = set(all_uids[:split])\n",
    "    test_uids = set(all_uids[split:])\n",
    "    return train_uids, test_uids\n",
    "\n",
    "def build_chunk_sequences(chunk: pd.DataFrame, seq_len: int, allowed_uids: set = None, allowed_days: set = None):\n",
    "    \"\"\"Return list of (X_seq_array, y_next_array, meta) from this chunk.\n",
    "    Sequences are built per-UID using only rows present in chunk (safe & memory-light).\n",
    "    \"\"\"\n",
    "    if chunk.empty:\n",
    "        return []\n",
    "    cols = ['x', 'y', 'time_sin', 'time_cos', 'dow_sin', 'dow_cos', 'is_post_gap']\n",
    "    if allowed_days is not None:\n",
    "        chunk = chunk[chunk['day'].isin(allowed_days)]\n",
    "    if allowed_uids is not None:\n",
    "        chunk = chunk[chunk['uid'].isin(allowed_uids)]\n",
    "    if chunk.empty:\n",
    "        return []\n",
    "    chunk = chunk.sort_values(['uid', 'day', 'time'])\n",
    "    seqs = []\n",
    "    for uid, g in chunk.groupby('uid'):\n",
    "        arr = g[cols].values.astype(np.float32)\n",
    "        meta_arr = g[['day', 'time', 'x', 'y']].values\n",
    "        L = len(arr)\n",
    "        if L <= seq_len:\n",
    "            continue\n",
    "        for i in range(L - seq_len):\n",
    "            X = arr[i:i + seq_len]\n",
    "            y_row = meta_arr[i + seq_len]\n",
    "            y = np.array([y_row[2], y_row[3]], dtype=np.float32)\n",
    "            meta = (int(uid), int(y_row[0]), int(y_row[1]))\n",
    "            seqs.append((X, y, meta))\n",
    "    return seqs\n",
    "\n",
    "# ------------------ Model ------------------\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim: int = 7, hidden: int = HIDDEN, num_layers: int = NUM_LAYERS):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, seq_len, feat]\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.fc(last)\n",
    "\n",
    "class ComboLoss(nn.Module):\n",
    "    \"\"\"SmoothL1 + Cosine similarity (directional) for spatial stability.\"\"\"\n",
    "    def __init__(self, alpha: float = 0.8, beta: float = 0.2, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.SmoothL1Loss()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        l_reg = self.l1(pred, target)\n",
    "        p = pred + self.eps\n",
    "        t = target + self.eps\n",
    "        cos = (p * t).sum(dim=1) / (p.norm(dim=1) * t.norm(dim=1) + self.eps)\n",
    "        l_cos = 1 - cos.mean()\n",
    "        return self.alpha * l_reg + self.beta * l_cos\n",
    "\n",
    "# ------------------ Training/Evaluation helpers ------------------\n",
    "def train_on_chunk(model, optimizer, scaler, chunk_seqs, device, criterion, accum_steps=1):\n",
    "    \"\"\"Train model on a *list* of sequences from a single chunk, then drop them.\"\"\"\n",
    "    if not chunk_seqs:\n",
    "        return None\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    cnt = 0\n",
    "    batch_X, batch_y = [], []\n",
    "    for (X, y, _) in chunk_seqs:\n",
    "        batch_X.append(X)\n",
    "        batch_y.append(y)\n",
    "        if len(batch_X) >= BATCH_SIZE:\n",
    "            Xb = torch.tensor(np.stack(batch_X), dtype=torch.float32, device=device)\n",
    "            yb = torch.tensor(np.stack(batch_y), dtype=torch.float32, device=device)\n",
    "            with torch.amp.autocast(device_type='cuda', enabled=FP16 and device.type == 'cuda'):\n",
    "                pred = model(Xb)\n",
    "                loss = criterion(pred, yb) / accum_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            if (cnt + 1) % accum_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            running_loss += float(loss.item()) * accum_steps\n",
    "            batch_X, batch_y = [], []\n",
    "            cnt += 1\n",
    "    # final partial batch\n",
    "    if batch_X:\n",
    "        Xb = torch.tensor(np.stack(batch_X), dtype=torch.float32, device=device)\n",
    "        yb = torch.tensor(np.stack(batch_y), dtype=torch.float32, device=device)\n",
    "        with torch.amp.autocast(device_type='cuda', enabled=FP16 and device.type == 'cuda'):\n",
    "            pred = model(Xb)\n",
    "            loss = criterion(pred, yb) / accum_steps\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        running_loss += float(loss.item()) * accum_steps\n",
    "    return running_loss\n",
    "\n",
    "def evaluate_city(model, csv_path, test_uids, device):\n",
    "    \"\"\"Stream through CSV, build sequences for test_uids and produce predictions per uid,\n",
    "    then compute GeoBLEU & DTW averaged over uids.\"\"\"\n",
    "    model.eval()\n",
    "    per_uid_true = defaultdict(list)\n",
    "    per_uid_pred = defaultdict(list)\n",
    "    for chunk in pd.read_csv(csv_path, usecols=['uid', 'x', 'y', 'd', 't'], chunksize=CHUNKSIZE):\n",
    "        chunk = chunk.rename(columns={'d': 'day', 't': 'time'})\n",
    "        chunk = add_features(chunk)\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "        chunk = chunk[(chunk['day'].isin(POST_GAP_DAYS)) & (chunk['uid'].isin(test_uids))]\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "        seqs = build_chunk_sequences(chunk, SEQ_LEN, allowed_uids=test_uids, allowed_days=POST_GAP_DAYS)\n",
    "        if not seqs:\n",
    "            continue\n",
    "        for i in range(0, len(seqs), BATCH_SIZE):\n",
    "            batch = seqs[i:i + BATCH_SIZE]\n",
    "            Xs = np.stack([s[0] for s in batch])\n",
    "            metas = [s[2] for s in batch]\n",
    "            Xb = torch.tensor(Xs, dtype=torch.float32, device=device)\n",
    "            with torch.no_grad(), torch.amp.autocast(device_type='cuda', enabled=FP16 and device.type == 'cuda'):\n",
    "                preds = model(Xb).cpu().numpy()\n",
    "            for (uid, day, time), p, s in zip(metas, preds, [b[1] for b in batch]):\n",
    "                per_uid_pred[uid].append((int(day), int(time), float(p[0]), float(p[1])))\n",
    "                per_uid_true[uid].append((int(day), int(time), float(s[0]), float(s[1])))\n",
    "        del seqs, chunk\n",
    "        gc.collect()\n",
    "\n",
    "    geobleu_scores = []\n",
    "    dtw_scores = []\n",
    "    for uid in per_uid_true:\n",
    "        true_seq = sorted(per_uid_true[uid], key=lambda x: (x[0], x[1]))\n",
    "        pred_seq = sorted(per_uid_pred.get(uid, []), key=lambda x: (x[0], x[1]))\n",
    "        if len(true_seq) == 0 or len(pred_seq) == 0:\n",
    "            continue\n",
    "        try:\n",
    "            g = calc_geobleu_single(pred_seq, true_seq)\n",
    "            d = calc_dtw_single(pred_seq, true_seq)\n",
    "        except Exception:\n",
    "            g = 0.0\n",
    "            d = float('inf')\n",
    "        geobleu_scores.append(g)\n",
    "        dtw_scores.append(d)\n",
    "    avg_geo = float(np.mean(geobleu_scores)) if geobleu_scores else 0.0\n",
    "    avg_dtw = float(np.mean(dtw_scores)) if dtw_scores else float('inf')\n",
    "    return avg_geo, avg_dtw, len(per_uid_true)\n",
    "\n",
    "# ------------------ Helpers: plotting & cleanup ------------------\n",
    "def plot_losses(city_name: str, epoch_losses: list):\n",
    "    try:\n",
    "        plt.figure()\n",
    "        plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, marker='o')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Average Chunk Loss')\n",
    "        plt.title(f'Loss Curve - {city_name}')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        png = f\"{city_name}_loss_curve.png\"\n",
    "        plt.savefig(png)\n",
    "        print(f\"Saved loss curve: {png}\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Could not plot loss curve:\", e)\n",
    "\n",
    "def cleanup_gpu_and_exit(force_terminate: bool = False):\n",
    "    \"\"\"Best-effort cleanup. NOTE: You cannot programmatically *disable* Kaggle's\n",
    "    accelerator from code; that setting is controlled by the 'Save Version' pane.\n",
    "    This frees CUDA memory and optionally terminates the process.\"\"\"\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "            # torch.cuda.ipc_collect may not exist on older torch; guard it\n",
    "            if hasattr(torch.cuda, 'ipc_collect'):\n",
    "                torch.cuda.ipc_collect()\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(\"Cleanup warning:\", e)\n",
    "    if force_terminate:\n",
    "        # Hard exit - ends the process immediately. Kaggle will free the runtime after job end.\n",
    "        os._exit(0)\n",
    "\n",
    "# ------------------ Main per-city loop ------------------\n",
    "def train_city(csv_path, city_name, device=DEVICE):\n",
    "    print(f\"=== City: {city_name} ===\")\n",
    "    train_uids, test_uids = uid_postgap_split_iter(csv_path, sample_chunks=8)\n",
    "    if not train_uids and not test_uids:\n",
    "        print(\"No unmasked post-gap uids found (check CSV). Exiting city.\")\n",
    "        return\n",
    "    print(f\"Train UIDs (sampled): {len(train_uids)} ; Test UIDs (sampled): {len(test_uids)}\")\n",
    "\n",
    "    model = LSTMModel(input_dim=7, hidden=HIDDEN, num_layers=NUM_LAYERS).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    criterion = ComboLoss(alpha=0.8, beta=0.2)\n",
    "    scaler = torch.amp.GradScaler(enabled=FP16 and device.type == 'cuda')\n",
    "\n",
    "    epoch_losses = []\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        print(f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
    "        total_loss = 0.0\n",
    "        chunks = 0\n",
    "        for chunk in pd.read_csv(csv_path, usecols=['uid', 'x', 'y', 'd', 't'], chunksize=CHUNKSIZE):\n",
    "            chunk = chunk.rename(columns={'d': 'day', 't': 'time'})\n",
    "            chunk = add_features(chunk)\n",
    "            if chunk.empty:\n",
    "                continue\n",
    "            mask = (chunk['day'].isin(TRAIN_DAYS)) | ((chunk['day'].isin(POST_GAP_DAYS)) & (chunk['uid'].isin(train_uids)))\n",
    "            chunk_train = chunk[mask]\n",
    "            if chunk_train.empty:\n",
    "                continue\n",
    "            seqs = build_chunk_sequences(chunk_train, SEQ_LEN, allowed_uids=None, allowed_days=None)\n",
    "            if not seqs:\n",
    "                continue\n",
    "            loss = train_on_chunk(model, optimizer, scaler, seqs, device, criterion, accum_steps=GRAD_ACCUM_STEPS)\n",
    "            if loss is not None:\n",
    "                total_loss += loss\n",
    "            chunks += 1\n",
    "            del seqs, chunk_train\n",
    "            gc.collect()\n",
    "        avg_loss = total_loss / max(1, chunks)\n",
    "        epoch_losses.append(avg_loss)\n",
    "        print(f\"  Avg chunk loss: {avg_loss:.5f} over {chunks} chunks\")\n",
    "\n",
    "    # Save final checkpoint after training\n",
    "    ckpt = f\"{city_name}_final.pt\"\n",
    "    torch.save({'model_state': model.state_dict(), 'optimizer': optimizer.state_dict()}, ckpt)\n",
    "    print(\"  Saved final checkpoint:\", ckpt)\n",
    "\n",
    "    # Plot & save loss curve\n",
    "    plot_losses(city_name, epoch_losses)\n",
    "\n",
    "    # Final evaluation (single pass) on test_uids\n",
    "    geo, dtw, n_uids = evaluate_city(model, csv_path, test_uids, device)\n",
    "    print(f\"Final GeoBLEU: {geo:.6f} | Final DTW: {dtw:.4f} | UIDs evaluated: {n_uids}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# ------------------ Run for each configured city ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        for city, path in FILE_PATHS.items():\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"[WARN] Missing {path}, skipping {city}\")\n",
    "                continue\n",
    "            train_city(path, city, DEVICE)\n",
    "    finally:\n",
    "        # Best-effort cleanup; to force process termination uncomment the next line (careful).\n",
    "        cleanup_gpu_and_exit(force_terminate=False)\n",
    "        # If you want the script to hard-exit immediately (releases process), use:\n",
    "        # cleanup_gpu_and_exit(force_terminate=True)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7587200,
     "sourceId": 12055345,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8122955,
     "sourceId": 12843248,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6425.809427,
   "end_time": "2025-08-26T16:24:34.754979",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-26T14:37:28.945552",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
