{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12055345,"sourceType":"datasetVersion","datasetId":7587200}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install -q git+https://github.com/yahoojapan/geobleu.git tqdm\n","metadata":{"execution":{"iopub.status.busy":"2025-06-10T07:13:41.305853Z","iopub.execute_input":"2025-06-10T07:13:41.306113Z","iopub.status.idle":"2025-06-10T07:13:49.986792Z","shell.execute_reply.started":"2025-06-10T07:13:41.306083Z","shell.execute_reply":"2025-06-10T07:13:49.985400Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport multiprocessing as mp\nfrom geobleu import calc_geobleu_single\nfrom collections import Counter, defaultdict\nfrom tqdm.auto import tqdm\nimport os\n\n# Ensure floats print with 5 decimals\npd.set_option(\"display.float_format\", \"{:.5f}\".format)\n\n# Constants\nDATA_DIR = \"/kaggle/input/humob-data/15313913\"\n# CITIES = [\"A\", \"B\", \"C\", \"D\"]\nCITIES = [\"D\"]\nCOLUMNS = [\"uid\", \"d\", \"t\", \"x\", \"y\"]\nDTYPES = {\n    \"uid\": \"int32\",\n    \"d\": \"int8\",\n    \"t\": \"int8\",\n    \"x\": \"int16\",\n    \"y\": \"int16\",\n}\nTRAIN_DAY_MAX = 60\nTEST_DAY_MIN = 61\nMASK_VALUE = 999\nCHUNK_SIZE = 500_000  # adjust as needed for memory/time\n\n# Set random seed for reproducible unigram sampling\nnp.random.seed(42)\n","metadata":{"execution":{"iopub.status.busy":"2025-06-10T07:14:45.896368Z","iopub.execute_input":"2025-06-10T07:14:45.897066Z","iopub.status.idle":"2025-06-10T07:14:47.018313Z","shell.execute_reply.started":"2025-06-10T07:14:45.897023Z","shell.execute_reply":"2025-06-10T07:14:47.017454Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _geobleu_for_group(group):\n    \"\"\"\n    Given a DataFrame for one uid (with columns 'd','t','x_pred','y_pred','x_gt','y_gt'),\n    compute and return its GEO-BLEU score.\n    \"\"\"\n    pred_seq = list(zip(group['d'], group['t'], group['x_pred'], group['y_pred']))\n    true_seq = list(zip(group['d'], group['t'], group['x_gt'], group['y_gt']))\n    return calc_geobleu_single(pred_seq, true_seq)\n\ndef evaluate_geobleu_parallel(pred_df: pd.DataFrame, gt_df: pd.DataFrame) -> float:\n    \"\"\"\n    - pred_df:  DataFrame with columns ['uid','d','t','x_pred','y_pred']\n    - gt_df:    DataFrame with columns ['uid','d','t','x_gt','y_gt']\n\n    Merges on ['uid','d','t'], then uses multiprocessing + tqdm to compute\n    GEO-BLEU per user in parallel. Returns the average GEO-BLEU over all users.\n    \"\"\"\n    merged = pd.merge(pred_df, gt_df, on=['uid', 'd', 't'], how='inner')\n    if merged.empty:\n        return 0.0\n\n    # Rename ground-truth x,y for readability\n    merged = merged.rename(columns={'x': 'x_gt', 'y': 'y_gt'})\n\n    # Split into list of DataFrames by uid\n    grouped = [grp for _, grp in merged.groupby('uid')]\n    num_users = len(grouped)\n    if num_users == 0:\n        return 0.0\n\n    print(f\"    ▶ Evaluating GEO-BLEU on {num_users} users...\")\n\n    # Use imap_unordered + tqdm for a progress bar\n    with mp.Pool(processes=max(1, mp.cpu_count() - 1)) as pool:\n        results = []\n        for score in tqdm(pool.imap_unordered(_geobleu_for_group, grouped),\n                          total=num_users,\n                          desc=\"      ⏳ GEO-BLEU\"):\n            results.append(score)\n    return float(np.mean(results)) if results else 0.0\n","metadata":{"execution":{"iopub.status.busy":"2025-06-10T07:14:49.600870Z","iopub.execute_input":"2025-06-10T07:14:49.601655Z","iopub.status.idle":"2025-06-10T07:14:49.610061Z","shell.execute_reply.started":"2025-06-10T07:14:49.601630Z","shell.execute_reply":"2025-06-10T07:14:49.609189Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_train_aggregates(city_code: str):\n    \"\"\"\n    Reads city_{city_code}_challengedata.csv in chunks (days 1–60) and computes:\n      - global mean (gm_x, gm_y)\n      - global mode (gmod_x, gmod_y)\n      - per_user_mean_df: DataFrame indexed by uid, columns ['x','y']\n      - per_user_mode_df: DataFrame indexed by uid, columns ['x','y']\n      - per_user_unigram_dict: Dictionary uid → Counter((x,y) → frequency)\n    \"\"\"\n    print(f\">>> Computing train aggregates for City {city_code} …\")\n\n    # Accumulators for global mean\n    total_x = 0\n    total_y = 0\n    total_count = 0\n\n    # 200×200 array for global mode counts\n    global_mode_counts = np.zeros((200, 200), dtype=np.int64)\n\n    # Per-user accumulators\n    per_user_sums = defaultdict(lambda: [0, 0, 0])   # uid → [sum_x, sum_y, count]\n    per_user_modes = defaultdict(Counter)           # uid → Counter((x,y) → freq)\n    per_user_unigrams = defaultdict(Counter)        # uid → Counter((x,y) → freq) for unigram model\n\n    path = os.path.join(DATA_DIR, f\"city_{city_code}_challengedata.csv\")\n\n    # Read the file in chunks\n    for chunk in tqdm(pd.read_csv(path, usecols=COLUMNS, dtype=DTYPES, chunksize=CHUNK_SIZE),\n                      desc=f\"  📥 Loading chunks (City {city_code})\"):\n        # Filter training portion (days 1–60)\n        train_chunk = chunk[chunk[\"d\"] <= TRAIN_DAY_MAX]\n        if train_chunk.empty:\n            continue\n\n        xs = train_chunk[\"x\"].to_numpy(dtype=np.int64)\n        ys = train_chunk[\"y\"].to_numpy(dtype=np.int64)\n\n        # Update global mean accumulators\n        total_x += xs.sum()\n        total_y += ys.sum()\n        total_count += len(train_chunk)\n\n        # Update global mode counts (zero-based indexing)\n        xi = xs - 1\n        yi = ys - 1\n        np.add.at(global_mode_counts, (xi, yi), 1)\n\n        # Update per-user sums, modes, and unigrams\n        for uid, sub in train_chunk.groupby(\"uid\"):\n            arr_x = sub[\"x\"].to_numpy(dtype=np.int64)\n            arr_y = sub[\"y\"].to_numpy(dtype=np.int64)\n            per_user_sums[uid][0] += arr_x.sum()\n            per_user_sums[uid][1] += arr_y.sum()\n            per_user_sums[uid][2] += len(sub)\n\n            coords = list(zip(sub[\"x\"], sub[\"y\"]))\n            per_user_modes[uid].update(coords)\n            per_user_unigrams[uid].update(coords)  # Same as modes for unigram frequency\n\n        del train_chunk  # free memory\n\n    # Compute global mean (rounded)\n    gm_x = int(round(total_x / total_count))\n    gm_y = int(round(total_y / total_count))\n\n    # Compute global mode from the 200×200 matrix\n    flat_idx = np.argmax(global_mode_counts)\n    gmod_x = (flat_idx // 200) + 1\n    gmod_y = (flat_idx % 200) + 1\n\n    # Build per-user mean DataFrame\n    user_mean_records = []\n    for uid, (sx, sy, cnt) in per_user_sums.items():\n        user_mean_records.append((uid, int(round(sx / cnt)), int(round(sy / cnt))))\n    per_user_mean_df = (\n        pd.DataFrame(user_mean_records, columns=[\"uid\", \"x\", \"y\"])\n          .set_index(\"uid\")\n          .astype(\"int16\")\n    )\n\n    # Build per-user mode DataFrame\n    user_mode_records = []\n    for uid, counter in per_user_modes.items():\n        (mx, my), _ = counter.most_common(1)[0]\n        user_mode_records.append((uid, int(mx), int(my)))\n    per_user_mode_df = (\n        pd.DataFrame(user_mode_records, columns=[\"uid\", \"x\", \"y\"])\n          .set_index(\"uid\")\n          .astype(\"int16\")\n    )\n\n    print(f\"    ✔ Train aggregates done: GM=({gm_x},{gm_y}), GMODE=({gmod_x},{gmod_y}), \"\n          f\"{len(per_user_mean_df)} users' means, {len(per_user_mode_df)} users' modes, \"\n          f\"{len(per_user_unigrams)} users' unigrams.\")\n    return (gm_x, gm_y), (gmod_x, gmod_y), per_user_mean_df, per_user_mode_df, dict(per_user_unigrams)\n","metadata":{"execution":{"iopub.status.busy":"2025-06-10T07:14:53.755491Z","iopub.execute_input":"2025-06-10T07:14:53.756067Z","iopub.status.idle":"2025-06-10T07:14:53.768441Z","shell.execute_reply.started":"2025-06-10T07:14:53.756040Z","shell.execute_reply":"2025-06-10T07:14:53.767559Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_test_dataframe(city_code: str) -> pd.DataFrame:\n    \"\"\"\n    Reads city_{city_code}_challengedata.csv in chunks and collects only the rows\n    where d ≥ 61 and x,y != 999. Returns a DataFrame [uid,d,t,x,y].\n    \"\"\"\n    print(f\">>> Building test DataFrame for City {city_code} …\")\n    path = os.path.join(DATA_DIR, f\"city_{city_code}_challengedata.csv\")\n    test_parts = []\n\n    for chunk in tqdm(pd.read_csv(path, usecols=COLUMNS, dtype=DTYPES, chunksize=CHUNK_SIZE),\n                      desc=f\"  📥 Loading test chunks (City {city_code})\"):\n        mask = (chunk[\"d\"] >= TEST_DAY_MIN) & (chunk[\"x\"] != MASK_VALUE) & (chunk[\"y\"] != MASK_VALUE)\n        sub = chunk.loc[mask, [\"uid\", \"d\", \"t\", \"x\", \"y\"]]\n        if not sub.empty:\n            test_parts.append(sub.copy())\n        del chunk\n\n    if test_parts:\n        test_df = pd.concat(test_parts, ignore_index=True)\n    else:\n        test_df = pd.DataFrame(columns=[\"uid\", \"d\", \"t\", \"x\", \"y\"]).astype(DTYPES)\n\n    print(f\"    ✔ Test DataFrame built: shape = {test_df.shape}\")\n    return test_df\n","metadata":{"execution":{"iopub.status.busy":"2025-06-10T07:14:59.026455Z","iopub.execute_input":"2025-06-10T07:14:59.026746Z","iopub.status.idle":"2025-06-10T07:14:59.033463Z","shell.execute_reply.started":"2025-06-10T07:14:59.026724Z","shell.execute_reply":"2025-06-10T07:14:59.032603Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_unigram_predictions(test_df: pd.DataFrame, per_user_unigram_dict: dict, \n                                gm_x: int, gm_y: int) -> pd.DataFrame:\n    \"\"\"\n    Generate predictions using unigram model for each user.\n    For each test point, sample from the user's location probability distribution.\n    Fallback to global mean for unseen users.\n    \"\"\"\n    print(f\"  • Generating Unigram predictions ...\")\n    \n    pred_unigram = test_df[[\"uid\", \"d\", \"t\"]].copy()\n    pred_unigram[\"x_pred\"] = 0\n    pred_unigram[\"y_pred\"] = 0\n    \n    # Group by user for efficient processing\n    for uid, group in tqdm(test_df.groupby(\"uid\"), desc=\"    ⏳ Unigram sampling\"):\n        if uid in per_user_unigram_dict:\n            # Get user's location distribution\n            location_counter = per_user_unigram_dict[uid]\n            locations = list(location_counter.keys())\n            frequencies = list(location_counter.values())\n            \n            # Convert frequencies to probabilities\n            total_freq = sum(frequencies)\n            probabilities = [f / total_freq for f in frequencies]\n            \n            # Sample locations for all test points of this user\n            num_samples = len(group)\n            sampled_indices = np.random.choice(len(locations), size=num_samples, p=probabilities)\n            sampled_locations = [locations[i] for i in sampled_indices]\n            \n            # Update predictions for this user\n            mask = pred_unigram[\"uid\"] == uid\n            pred_unigram.loc[mask, \"x_pred\"] = [loc[0] for loc in sampled_locations]\n            pred_unigram.loc[mask, \"y_pred\"] = [loc[1] for loc in sampled_locations]\n        else:\n            # Fallback to global mean for unseen users\n            mask = pred_unigram[\"uid\"] == uid\n            pred_unigram.loc[mask, \"x_pred\"] = gm_x\n            pred_unigram.loc[mask, \"y_pred\"] = gm_y\n    \n    return pred_unigram.astype({\"x_pred\": \"int16\", \"y_pred\": \"int16\"})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T07:15:02.663538Z","iopub.execute_input":"2025-06-10T07:15:02.663840Z","iopub.status.idle":"2025-06-10T07:15:02.672031Z","shell.execute_reply.started":"2025-06-10T07:15:02.663817Z","shell.execute_reply":"2025-06-10T07:15:02.671069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_city(city_code: str) -> dict:\n    \"\"\"\n    1. Compute train aggregates\n    2. Build test_df\n    3. Prepare gt_df\n    4. Build each baseline's pred_df (showing progress)\n    5. Evaluate GEO-BLEU (with tqdm inside evaluate_geobleu_parallel)\n    Returns dict: {baseline_name: GEO-BLEU score}.\n    \"\"\"\n    print(f\"\\n>>> Starting City {city_code}\")\n\n    # 1) Train aggregates\n    (gm_x, gm_y), (gmod_x, gmod_y), per_user_mean_df, per_user_mode_df, per_user_unigram_dict = compute_train_aggregates(city_code)\n\n    # 2) Test DataFrame\n    test_df = build_test_dataframe(city_code)\n\n    # 3) Ground-truth DataFrame\n    gt_df = test_df.rename(columns={\"x\": \"x_gt\", \"y\": \"y_gt\"})[[\"uid\", \"d\", \"t\", \"x_gt\", \"y_gt\"]]\n\n    results = {}\n\n    # 4a) Global Mean Prediction\n    print(f\"  • City {city_code} → Global Mean prediction ...\")\n    pred_gm = test_df[[\"uid\", \"d\", \"t\"]].copy()\n    pred_gm[\"x_pred\"] = gm_x\n    pred_gm[\"y_pred\"] = gm_y\n    score_gm = evaluate_geobleu_parallel(pred_gm, gt_df)\n    results[\"Global Mean\"] = round(score_gm, 5)\n    print(f\"    ✔ Global Mean GEO-BLEU = {results['Global Mean']}\")\n\n    # 4b) Global Mode Prediction\n    print(f\"  • City {city_code} → Global Mode prediction ...\")\n    pred_gmod = test_df[[\"uid\", \"d\", \"t\"]].copy()\n    pred_gmod[\"x_pred\"] = gmod_x\n    pred_gmod[\"y_pred\"] = gmod_y\n    score_gmod = evaluate_geobleu_parallel(pred_gmod, gt_df)\n    results[\"Global Mode\"] = round(score_gmod, 5)\n    print(f\"    ✔ Global Mode GEO-BLEU = {results['Global Mode']}\")\n\n    # 4c) Per-User Mean Prediction\n    print(f\"  • City {city_code} → Per-User Mean prediction ...\")\n    pred_pum = test_df[[\"uid\", \"d\", \"t\"]].copy()\n    pred_pum = pred_pum.join(per_user_mean_df, on=\"uid\", how=\"left\", rsuffix=\"_tmp\")\n    pred_pum = pred_pum.rename(columns={\"x\": \"x_pred\", \"y\": \"y_pred\"})\n    # Fallback for unseen users\n    pred_pum[\"x_pred\"] = pred_pum[\"x_pred\"].fillna(gm_x).astype(\"int16\")\n    pred_pum[\"y_pred\"] = pred_pum[\"y_pred\"].fillna(gm_y).astype(\"int16\")\n    score_pum = evaluate_geobleu_parallel(pred_pum, gt_df)\n    results[\"Per-User Mean\"] = round(score_pum, 5)\n    print(f\"    ✔ Per-User Mean GEO-BLEU = {results['Per-User Mean']}\")\n\n    # 4d) Per-User Mode Prediction\n    print(f\"  • City {city_code} → Per-User Mode prediction ...\")\n    pred_pumod = test_df[[\"uid\", \"d\", \"t\"]].copy()\n    pred_pumod = pred_pumod.join(per_user_mode_df, on=\"uid\", how=\"left\", rsuffix=\"_tmp\")\n    pred_pumod = pred_pumod.rename(columns={\"x\": \"x_pred\", \"y\": \"y_pred\"})\n    # Fallback for unseen users\n    pred_pumod[\"x_pred\"] = pred_pumod[\"x_pred\"].fillna(gmod_x).astype(\"int16\")\n    pred_pumod[\"y_pred\"] = pred_pumod[\"y_pred\"].fillna(gmod_y).astype(\"int16\")\n    score_pumod = evaluate_geobleu_parallel(pred_pumod, gt_df)\n    results[\"Per-User Mode\"] = round(score_pumod, 5)\n    print(f\"    ✔ Per-User Mode GEO-BLEU = {results['Per-User Mode']}\")\n\n    # 4e) Unigram Model Prediction\n    print(f\"  • City {city_code} → Unigram Model prediction ...\")\n    pred_unigram = generate_unigram_predictions(test_df, per_user_unigram_dict, gm_x, gm_y)\n    score_unigram = evaluate_geobleu_parallel(pred_unigram, gt_df)\n    results[\"Unigram Model\"] = round(score_unigram, 5)\n    print(f\"    ✔ Unigram Model GEO-BLEU = {results['Unigram Model']}\")\n\n    print(f\"<<< Finished City {city_code} with results: {results}\\n\")\n    return results\n","metadata":{"execution":{"iopub.status.busy":"2025-06-10T07:15:07.206702Z","iopub.execute_input":"2025-06-10T07:15:07.207011Z","iopub.status.idle":"2025-06-10T07:15:07.218623Z","shell.execute_reply.started":"2025-06-10T07:15:07.206988Z","shell.execute_reply":"2025-06-10T07:15:07.217449Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_data_coverage(city_code: str) -> dict:\n    \"\"\"\n    Analyze masked vs unmasked entries in the dataset for a given city.\n    Returns statistics about data coverage across training and test periods.\n    \"\"\"\n    print(f\">>> Analyzing data coverage for City {city_code} ...\")\n    \n    path = os.path.join(DATA_DIR, f\"city_{city_code}_challengedata.csv\")\n    \n    # Initialize counters\n    stats = {\n        'train_total': 0,\n        'train_masked': 0,\n        'train_unmasked': 0,\n        'test_total': 0,\n        'test_masked': 0,\n        'test_unmasked': 0,\n        'unique_users': set(),\n        'train_users': set(),\n        'test_users': set()\n    }\n    \n    # Process data in chunks\n    for chunk in tqdm(pd.read_csv(path, usecols=COLUMNS, dtype=DTYPES, chunksize=CHUNK_SIZE),\n                      desc=f\"  📊 Analyzing coverage (City {city_code})\"):\n        \n        # Split into train and test\n        train_chunk = chunk[chunk[\"d\"] <= TRAIN_DAY_MAX]\n        test_chunk = chunk[chunk[\"d\"] >= TEST_DAY_MIN]\n        \n        # Update unique users\n        stats['unique_users'].update(chunk['uid'].unique())\n        \n        # Training data analysis\n        if not train_chunk.empty:\n            stats['train_total'] += len(train_chunk)\n            masked_train = (train_chunk[\"x\"] == MASK_VALUE) | (train_chunk[\"y\"] == MASK_VALUE)\n            stats['train_masked'] += masked_train.sum()\n            stats['train_unmasked'] += (~masked_train).sum()\n            stats['train_users'].update(train_chunk['uid'].unique())\n        \n        # Test data analysis\n        if not test_chunk.empty:\n            stats['test_total'] += len(test_chunk)\n            masked_test = (test_chunk[\"x\"] == MASK_VALUE) | (test_chunk[\"y\"] == MASK_VALUE)\n            stats['test_masked'] += masked_test.sum()\n            stats['test_unmasked'] += (~masked_test).sum()\n            stats['test_users'].update(test_chunk['uid'].unique())\n        \n        del chunk, train_chunk, test_chunk\n    \n    # Convert sets to counts\n    stats['unique_users'] = len(stats['unique_users'])\n    stats['train_users'] = len(stats['train_users'])\n    stats['test_users'] = len(stats['test_users'])\n    \n    # Calculate percentages\n    if stats['train_total'] > 0:\n        stats['train_masked_pct'] = (stats['train_masked'] / stats['train_total']) * 100\n        stats['train_unmasked_pct'] = (stats['train_unmasked'] / stats['train_total']) * 100\n    else:\n        stats['train_masked_pct'] = stats['train_unmasked_pct'] = 0\n    \n    if stats['test_total'] > 0:\n        stats['test_masked_pct'] = (stats['test_masked'] / stats['test_total']) * 100\n        stats['test_unmasked_pct'] = (stats['test_unmasked'] / stats['test_total']) * 100\n    else:\n        stats['test_masked_pct'] = stats['test_unmasked_pct'] = 0\n    \n    return stats\n\ndef print_coverage_summary(stats: dict, city_code: str):\n    \"\"\"Print a formatted summary of data coverage statistics.\"\"\"\n    print(f\"\\n=== Data Coverage Summary for City {city_code} ===\")\n    print(f\"📊 Overall Statistics:\")\n    print(f\"   • Total unique users: {stats['unique_users']:,}\")\n    print(f\"   • Users in training: {stats['train_users']:,}\")\n    print(f\"   • Users in testing: {stats['test_users']:,}\")\n    \n    print(f\"\\n📅 Training Period (Days 1-{TRAIN_DAY_MAX}):\")\n    print(f\"   • Total entries: {stats['train_total']:,}\")\n    print(f\"   • Unmasked entries: {stats['train_unmasked']:,} ({stats['train_unmasked_pct']:.2f}%)\")\n    print(f\"   • Masked entries: {stats['train_masked']:,} ({stats['train_masked_pct']:.2f}%)\")\n    \n    print(f\"\\n🧪 Test Period (Days {TEST_DAY_MIN}+):\")\n    print(f\"   • Total entries: {stats['test_total']:,}\")\n    print(f\"   • Unmasked entries: {stats['test_unmasked']:,} ({stats['test_unmasked_pct']:.2f}%)\")\n    print(f\"   • Masked entries: {stats['test_masked']:,} ({stats['test_masked_pct']:.2f}%)\")\n    \n    if stats['test_unmasked'] > 0:\n        print(f\"\\n✅ Evaluation will be performed on {stats['test_unmasked']:,} test entries\")\n    else:\n        print(f\"\\n⚠️  No unmasked test entries found!\")\n\n# Run coverage analysis for all cities\nprint(\"=\" * 60)\nprint(\"DATA COVERAGE ANALYSIS\")\nprint(\"=\" * 60)\n\nall_coverage_stats = {}\nfor city in [\"A\", \"B\", \"C\", \"D\"]:\n    coverage_stats = analyze_data_coverage(city)\n    all_coverage_stats[city] = coverage_stats\n    print_coverage_summary(coverage_stats, city)\n    print()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T07:18:53.548902Z","iopub.execute_input":"2025-06-10T07:18:53.549704Z","iopub.status.idle":"2025-06-10T07:20:01.556270Z","shell.execute_reply.started":"2025-06-10T07:18:53.549679Z","shell.execute_reply":"2025-06-10T07:20:01.555300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"methods = [\"Global Mean\", \"Global Mode\", \"Per-User Mean\", \"Per-User Mode\", \"Unigram Model\"]\nall_scores = {method: [] for method in methods}\n\nfor city in CITIES:\n    city_scores = process_city(city)\n    for method in methods:\n        all_scores[method].append(city_scores[method])\n\ndf_results = pd.DataFrame(\n    all_scores,\n    index=[f\"City {c}\" for c in CITIES]\n).T\ndf_results[\"Average\"] = df_results.mean(axis=1)\n\nprint(\"\\n=== Final GEO-BLEU Scores ===\")\ndisplay(df_results)\n","metadata":{"execution":{"iopub.status.busy":"2025-06-10T07:20:01.557862Z","iopub.execute_input":"2025-06-10T07:20:01.558156Z","iopub.status.idle":"2025-06-10T08:41:14.299698Z","shell.execute_reply.started":"2025-06-10T07:20:01.558135Z","shell.execute_reply":"2025-06-10T08:41:14.298827Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}