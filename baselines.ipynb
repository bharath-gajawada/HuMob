{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb573507",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T07:13:41.306113Z",
     "iopub.status.busy": "2025-06-10T07:13:41.305853Z",
     "iopub.status.idle": "2025-06-10T07:13:49.986792Z",
     "shell.execute_reply": "2025-06-10T07:13:49.985400Z",
     "shell.execute_reply.started": "2025-06-10T07:13:41.306083Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for geobleu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q git+https://github.com/yahoojapan/geobleu.git tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1edce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T07:14:45.897066Z",
     "iopub.status.busy": "2025-06-10T07:14:45.896368Z",
     "iopub.status.idle": "2025-06-10T07:14:47.018313Z",
     "shell.execute_reply": "2025-06-10T07:14:47.017454Z",
     "shell.execute_reply.started": "2025-06-10T07:14:45.897023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from geobleu import calc_geobleu_single\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# Ensure floats print with 5 decimals\n",
    "pd.set_option(\"display.float_format\", \"{:.5f}\".format)\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = \"/kaggle/input/humob-data/15313913\"\n",
    "# CITIES = [\"A\", \"B\", \"C\", \"D\"]\n",
    "CITIES = [\"D\"]\n",
    "COLUMNS = [\"uid\", \"d\", \"t\", \"x\", \"y\"]\n",
    "DTYPES = {\n",
    "    \"uid\": \"int32\",\n",
    "    \"d\": \"int8\",\n",
    "    \"t\": \"int8\",\n",
    "    \"x\": \"int16\",\n",
    "    \"y\": \"int16\",\n",
    "}\n",
    "TRAIN_DAY_MAX = 60\n",
    "TEST_DAY_MIN = 61\n",
    "MASK_VALUE = 999\n",
    "CHUNK_SIZE = 500_000  # adjust as needed for memory/time\n",
    "\n",
    "# Set random seed for reproducible sampling\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8fb1fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T07:14:49.601655Z",
     "iopub.status.busy": "2025-06-10T07:14:49.600870Z",
     "iopub.status.idle": "2025-06-10T07:14:49.610061Z",
     "shell.execute_reply": "2025-06-10T07:14:49.609189Z",
     "shell.execute_reply.started": "2025-06-10T07:14:49.601630Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _geobleu_for_group(group):\n",
    "    \"\"\"\n",
    "    Given a DataFrame for one uid (with columns 'd','t','x_pred','y_pred','x_gt','y_gt'),\n",
    "    compute and return its GEO-BLEU score.\n",
    "    \"\"\"\n",
    "    pred_seq = list(zip(group['d'], group['t'], group['x_pred'], group['y_pred']))\n",
    "    true_seq = list(zip(group['d'], group['t'], group['x_gt'], group['y_gt']))\n",
    "    return calc_geobleu_single(pred_seq, true_seq)\n",
    "\n",
    "def evaluate_geobleu_parallel(pred_df: pd.DataFrame, gt_df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    - pred_df:  DataFrame with columns ['uid','d','t','x_pred','y_pred']\n",
    "    - gt_df:    DataFrame with columns ['uid','d','t','x_gt','y_gt']\n",
    "\n",
    "    Merges on ['uid','d','t'], then uses multiprocessing + tqdm to compute\n",
    "    GEO-BLEU per user in parallel. Returns the average GEO-BLEU over all users.\n",
    "    \"\"\"\n",
    "    merged = pd.merge(pred_df, gt_df, on=['uid', 'd', 't'], how='inner')\n",
    "    if merged.empty:\n",
    "        return 0.0\n",
    "\n",
    "    # Rename ground-truth x,y for readability\n",
    "    merged = merged.rename(columns={'x': 'x_gt', 'y': 'y_gt'})\n",
    "\n",
    "    # Split into list of DataFrames by uid\n",
    "    grouped = [grp for _, grp in merged.groupby('uid')]\n",
    "    num_users = len(grouped)\n",
    "    if num_users == 0:\n",
    "        return 0.0\n",
    "\n",
    "    print(f\"    ▶ Evaluating GEO-BLEU on {num_users} users...\")\n",
    "\n",
    "    # Use imap_unordered + tqdm for a progress bar\n",
    "    with mp.Pool(processes=max(1, mp.cpu_count() - 1)) as pool:\n",
    "        results = []\n",
    "        for score in tqdm(pool.imap_unordered(_geobleu_for_group, grouped),\n",
    "                          total=num_users,\n",
    "                          desc=\"      ⏳ GEO-BLEU\"):\n",
    "            results.append(score)\n",
    "    return float(np.mean(results)) if results else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d62d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T07:14:53.756067Z",
     "iopub.status.busy": "2025-06-10T07:14:53.755491Z",
     "iopub.status.idle": "2025-06-10T07:14:53.768441Z",
     "shell.execute_reply": "2025-06-10T07:14:53.767559Z",
     "shell.execute_reply.started": "2025-06-10T07:14:53.756040Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_train_aggregates(city_code: str):\n",
    "    \"\"\"\n",
    "    Reads city_{city_code}_challengedata.csv in chunks (days 1–60) and computes:\n",
    "      - global mean (gm_x, gm_y)\n",
    "      - global mode (gmod_x, gmod_y)\n",
    "      - per_user_mean_df: DataFrame indexed by uid, columns ['x','y']\n",
    "      - per_user_mode_df: DataFrame indexed by uid, columns ['x','y']\n",
    "      - per_user_unigram_dict: Dictionary uid → Counter((x,y) → frequency)\n",
    "      - per_user_bigram_dict: Dictionary uid → Counter(((x1,y1), (x2,y2)) → frequency)\n",
    "    \"\"\"\n",
    "    print(f\">>> Computing train aggregates for City {city_code} ...\")\n",
    "\n",
    "    # Accumulators for global mean\n",
    "    total_x = 0\n",
    "    total_y = 0\n",
    "    total_count = 0\n",
    "\n",
    "    # 200×200 array for global mode counts\n",
    "    global_mode_counts = np.zeros((200, 200), dtype=np.int64)\n",
    "\n",
    "    # Per-user accumulators\n",
    "    per_user_sums = defaultdict(lambda: [0, 0, 0])   # uid → [sum_x, sum_y, count]\n",
    "    per_user_modes = defaultdict(Counter)           # uid → Counter((x,y) → freq)\n",
    "    per_user_unigrams = defaultdict(Counter)        # uid → Counter((x,y) → freq) for unigram model\n",
    "    per_user_bigrams = defaultdict(Counter)         # uid → Counter(((x1,y1), (x2,y2)) → freq)\n",
    "\n",
    "    path = os.path.join(DATA_DIR, f\"city_{city_code}_challengedata.csv\")\n",
    "\n",
    "    # Read the file in chunks\n",
    "    for chunk in tqdm(pd.read_csv(path, usecols=COLUMNS, dtype=DTYPES, chunksize=CHUNK_SIZE),\n",
    "                      desc=f\"Loading chunks (City {city_code})\"):\n",
    "        # Filter training portion (days 1–60)\n",
    "        train_chunk = chunk[chunk[\"d\"] <= TRAIN_DAY_MAX]\n",
    "        if train_chunk.empty:\n",
    "            continue\n",
    "\n",
    "        xs = train_chunk[\"x\"].to_numpy(dtype=np.int64)\n",
    "        ys = train_chunk[\"y\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "        # Update global mean accumulators\n",
    "        total_x += xs.sum()\n",
    "        total_y += ys.sum()\n",
    "        total_count += len(train_chunk)\n",
    "\n",
    "        # Update global mode counts (zero-based indexing)\n",
    "        xi = xs - 1\n",
    "        yi = ys - 1\n",
    "        np.add.at(global_mode_counts, (xi, yi), 1)\n",
    "\n",
    "        # Update per-user sums, modes, unigrams, and bigrams\n",
    "        for uid, sub in train_chunk.groupby(\"uid\"):\n",
    "            arr_x = sub[\"x\"].to_numpy(dtype=np.int64)\n",
    "            arr_y = sub[\"y\"].to_numpy(dtype=np.int64)\n",
    "            per_user_sums[uid][0] += arr_x.sum()\n",
    "            per_user_sums[uid][1] += arr_y.sum()\n",
    "            per_user_sums[uid][2] += len(sub)\n",
    "\n",
    "            coords = list(zip(sub[\"x\"], sub[\"y\"]))\n",
    "            per_user_modes[uid].update(coords)\n",
    "            per_user_unigrams[uid].update(coords)\n",
    "            \n",
    "            # Build bigrams within each user's trajectory (ordered by d, t)\n",
    "            user_sub = sub.sort_values(['d', 't'])\n",
    "            user_coords = list(zip(user_sub[\"x\"], user_sub[\"y\"]))\n",
    "            if len(user_coords) > 1:\n",
    "                bigrams = [(user_coords[i], user_coords[i+1]) for i in range(len(user_coords)-1)]\n",
    "                per_user_bigrams[uid].update(bigrams)\n",
    "\n",
    "        del train_chunk  # free memory\n",
    "\n",
    "    # Compute global mean (rounded)\n",
    "    gm_x = int(round(total_x / total_count))\n",
    "    gm_y = int(round(total_y / total_count))\n",
    "\n",
    "    # Compute global mode from the 200×200 matrix\n",
    "    flat_idx = np.argmax(global_mode_counts)\n",
    "    gmod_x = (flat_idx // 200) + 1\n",
    "    gmod_y = (flat_idx % 200) + 1\n",
    "\n",
    "    # Build per-user mean DataFrame\n",
    "    user_mean_records = []\n",
    "    for uid, (sx, sy, cnt) in per_user_sums.items():\n",
    "        user_mean_records.append((uid, int(round(sx / cnt)), int(round(sy / cnt))))\n",
    "    per_user_mean_df = (\n",
    "        pd.DataFrame(user_mean_records, columns=[\"uid\", \"x\", \"y\"])\n",
    "          .set_index(\"uid\")\n",
    "          .astype(\"int16\")\n",
    "    )\n",
    "\n",
    "    # Build per-user mode DataFrame\n",
    "    user_mode_records = []\n",
    "    for uid, counter in per_user_modes.items():\n",
    "        (mx, my), _ = counter.most_common(1)[0]\n",
    "        user_mode_records.append((uid, int(mx), int(my)))\n",
    "    per_user_mode_df = (\n",
    "        pd.DataFrame(user_mode_records, columns=[\"uid\", \"x\", \"y\"])\n",
    "          .set_index(\"uid\")\n",
    "          .astype(\"int16\")\n",
    "    )\n",
    "\n",
    "    print(f\"Train aggregates done: GM=({gm_x},{gm_y}), GMODE=({gmod_x},{gmod_y}), \"\n",
    "          f\"{len(per_user_mean_df)} users' means, {len(per_user_mode_df)} users' modes, \"\n",
    "          f\"{len(per_user_unigrams)} users' unigrams, {len(per_user_bigrams)} users' bigrams.\")\n",
    "    return (gm_x, gm_y), (gmod_x, gmod_y), per_user_mean_df, per_user_mode_df, dict(per_user_unigrams), dict(per_user_bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49898ad9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T07:14:59.026746Z",
     "iopub.status.busy": "2025-06-10T07:14:59.026455Z",
     "iopub.status.idle": "2025-06-10T07:14:59.033463Z",
     "shell.execute_reply": "2025-06-10T07:14:59.032603Z",
     "shell.execute_reply.started": "2025-06-10T07:14:59.026724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_test_dataframe(city_code: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads city_{city_code}_challengedata.csv in chunks and collects only the rows\n",
    "    where d ≥ 61 and x,y != 999. Returns a DataFrame [uid,d,t,x,y].\n",
    "    \"\"\"\n",
    "    print(f\">>> Building test DataFrame for City {city_code} ...\")\n",
    "    path = os.path.join(DATA_DIR, f\"city_{city_code}_challengedata.csv\")\n",
    "    test_parts = []\n",
    "\n",
    "    for chunk in tqdm(pd.read_csv(path, usecols=COLUMNS, dtype=DTYPES, chunksize=CHUNK_SIZE),\n",
    "                      desc=f\"Loading test chunks (City {city_code})\"):\n",
    "        mask = (chunk[\"d\"] >= TEST_DAY_MIN) & (chunk[\"x\"] != MASK_VALUE) & (chunk[\"y\"] != MASK_VALUE)\n",
    "        sub = chunk.loc[mask, [\"uid\", \"d\", \"t\", \"x\", \"y\"]]\n",
    "        if not sub.empty:\n",
    "            test_parts.append(sub.copy())\n",
    "        del chunk\n",
    "\n",
    "    if test_parts:\n",
    "        test_df = pd.concat(test_parts, ignore_index=True)\n",
    "    else:\n",
    "        test_df = pd.DataFrame(columns=[\"uid\", \"d\", \"t\", \"x\", \"y\"]).astype(DTYPES)\n",
    "\n",
    "    print(f\"Test DataFrame built: shape = {test_df.shape}\")\n",
    "    return test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c986ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T07:15:02.663840Z",
     "iopub.status.busy": "2025-06-10T07:15:02.663538Z",
     "iopub.status.idle": "2025-06-10T07:15:02.672031Z",
     "shell.execute_reply": "2025-06-10T07:15:02.671069Z",
     "shell.execute_reply.started": "2025-06-10T07:15:02.663817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_unigram_predictions(test_df: pd.DataFrame, per_user_unigram_dict: dict, \n",
    "                                gm_x: int, gm_y: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate predictions using unigram model for each user.\n",
    "    For each test point, sample from the user's location probability distribution.\n",
    "    Fallback to global mean for unseen users.\n",
    "    \"\"\"\n",
    "    print(f\"Generating Unigram predictions ...\")\n",
    "    \n",
    "    pred_unigram = test_df[[\"uid\", \"d\", \"t\"]].copy()\n",
    "    pred_unigram[\"x_pred\"] = 0\n",
    "    pred_unigram[\"y_pred\"] = 0\n",
    "    \n",
    "    # Group by user for efficient processing\n",
    "    for uid, group in tqdm(test_df.groupby(\"uid\"), desc=\"Unigram sampling\"):\n",
    "        if uid in per_user_unigram_dict:\n",
    "            # Get user's location distribution\n",
    "            location_counter = per_user_unigram_dict[uid]\n",
    "            locations = list(location_counter.keys())\n",
    "            frequencies = list(location_counter.values())\n",
    "            \n",
    "            # Convert frequencies to probabilities\n",
    "            total_freq = sum(frequencies)\n",
    "            probabilities = [f / total_freq for f in frequencies]\n",
    "            \n",
    "            # Sample locations for all test points of this user\n",
    "            num_samples = len(group)\n",
    "            sampled_indices = np.random.choice(len(locations), size=num_samples, p=probabilities)\n",
    "            sampled_locations = [locations[i] for i in sampled_indices]\n",
    "            \n",
    "            # Update predictions for this user\n",
    "            mask = pred_unigram[\"uid\"] == uid\n",
    "            pred_unigram.loc[mask, \"x_pred\"] = [loc[0] for loc in sampled_locations]\n",
    "            pred_unigram.loc[mask, \"y_pred\"] = [loc[1] for loc in sampled_locations]\n",
    "        else:\n",
    "            # Fallback to global mean for unseen users\n",
    "            mask = pred_unigram[\"uid\"] == uid\n",
    "            pred_unigram.loc[mask, \"x_pred\"] = gm_x\n",
    "            pred_unigram.loc[mask, \"y_pred\"] = gm_y\n",
    "    \n",
    "    return pred_unigram.astype({\"x_pred\": \"int16\", \"y_pred\": \"int16\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a9ed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sampling(probabilities, top_p=0.7):\n",
    "    \"\"\"\n",
    "    Apply top-p (nucleus) sampling to probability distribution.\n",
    "    Returns indices and renormalized probabilities.\n",
    "    \"\"\"\n",
    "    # Sort probabilities in descending order\n",
    "    sorted_indices = np.argsort(probabilities)[::-1]\n",
    "    sorted_probs = np.array(probabilities)[sorted_indices]\n",
    "    \n",
    "    # Calculate cumulative probabilities\n",
    "    cumulative_probs = np.cumsum(sorted_probs)\n",
    "    \n",
    "    # Find cutoff point where cumulative probability exceeds top_p\n",
    "    cutoff_idx = np.searchsorted(cumulative_probs, top_p) + 1\n",
    "    cutoff_idx = min(cutoff_idx, len(sorted_probs))\n",
    "    \n",
    "    # Select top-p subset\n",
    "    selected_indices = sorted_indices[:cutoff_idx]\n",
    "    selected_probs = sorted_probs[:cutoff_idx]\n",
    "    \n",
    "    # Renormalize probabilities\n",
    "    selected_probs = selected_probs / selected_probs.sum()\n",
    "    \n",
    "    return selected_indices, selected_probs\n",
    "\n",
    "def generate_bigram_predictions(test_df: pd.DataFrame, per_user_bigram_dict: dict,\n",
    "                               per_user_unigram_dict: dict, gm_x: int, gm_y: int,\n",
    "                               top_p=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate predictions using bigram model for each user.\n",
    "    For each test point, use the previous location to predict the next location.\n",
    "    Apply top-p sampling if specified.\n",
    "    Fallback to unigram model if no bigram history, then to global mean.\n",
    "    \"\"\"\n",
    "    model_name = f\"Bigram Model (top_p={top_p})\" if top_p else \"Bigram Model\"\n",
    "    print(f\"Generating {model_name} predictions ...\")\n",
    "    \n",
    "    pred_bigram = test_df[[\"uid\", \"d\", \"t\"]].copy()\n",
    "    pred_bigram[\"x_pred\"] = 0\n",
    "    pred_bigram[\"y_pred\"] = 0\n",
    "    \n",
    "    # Process each user separately to maintain sequence order\n",
    "    for uid, group in tqdm(test_df.groupby(\"uid\"), desc=f\"{model_name} sampling\"):\n",
    "        # Sort test points by day and time to maintain sequence\n",
    "        user_test = group.sort_values(['d', 't']).copy()\n",
    "        \n",
    "        if uid in per_user_bigram_dict and per_user_bigram_dict[uid]:\n",
    "            bigram_counter = per_user_bigram_dict[uid]\n",
    "            \n",
    "            # Get the last location from training data as starting context\n",
    "            # Use the most frequent location as initial context\n",
    "            if uid in per_user_unigram_dict:\n",
    "                unigram_counter = per_user_unigram_dict[uid]\n",
    "                prev_location = unigram_counter.most_common(1)[0][0]\n",
    "            else:\n",
    "                prev_location = (gm_x, gm_y)\n",
    "            \n",
    "            predictions = []\n",
    "            \n",
    "            for idx, row in user_test.iterrows():\n",
    "                # Find all bigrams that start with prev_location\n",
    "                next_locations = {}\n",
    "                for (loc1, loc2), freq in bigram_counter.items():\n",
    "                    if loc1 == prev_location:\n",
    "                        next_locations[loc2] = freq\n",
    "                \n",
    "                if next_locations:\n",
    "                    # Sample from next locations\n",
    "                    locations = list(next_locations.keys())\n",
    "                    frequencies = list(next_locations.values())\n",
    "                    total_freq = sum(frequencies)\n",
    "                    probabilities = [f / total_freq for f in frequencies]\n",
    "                    \n",
    "                    if top_p is not None:\n",
    "                        # Apply top-p sampling\n",
    "                        selected_indices, selected_probs = top_p_sampling(probabilities, top_p)\n",
    "                        selected_locations = [locations[i] for i in selected_indices]\n",
    "                        sampled_location = np.random.choice(len(selected_locations), p=selected_probs)\n",
    "                        next_location = selected_locations[sampled_location]\n",
    "                    else:\n",
    "                        # Regular sampling\n",
    "                        sampled_idx = np.random.choice(len(locations), p=probabilities)\n",
    "                        next_location = locations[sampled_idx]\n",
    "                    \n",
    "                    predictions.append(next_location)\n",
    "                    prev_location = next_location\n",
    "                else:\n",
    "                    # Fallback to unigram model\n",
    "                    if uid in per_user_unigram_dict:\n",
    "                        unigram_counter = per_user_unigram_dict[uid]\n",
    "                        locations = list(unigram_counter.keys())\n",
    "                        frequencies = list(unigram_counter.values())\n",
    "                        total_freq = sum(frequencies)\n",
    "                        probabilities = [f / total_freq for f in frequencies]\n",
    "                        sampled_idx = np.random.choice(len(locations), p=probabilities)\n",
    "                        next_location = locations[sampled_idx]\n",
    "                    else:\n",
    "                        next_location = (gm_x, gm_y)\n",
    "                    \n",
    "                    predictions.append(next_location)\n",
    "                    prev_location = next_location\n",
    "            \n",
    "            # Update predictions for this user\n",
    "            user_indices = user_test.index\n",
    "            pred_bigram.loc[user_indices, \"x_pred\"] = [pred[0] for pred in predictions]\n",
    "            pred_bigram.loc[user_indices, \"y_pred\"] = [pred[1] for pred in predictions]\n",
    "            \n",
    "        else:\n",
    "            # Fallback to unigram model for users without bigram data\n",
    "            if uid in per_user_unigram_dict:\n",
    "                location_counter = per_user_unigram_dict[uid]\n",
    "                locations = list(location_counter.keys())\n",
    "                frequencies = list(location_counter.values())\n",
    "                total_freq = sum(frequencies)\n",
    "                probabilities = [f / total_freq for f in frequencies]\n",
    "                \n",
    "                num_samples = len(user_test)\n",
    "                sampled_indices = np.random.choice(len(locations), size=num_samples, p=probabilities)\n",
    "                sampled_locations = [locations[i] for i in sampled_indices]\n",
    "                \n",
    "                user_indices = user_test.index\n",
    "                pred_bigram.loc[user_indices, \"x_pred\"] = [loc[0] for loc in sampled_locations]\n",
    "                pred_bigram.loc[user_indices, \"y_pred\"] = [loc[1] for loc in sampled_locations]\n",
    "            else:\n",
    "                # Final fallback to global mean\n",
    "                mask = pred_bigram[\"uid\"] == uid\n",
    "                pred_bigram.loc[mask, \"x_pred\"] = gm_x\n",
    "                pred_bigram.loc[mask, \"y_pred\"] = gm_y\n",
    "    \n",
    "    return pred_bigram.astype({\"x_pred\": \"int16\", \"y_pred\": \"int16\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32639f4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T07:15:07.207011Z",
     "iopub.status.busy": "2025-06-10T07:15:07.206702Z",
     "iopub.status.idle": "2025-06-10T07:15:07.218623Z",
     "shell.execute_reply": "2025-06-10T07:15:07.217449Z",
     "shell.execute_reply.started": "2025-06-10T07:15:07.206988Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_city(city_code: str) -> dict:\n",
    "    \"\"\"\n",
    "    1. Compute train aggregates\n",
    "    2. Build test_df\n",
    "    3. Prepare gt_df\n",
    "    4. Build each baseline's pred_df (showing progress)\n",
    "    5. Evaluate GEO-BLEU (with tqdm inside evaluate_geobleu_parallel)\n",
    "    Returns dict: {baseline_name: GEO-BLEU score}.\n",
    "    \"\"\"\n",
    "    print(f\"\\n>>> Starting City {city_code}\")\n",
    "\n",
    "    # 1) Train aggregates\n",
    "    (gm_x, gm_y), (gmod_x, gmod_y), per_user_mean_df, per_user_mode_df, per_user_unigram_dict, per_user_bigram_dict = compute_train_aggregates(city_code)\n",
    "\n",
    "    # 2) Test DataFrame\n",
    "    test_df = build_test_dataframe(city_code)\n",
    "\n",
    "    # 3) Ground-truth DataFrame\n",
    "    gt_df = test_df.rename(columns={\"x\": \"x_gt\", \"y\": \"y_gt\"})[[\"uid\", \"d\", \"t\", \"x_gt\", \"y_gt\"]]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # 4a) Global Mean Prediction\n",
    "    print(f\"City {city_code} -> Global Mean prediction ...\")\n",
    "    pred_gm = test_df[[\"uid\", \"d\", \"t\"]].copy()\n",
    "    pred_gm[\"x_pred\"] = gm_x\n",
    "    pred_gm[\"y_pred\"] = gm_y\n",
    "    score_gm = evaluate_geobleu_parallel(pred_gm, gt_df)\n",
    "    results[\"Global Mean\"] = round(score_gm, 5)\n",
    "    print(f\"Global Mean GEO-BLEU = {results['Global Mean']}\")\n",
    "\n",
    "    # 4b) Global Mode Prediction\n",
    "    print(f\"City {city_code} -> Global Mode prediction ...\")\n",
    "    pred_gmod = test_df[[\"uid\", \"d\", \"t\"]].copy()\n",
    "    pred_gmod[\"x_pred\"] = gmod_x\n",
    "    pred_gmod[\"y_pred\"] = gmod_y\n",
    "    score_gmod = evaluate_geobleu_parallel(pred_gmod, gt_df)\n",
    "    results[\"Global Mode\"] = round(score_gmod, 5)\n",
    "    print(f\"Global Mode GEO-BLEU = {results['Global Mode']}\")\n",
    "\n",
    "    # 4c) Per-User Mean Prediction\n",
    "    print(f\"City {city_code} -> Per-User Mean prediction ...\")\n",
    "    pred_pum = test_df[[\"uid\", \"d\", \"t\"]].copy()\n",
    "    pred_pum = pred_pum.join(per_user_mean_df, on=\"uid\", how=\"left\", rsuffix=\"_tmp\")\n",
    "    pred_pum = pred_pum.rename(columns={\"x\": \"x_pred\", \"y\": \"y_pred\"})\n",
    "    # Fallback for unseen users\n",
    "    pred_pum[\"x_pred\"] = pred_pum[\"x_pred\"].fillna(gm_x).astype(\"int16\")\n",
    "    pred_pum[\"y_pred\"] = pred_pum[\"y_pred\"].fillna(gm_y).astype(\"int16\")\n",
    "    score_pum = evaluate_geobleu_parallel(pred_pum, gt_df)\n",
    "    results[\"Per-User Mean\"] = round(score_pum, 5)\n",
    "    print(f\"Per-User Mean GEO-BLEU = {results['Per-User Mean']}\")\n",
    "\n",
    "    # 4d) Per-User Mode Prediction\n",
    "    print(f\"City {city_code} -> Per-User Mode prediction ...\")\n",
    "    pred_pumod = test_df[[\"uid\", \"d\", \"t\"]].copy()\n",
    "    pred_pumod = pred_pumod.join(per_user_mode_df, on=\"uid\", how=\"left\", rsuffix=\"_tmp\")\n",
    "    pred_pumod = pred_pumod.rename(columns={\"x\": \"x_pred\", \"y\": \"y_pred\"})\n",
    "    # Fallback for unseen users\n",
    "    pred_pumod[\"x_pred\"] = pred_pumod[\"x_pred\"].fillna(gmod_x).astype(\"int16\")\n",
    "    pred_pumod[\"y_pred\"] = pred_pumod[\"y_pred\"].fillna(gmod_y).astype(\"int16\")\n",
    "    score_pumod = evaluate_geobleu_parallel(pred_pumod, gt_df)\n",
    "    results[\"Per-User Mode\"] = round(score_pumod, 5)\n",
    "    print(f\"Per-User Mode GEO-BLEU = {results['Per-User Mode']}\")\n",
    "\n",
    "    # 4e) Unigram Model Prediction\n",
    "    print(f\"City {city_code} -> Unigram Model prediction ...\")\n",
    "    pred_unigram = generate_unigram_predictions(test_df, per_user_unigram_dict, gm_x, gm_y)\n",
    "    score_unigram = evaluate_geobleu_parallel(pred_unigram, gt_df)\n",
    "    results[\"Unigram Model\"] = round(score_unigram, 5)\n",
    "    print(f\"Unigram Model GEO-BLEU = {results['Unigram Model']}\")\n",
    "\n",
    "    # 4f) Bigram Model Prediction\n",
    "    print(f\"City {city_code} -> Bigram Model prediction ...\")\n",
    "    pred_bigram = generate_bigram_predictions(test_df, per_user_bigram_dict, per_user_unigram_dict, gm_x, gm_y)\n",
    "    score_bigram = evaluate_geobleu_parallel(pred_bigram, gt_df)\n",
    "    results[\"Bigram Model\"] = round(score_bigram, 5)\n",
    "    print(f\"Bigram Model GEO-BLEU = {results['Bigram Model']}\")\n",
    "\n",
    "    # 4g) Bigram Model with top_p=0.7 Prediction\n",
    "    print(f\"City {city_code} -> Bigram Model (top_p=0.7) prediction ...\")\n",
    "    pred_bigram_top_p = generate_bigram_predictions(test_df, per_user_bigram_dict, per_user_unigram_dict, gm_x, gm_y, top_p=0.7)\n",
    "    score_bigram_top_p = evaluate_geobleu_parallel(pred_bigram_top_p, gt_df)\n",
    "    results[\"Bigram Model (top_p=0.7)\"] = round(score_bigram_top_p, 5)\n",
    "    print(f\"Bigram Model (top_p=0.7) GEO-BLEU = {results['Bigram Model (top_p=0.7)']}\")\n",
    "\n",
    "    print(f\"<<< Finished City {city_code} with results: {results}\\n\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2dccd9-5e58-4bdc-8f19-47d491c763b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T07:18:53.549704Z",
     "iopub.status.busy": "2025-06-10T07:18:53.548902Z",
     "iopub.status.idle": "2025-06-10T07:20:01.556270Z",
     "shell.execute_reply": "2025-06-10T07:20:01.555300Z",
     "shell.execute_reply.started": "2025-06-10T07:18:53.549679Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA COVERAGE ANALYSIS\n",
      "============================================================\n",
      ">>> Analyzing data coverage for City A ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8df06e1ae4f4ced83d21b8facaa42f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  📊 Analyzing coverage (City A): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Coverage Summary for City A ===\n",
      "📊 Overall Statistics:\n",
      "   • Total unique users: 150,000\n",
      "   • Users in training: 150,000\n",
      "   • Users in testing: 150,000\n",
      "\n",
      "📅 Training Period (Days 1-60):\n",
      "   • Total entries: 67,862,502\n",
      "   • Unmasked entries: 67,862,502 (100.00%)\n",
      "   • Masked entries: 0 (0.00%)\n",
      "\n",
      "🧪 Test Period (Days 61+):\n",
      "   • Total entries: 19,179,916\n",
      "   • Unmasked entries: 18,859,525 (98.33%)\n",
      "   • Masked entries: 320,391 (1.67%)\n",
      "\n",
      "✅ Evaluation will be performed on 18,859,525 test entries\n",
      "\n",
      ">>> Analyzing data coverage for City B ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a659c80e40e4844972ad679b4c3b71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  📊 Analyzing coverage (City B): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Coverage Summary for City B ===\n",
      "📊 Overall Statistics:\n",
      "   • Total unique users: 30,000\n",
      "   • Users in training: 30,000\n",
      "   • Users in testing: 30,000\n",
      "\n",
      "📅 Training Period (Days 1-60):\n",
      "   • Total entries: 14,194,433\n",
      "   • Unmasked entries: 14,194,433 (100.00%)\n",
      "   • Masked entries: 0 (0.00%)\n",
      "\n",
      "🧪 Test Period (Days 61+):\n",
      "   • Total entries: 4,002,560\n",
      "   • Unmasked entries: 3,627,062 (90.62%)\n",
      "   • Masked entries: 375,498 (9.38%)\n",
      "\n",
      "✅ Evaluation will be performed on 3,627,062 test entries\n",
      "\n",
      ">>> Analyzing data coverage for City C ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac71091534449d99630f1ab30ad8b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  📊 Analyzing coverage (City C): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Coverage Summary for City C ===\n",
      "📊 Overall Statistics:\n",
      "   • Total unique users: 25,000\n",
      "   • Users in training: 25,000\n",
      "   • Users in testing: 25,000\n",
      "\n",
      "📅 Training Period (Days 1-60):\n",
      "   • Total entries: 11,226,812\n",
      "   • Unmasked entries: 11,226,812 (100.00%)\n",
      "   • Masked entries: 0 (0.00%)\n",
      "\n",
      "🧪 Test Period (Days 61+):\n",
      "   • Total entries: 3,248,335\n",
      "   • Unmasked entries: 2,953,708 (90.93%)\n",
      "   • Masked entries: 294,627 (9.07%)\n",
      "\n",
      "✅ Evaluation will be performed on 2,953,708 test entries\n",
      "\n",
      ">>> Analyzing data coverage for City D ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185b590bc01c45738c38a0de7b57c1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  📊 Analyzing coverage (City D): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Coverage Summary for City D ===\n",
      "📊 Overall Statistics:\n",
      "   • Total unique users: 20,000\n",
      "   • Users in training: 20,000\n",
      "   • Users in testing: 20,000\n",
      "\n",
      "📅 Training Period (Days 1-60):\n",
      "   • Total entries: 9,358,783\n",
      "   • Unmasked entries: 9,358,783 (100.00%)\n",
      "   • Masked entries: 0 (0.00%)\n",
      "\n",
      "🧪 Test Period (Days 61+):\n",
      "   • Total entries: 2,671,295\n",
      "   • Unmasked entries: 2,361,882 (88.42%)\n",
      "   • Masked entries: 309,413 (11.58%)\n",
      "\n",
      "✅ Evaluation will be performed on 2,361,882 test entries\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_data_coverage(city_code: str) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze masked vs unmasked entries in the dataset for a given city.\n",
    "    Returns statistics about data coverage across training and test periods.\n",
    "    \"\"\"\n",
    "    print(f\">>> Analyzing data coverage for City {city_code} ...\")\n",
    "    \n",
    "    path = os.path.join(DATA_DIR, f\"city_{city_code}_challengedata.csv\")\n",
    "    \n",
    "    # Initialize counters\n",
    "    stats = {\n",
    "        'train_total': 0,\n",
    "        'train_masked': 0,\n",
    "        'train_unmasked': 0,\n",
    "        'test_total': 0,\n",
    "        'test_masked': 0,\n",
    "        'test_unmasked': 0,\n",
    "        'unique_users': set(),\n",
    "        'train_users': set(),\n",
    "        'test_users': set()\n",
    "    }\n",
    "    \n",
    "    # Process data in chunks\n",
    "    for chunk in tqdm(pd.read_csv(path, usecols=COLUMNS, dtype=DTYPES, chunksize=CHUNK_SIZE),\n",
    "                      desc=f\"Analyzing coverage (City {city_code})\"):\n",
    "        \n",
    "        # Split into train and test\n",
    "        train_chunk = chunk[chunk[\"d\"] <= TRAIN_DAY_MAX]\n",
    "        test_chunk = chunk[chunk[\"d\"] >= TEST_DAY_MIN]\n",
    "        \n",
    "        # Update unique users\n",
    "        stats['unique_users'].update(chunk['uid'].unique())\n",
    "        \n",
    "        # Training data analysis\n",
    "        if not train_chunk.empty:\n",
    "            stats['train_total'] += len(train_chunk)\n",
    "            masked_train = (train_chunk[\"x\"] == MASK_VALUE) | (train_chunk[\"y\"] == MASK_VALUE)\n",
    "            stats['train_masked'] += masked_train.sum()\n",
    "            stats['train_unmasked'] += (~masked_train).sum()\n",
    "            stats['train_users'].update(train_chunk['uid'].unique())\n",
    "        \n",
    "        # Test data analysis\n",
    "        if not test_chunk.empty:\n",
    "            stats['test_total'] += len(test_chunk)\n",
    "            masked_test = (test_chunk[\"x\"] == MASK_VALUE) | (test_chunk[\"y\"] == MASK_VALUE)\n",
    "            stats['test_masked'] += masked_test.sum()\n",
    "            stats['test_unmasked'] += (~masked_test).sum()\n",
    "            stats['test_users'].update(test_chunk['uid'].unique())\n",
    "        \n",
    "        del chunk, train_chunk, test_chunk\n",
    "    \n",
    "    # Convert sets to counts\n",
    "    stats['unique_users'] = len(stats['unique_users'])\n",
    "    stats['train_users'] = len(stats['train_users'])\n",
    "    stats['test_users'] = len(stats['test_users'])\n",
    "    \n",
    "    # Calculate percentages\n",
    "    if stats['train_total'] > 0:\n",
    "        stats['train_masked_pct'] = (stats['train_masked'] / stats['train_total']) * 100\n",
    "        stats['train_unmasked_pct'] = (stats['train_unmasked'] / stats['train_total']) * 100\n",
    "    else:\n",
    "        stats['train_masked_pct'] = stats['train_unmasked_pct'] = 0\n",
    "    \n",
    "    if stats['test_total'] > 0:\n",
    "        stats['test_masked_pct'] = (stats['test_masked'] / stats['test_total']) * 100\n",
    "        stats['test_unmasked_pct'] = (stats['test_unmasked'] / stats['test_total']) * 100\n",
    "    else:\n",
    "        stats['test_masked_pct'] = stats['test_unmasked_pct'] = 0\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def print_coverage_summary(stats: dict, city_code: str):\n",
    "    \"\"\"Print a formatted summary of data coverage statistics.\"\"\"\n",
    "    print(f\"\\n=== Data Coverage Summary for City {city_code} ===\")\n",
    "    print(f\"Overall Statistics:\")\n",
    "    print(f\"   Total unique users: {stats['unique_users']:,}\")\n",
    "    print(f\"   Users in training: {stats['train_users']:,}\")\n",
    "    print(f\"   Users in testing: {stats['test_users']:,}\")\n",
    "    \n",
    "    print(f\"\\nTraining Period (Days 1-{TRAIN_DAY_MAX}):\")\n",
    "    print(f\"   Total entries: {stats['train_total']:,}\")\n",
    "    print(f\"   Unmasked entries: {stats['train_unmasked']:,} ({stats['train_unmasked_pct']:.2f}%)\")\n",
    "    print(f\"   Masked entries: {stats['train_masked']:,} ({stats['train_masked_pct']:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nTest Period (Days {TEST_DAY_MIN}+):\")\n",
    "    print(f\"   Total entries: {stats['test_total']:,}\")\n",
    "    print(f\"   Unmasked entries: {stats['test_unmasked']:,} ({stats['test_unmasked_pct']:.2f}%)\")\n",
    "    print(f\"   Masked entries: {stats['test_masked']:,} ({stats['test_masked_pct']:.2f}%)\")\n",
    "    \n",
    "    if stats['test_unmasked'] > 0:\n",
    "        print(f\"\\nEvaluation will be performed on {stats['test_unmasked']:,} test entries\")\n",
    "    else:\n",
    "        print(f\"\\nNo unmasked test entries found!\")\n",
    "\n",
    "# Run coverage analysis for all cities\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA COVERAGE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_coverage_stats = {}\n",
    "for city in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "    coverage_stats = analyze_data_coverage(city)\n",
    "    all_coverage_stats[city] = coverage_stats\n",
    "    print_coverage_summary(coverage_stats, city)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f60ca3-2bac-44a9-8333-2504917c34d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T07:20:01.558156Z",
     "iopub.status.busy": "2025-06-10T07:20:01.557862Z",
     "iopub.status.idle": "2025-06-10T08:41:14.299698Z",
     "shell.execute_reply": "2025-06-10T08:41:14.298827Z",
     "shell.execute_reply.started": "2025-06-10T07:20:01.558135Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Starting City D\n",
      ">>> Computing train aggregates for City D …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa41988d643f4e938a5e34cfb06c713e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  📥 Loading chunks (City D): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✔ Train aggregates done: GM=(110,91), GMODE=(142,107), 20000 users' means, 20000 users' modes, 20000 users' unigrams.\n",
      ">>> Building test DataFrame for City D …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d077bbdce9ab44ccac5dfa0da62d65f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  📥 Loading test chunks (City D): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✔ Test DataFrame built: shape = (2361882, 5)\n",
      "  • City D → Global Mean prediction ...\n",
      "    ▶ Evaluating GEO-BLEU on 17000 users...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec27a9f5170412798d08b0c78c21793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "      ⏳ GEO-BLEU:   0%|          | 0/17000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✔ Global Mean GEO-BLEU = 1e-05\n",
      "  • City D → Global Mode prediction ...\n",
      "    ▶ Evaluating GEO-BLEU on 17000 users...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc0f392a0194623b27587ead972b6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "      ⏳ GEO-BLEU:   0%|          | 0/17000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✔ Global Mode GEO-BLEU = 0.00191\n",
      "  • City D → Per-User Mean prediction ...\n",
      "    ▶ Evaluating GEO-BLEU on 17000 users...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960333c349954c0abdda670b7229b3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "      ⏳ GEO-BLEU:   0%|          | 0/17000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✔ Per-User Mean GEO-BLEU = 0.02026\n",
      "  • City D → Per-User Mode prediction ...\n",
      "    ▶ Evaluating GEO-BLEU on 17000 users...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b75ab4b66804844bd4b9350c79c81d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "      ⏳ GEO-BLEU:   0%|          | 0/17000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✔ Per-User Mode GEO-BLEU = 0.08866\n",
      "  • City D → Unigram Model prediction ...\n",
      "  • Generating Unigram predictions ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c157fcee774da991f1ecb434fb88b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    ⏳ Unigram sampling:   0%|          | 0/17000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ▶ Evaluating GEO-BLEU on 17000 users...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13ae9e7edb54aa286429ee4eced4d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "      ⏳ GEO-BLEU:   0%|          | 0/17000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✔ Unigram Model GEO-BLEU = 0.04331\n",
      "<<< Finished City D with results: {'Global Mean': 1e-05, 'Global Mode': 0.00191, 'Per-User Mean': 0.02026, 'Per-User Mode': 0.08866, 'Unigram Model': 0.04331}\n",
      "\n",
      "\n",
      "=== Final GEO-BLEU Scores ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City D</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Global Mean</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Global Mode</th>\n",
       "      <td>0.00191</td>\n",
       "      <td>0.00191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Per-User Mean</th>\n",
       "      <td>0.02026</td>\n",
       "      <td>0.02026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Per-User Mode</th>\n",
       "      <td>0.08866</td>\n",
       "      <td>0.08866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unigram Model</th>\n",
       "      <td>0.04331</td>\n",
       "      <td>0.04331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City D  Average\n",
       "Global Mean   0.00001  0.00001\n",
       "Global Mode   0.00191  0.00191\n",
       "Per-User Mean 0.02026  0.02026\n",
       "Per-User Mode 0.08866  0.08866\n",
       "Unigram Model 0.04331  0.04331"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "methods = [\"Global Mean\", \"Global Mode\", \"Per-User Mean\", \"Per-User Mode\", \"Unigram Model\", \"Bigram Model\", \"Bigram Model (top_p=0.7)\"]\n",
    "all_scores = {method: [] for method in methods}\n",
    "\n",
    "for city in CITIES:\n",
    "    city_scores = process_city(city)\n",
    "    for method in methods:\n",
    "        all_scores[method].append(city_scores[method])\n",
    "\n",
    "df_results = pd.DataFrame(\n",
    "    all_scores,\n",
    "    index=[f\"City {c}\" for c in CITIES]\n",
    ").T\n",
    "df_results[\"Average\"] = df_results.mean(axis=1)\n",
    "\n",
    "print(\"\\n=== Final GEO-BLEU Scores ===\")\n",
    "display(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f913adf3-5934-427f-95a8-e732a2271821",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7587200,
     "sourceId": 12055345,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
