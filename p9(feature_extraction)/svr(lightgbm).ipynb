{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b97f967",
   "metadata": {},
   "source": [
    "**Goal:** Predict user `(x, y)` coordinates by training a separate model for each individual user, optimizing for large datasets and efficient execution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab8a09",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "* `DATA_DIR`: Root path of input CSV files.\n",
    "* `CITIES`: List of city identifiers to process (e.g., `[\"B\", \"C\"]`).\n",
    "* `MODEL_TYPE`: Machine learning model to use (`'SVR'`, `'LightGBM'`, `'XGBoost'`).\n",
    "* `SAMPLE_FRACTION_PER_CITY`: Fraction of users to sample from each city to manage data size.\n",
    "* `HOLIDAYS`: Set of specific day numbers (d) designated as holidays.\n",
    "* `INTERPOLATION_MAX_GAP_HOURS`: Maximum time gap (in hours) for linear interpolation of missing data.\n",
    "* `TEST_SIZE_FRACTION`: Proportion of prediction period data (days 61-75) reserved for testing.\n",
    "* Model-Specific Hyperparameters: `SVR_C`, `SVR_GAMMA`, `LGBM_N_ESTIMATORS`, `LGBM_LEARNING_RATE`, `LGBM_MAX_DEPTH`, `XGB_N_ESTIMATORS`, `XGB_LEARNING_RATE`, `XGB_MAX_DEPTH`, `XGB_N_JOBS`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da8ff85",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Core Pipeline \n",
    "\n",
    "The pipeline processes data city by city, and then user by user within each city.\n",
    "\n",
    "* **Initialize Global Score Aggregators:** `all_geobleu_scores`, `all_dtw_scores`, `all_predicted_data_overall`.\n",
    "\n",
    "* **For Each `city_file` in `DATA_DIR`:**\n",
    "    * **Load Data:** Read `uid`, `d`, `t`, `x`, `y` data from `city_file`.\n",
    "    * **Sample Data (if configured):** If `SAMPLE_FRACTION_PER_CITY < 1.0`, randomly select a fraction of unique `uid`s from `df_city` and keep only their data.\n",
    "    * **Memory Management:** Explicitly trigger garbage collection (`gc.collect()`).\n",
    "    * **Feature Extraction:**\n",
    "        * For each `uid` within the current `df_city`:\n",
    "            * [cite_start]**Interpolate Missing Values:** Apply linear interpolation to fill gaps in `x` and `y` coordinates to ensure trajectory continuity[cite: 28].\n",
    "            * **Generate Features:**\n",
    "                * [cite_start]Time-based: `activity_time`, `day_of_week`, `is_holiday`, `is_weekday`, `am_pm`[cite: 29, 112].\n",
    "                * [cite_start]Mobility-related: `num_moves`, `avg_travel_dist`, `std_travel_dist`, `avg_travel_angle`, `avg_speed`, `std_speed`[cite: 29, 112].\n",
    "        * Concatenate user-specific features into `features_df_city`.\n",
    "    * **Memory Management:** Delete original `df_city` and `gc.collect()`.\n",
    "\n",
    "    * **Train & Predict (Per User):**\n",
    "        * For each unique `uid` in `features_df_city`:\n",
    "            * **Prepare User Data:** Filter `features_df_city` for the current `uid`.\n",
    "            * **Split Data:** Separate user data into historical (d<=60) for general training and prediction period (d>=61) for test/validation split.\n",
    "            * **Scale Features:** Apply `StandardScaler` to training and test features.\n",
    "            * [cite_start]**Initialize Models:** Create two independent models (`model_x_user`, `model_y_user`) based on `MODEL_TYPE` and specified hyperparameters[cite: 57].\n",
    "            * **Train Models:** Fit `model_x_user` and `model_y_user` using the combined training data for the current user. Suppress model output/warnings during training for cleaner logs.\n",
    "            * **Predict:** Use `model_x_user` and `model_y_user` to predict `x_predicted` and `y_predicted` for the user's test data.\n",
    "            * **Collect Results:** Store `uid`, `d`, `t`, `x_actual`, `y_actual`, `x_predicted`, `y_predicted` for this user.\n",
    "            * **Memory Management:** Delete user-specific models and data, `gc.collect()`.\n",
    "        * Concatenate all collected actual and predicted results for the current city into `actual_test_data_city` and `predicted_test_data_city`.\n",
    "\n",
    "    * **Evaluate City Performance:**\n",
    "        * Merge `actual_test_data_city` and `predicted_test_data_city` to ensure alignment by `uid`, `d`, `t`.\n",
    "        * Prepare trajectory lists for `geobleu` library (list of `(uid, d, t, x, y)` tuples for each user's trajectory).\n",
    "        * **Calculate Metrics:** For each user's trajectory pair:\n",
    "            * [cite_start]`calc_geobleu_single()`: Calculates GEO-BLEU score[cite: 127, 129].\n",
    "            * [cite_start]`calc_dtw_single()`: Calculates DTW score[cite: 127, 132].\n",
    "        * Calculate the mean GEO-BLEU and DTW scores for the current city.\n",
    "        * Add these mean scores to `all_geobleu_scores` and `all_dtw_scores`.\n",
    "        * Store `predicted_test_data_city` into `all_predicted_data_overall`.\n",
    "\n",
    "* **Final Overall Results:**\n",
    "    * Calculate and print the `Overall Average GEO-BLEU Score` (mean of `all_geobleu_scores`).\n",
    "    * Calculate and print the `Overall Average DTW Score` (mean of `all_dtw_scores`).\n",
    "    * **Save Predictions:** Concatenate all `all_predicted_data_overall` into a single DataFrame and save it to a CSV file in `/kaggle/working/`, with a filename indicating the model, cities, and sampling fraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d9c56b",
   "metadata": {},
   "source": [
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You typically run installation commands in a separate code cell at the beginning of your notebook:\n",
    "!pip install pandas numpy scikit-learn lightgbm xgboost\n",
    "!pip install git+https://github.com/yahoojapan/geobleu.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e67b7b",
   "metadata": {},
   "source": [
    "## Import Libraries and Set User-Configurable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3357f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc # Import garbage collection module\n",
    "import sys # Import sys module for stdout redirection\n",
    "from contextlib import contextmanager # Import contextmanager for a cleaner way to redirect stdout\n",
    "import warnings # Import warnings module\n",
    "\n",
    "# Import LightGBM and XGBoost\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# IMPORTANT: To run this script in a Kaggle Notebook, ensure 'geobleu' is installed.\n",
    "#\n",
    "# You typically run installation commands in a separate code cell at the beginning of your notebook:\n",
    "# !pip install pandas numpy scikit-learn lightgbm xgboost\n",
    "# !pip install git+https://github.com/yahoojapan/geobleu.git\n",
    "#\n",
    "# If you encounter \"AttributeError: module 'geobleu' has no attribute 'geobleu'\"\n",
    "# or \"ModuleNotFoundError\", please ensure the library is installed correctly.\n",
    "\n",
    "# ==============================================================================\n",
    "# User-Configurable Parameters\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Input Data Configuration\n",
    "# Path to the directory containing your mobility data CSV files within Kaggle.\n",
    "# This path is common for competition datasets.\n",
    "DATA_DIR = \"/kaggle/input/humob-data/15313913\" # <--- USER: Set your data directory path here\n",
    "\n",
    "# List of cities to process. Files are expected to be named like 'city_B_challengedata.csv', etc.\n",
    "# If your files are just 'data.csv', set CITIES = [\"\"] and ensure DATA_DIR points directly to 'data.csv'\n",
    "CITIES = [\"B\", \"C\"] # <--- USER: Specify cities to process (e.g., [\"B\"], [\"B\", \"C\"])\n",
    "# For very large files, consider processing one city at a time, e.g., CITIES = [\"D\"]\n",
    "\n",
    "# 2. Preprocessing Parameters\n",
    "# A set of day numbers (d) that are considered holidays.\n",
    "HOLIDAYS = {} # <--- USER: Adjust holiday day numbers as needed\n",
    "\n",
    "# The maximum gap in hours for which linear interpolation will be applied.\n",
    "# Gaps larger than this will result in NaNs. (20 30-min time slots = 10 hours)\n",
    "INTERPOLATION_MAX_GAP_HOURS = 5 # <--- USER: Adjust interpolation gap\n",
    "\n",
    "# 3. Model Training and Evaluation Parameters\n",
    "# Fraction of day 61-75 data to use for testing/validation.\n",
    "TEST_SIZE_FRACTION = 0.1 # <--- USER: Adjust train/test split for prediction period\n",
    "\n",
    "# Choose the model type: 'SVR', 'LightGBM', 'XGBoost'\n",
    "MODEL_TYPE = 'LightGBM' # <--- USER: Select your desired model (e.g., 'SVR', 'LightGBM', 'XGBoost')\n",
    "\n",
    "# SVR Model Hyperparameters (only used if MODEL_TYPE is 'SVR')\n",
    "SVR_C = 100    # Regularization parameter\n",
    "SVR_GAMMA = 0.1 # Kernel coefficient\n",
    "# <--- USER: Tune SVR_C and SVR_GAMMA for SVR\n",
    "\n",
    "# LightGBM Model Hyperparameters (only used if MODEL_TYPE is 'LightGBM')\n",
    "LGBM_N_ESTIMATORS = 100\n",
    "LGBM_LEARNING_RATE = 0.1\n",
    "LGBM_MAX_DEPTH = 7\n",
    "# To suppress \"No further splits with positive gain\" warnings, set verbose=-1.\n",
    "# These warnings are common when training on small/homogeneous per-user datasets.\n",
    "# Note: For complete suppression, we will also redirect stdout/stderr during fitting.\n",
    "LGBM_VERBOSE = -1 # <--- USER: Set to 0 for info, >0 for more detail, -1 to suppress warnings\n",
    "# <--- USER: Tune LGBM hyperparameters\n",
    "\n",
    "# XGBoost Model Hyperparameters (only used if MODEL_TYPE is 'XGBoost')\n",
    "XGB_N_ESTIMATORS = 100\n",
    "XGB_LEARNING_RATE = 0.1\n",
    "XGB_MAX_DEPTH = 7\n",
    "XGB_N_JOBS = -1 # Use all available CPU cores\n",
    "# <--- USER: Tune XGB hyperparameters\n",
    "\n",
    "# 4. Data Sampling Parameters\n",
    "# Fraction of data to sample per city before feature extraction.\n",
    "# Set to 1.0 to use all data (might cause Out-Of-Memory for large files).\n",
    "# Start with a small fraction (e.g., 0.1 or 0.05) if you encounter OOM errors.\n",
    "SAMPLE_FRACTION_PER_CITY = 1 # <--- USER: Fraction of data to sample per city (e.g., 0.1 for 10%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512cad0",
   "metadata": {},
   "source": [
    "## Helper Functions for Data Loading and File Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f962d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Helper Functions for Data Loading and File Selection\n",
    "# ==============================================================================\n",
    "\n",
    "def get_input_files(data_dir, cities):\n",
    "    \"\"\"\n",
    "    Constructs a list of input CSV file paths based on the data directory and cities.\n",
    "    Assumes file naming convention like 'city_B_challengedata.csv'.\n",
    "    \"\"\"\n",
    "    input_files = []\n",
    "    for city in cities:\n",
    "        file_name = f\"city_{city}_challengedata.csv\" if city else \"data.csv\"\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            input_files.append(file_path)\n",
    "        else:\n",
    "            print(f\"Warning: File not found: {file_path}. Skipping.\")\n",
    "    return input_files\n",
    "\n",
    "# ---- Step 1: Load Data ----\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Loads mobility data from a CSV file, ensuring correct types and sorting.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df['uid'] = df['uid'].astype(int)\n",
    "    df['d'] = df['d'].astype(int)\n",
    "    df['t'] = df['t'].astype(int)\n",
    "    df['x'] = df['x'].astype(float)\n",
    "    df['y'] = df['y'].astype(float)\n",
    "    df = df.sort_values(['uid', 'd', 't'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d09c19",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca45f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 2: Feature Extraction ----\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extracts mobility and time-based features, including linear interpolation for missing data.\n",
    "    \"\"\"\n",
    "    def __init__(self, holidays, interpolation_max_gap_hours):\n",
    "        self.holidays = holidays\n",
    "        self.interpolation_max_gap_slots = int(interpolation_max_gap_hours * 2)\n",
    "\n",
    "    def _to_datetime(self, day, t):\n",
    "        \"\"\"\n",
    "        Converts day and time slot to a datetime object, accounting for the dataset's time gap.\n",
    "        \"\"\"\n",
    "        minutes_offset = int(t - 1) * 30\n",
    "        if day <= 60:\n",
    "            base_date = datetime(2025, 1, 1) # Assumes day 1 is 2025-01-01\n",
    "            return base_date + timedelta(days=int(day)-1, minutes=minutes_offset)\n",
    "        else:\n",
    "            base_date_after_gap = datetime(2025, 5, 1) # Day 61 corresponds to 2025-05-01\n",
    "            day_offset_after_gap = int(day) - 61\n",
    "            return base_date_after_gap + timedelta(days=day_offset_after_gap, minutes=minutes_offset)\n",
    "\n",
    "    def _interpolate_missing_data(self, group_df):\n",
    "        \"\"\"Applies linear interpolation to fill missing coordinates within a user's trajectory.\"\"\"\n",
    "        min_day = group_df['d'].min()\n",
    "        max_day = group_df['d'].max()\n",
    "        full_time_slots = pd.MultiIndex.from_product(\n",
    "            [range(min_day, max_day + 1), range(1, 49)], names=['d', 't']\n",
    "        )\n",
    "        full_df = pd.DataFrame(index=full_time_slots).reset_index()\n",
    "        merged_df = pd.merge(full_df, group_df[['d', 't', 'x', 'y']], on=['d', 't'], how='left')\n",
    "        merged_df['uid'] = group_df['uid'].iloc[0]\n",
    "        merged_df = merged_df.sort_values(['d', 't'])\n",
    "        merged_df['x'] = merged_df['x'].interpolate(method='linear', limit=self.interpolation_max_gap_slots, limit_direction='both')\n",
    "        merged_df['y'] = merged_df['y'].interpolate(method='linear', limit=self.interpolation_max_gap_slots, limit_direction='both')\n",
    "        merged_df.dropna(subset=['x', 'y'], inplace=True)\n",
    "        return merged_df\n",
    "\n",
    "    def extract(self, df):\n",
    "        \"\"\"\n",
    "        Extracts features for all users in the input DataFrame, applying interpolation and feature engineering.\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        df = df.sort_values(['uid', 'd', 't'])\n",
    "\n",
    "        for uid, group in df.groupby('uid'):\n",
    "            interpolated_group = self._interpolate_missing_data(group.copy())\n",
    "            if interpolated_group.empty:\n",
    "                print(f\"Warning: No valid data for uid {uid} after interpolation. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            current_group = interpolated_group.reset_index(drop=True)\n",
    "\n",
    "            # Time-based features\n",
    "            current_group['datetime'] = current_group.apply(lambda r: self._to_datetime(r['d'], r['t']), axis=1)\n",
    "            current_group['activity_time'] = (current_group['datetime'] - current_group['datetime'].iloc[0]).dt.total_seconds() / 60\n",
    "            current_group['day_of_week'] = current_group['datetime'].dt.weekday\n",
    "            current_group['is_holiday'] = current_group['d'].apply(lambda d: 1 if d in self.holidays else 0)\n",
    "            current_group['is_weekday'] = current_group['day_of_week'].apply(lambda dow: 1 if dow < 5 else 0)\n",
    "            current_group['am_pm'] = current_group['datetime'].dt.hour.apply(lambda h: 0 if h < 12 else 1)\n",
    "\n",
    "            # Mobility-related features\n",
    "            coords = current_group[['x', 'y']].values\n",
    "            moves = np.diff(coords, axis=0)\n",
    "            num_moves = len(moves)\n",
    "\n",
    "            avg_dist = 0.0\n",
    "            std_dist = 0.0\n",
    "            avg_angle = 0.0\n",
    "            avg_speed = 0.0\n",
    "            std_speed = 0.0\n",
    "\n",
    "            if num_moves > 0:\n",
    "                travel_distances = np.linalg.norm(moves, axis=1)\n",
    "                avg_dist = travel_distances.mean()\n",
    "                std_dist = travel_distances.std()\n",
    "                travel_angles = np.degrees(np.arctan2(moves[:,1], moves[:,0]))\n",
    "                avg_angle = travel_angles.mean()\n",
    "                time_interval_minutes = 30\n",
    "                travel_speeds = travel_distances / time_interval_minutes\n",
    "                avg_speed = travel_speeds.mean()\n",
    "                std_speed = travel_speeds.std()\n",
    "\n",
    "            current_group['num_moves'] = num_moves\n",
    "            current_group['avg_travel_dist'] = avg_dist\n",
    "            current_group['std_travel_dist'] = std_dist\n",
    "            current_group['avg_travel_angle'] = avg_angle\n",
    "            current_group['avg_speed'] = avg_speed\n",
    "            current_group['std_speed'] = std_speed\n",
    "\n",
    "            current_group = current_group.drop(columns=['datetime'])\n",
    "            features.append(current_group)\n",
    "\n",
    "        return pd.concat(features, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cbdb73",
   "metadata": {},
   "source": [
    "## Model Training and Prediction (Per User)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab237cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 3: Model Training and Prediction (Per User) ----\n",
    "@contextmanager\n",
    "def suppress_stdout_stderr():\n",
    "    \"\"\"A context manager to suppress both stdout and stderr.\"\"\"\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        old_stderr = sys.stderr\n",
    "        sys.stdout = devnull\n",
    "        sys.stderr = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "            sys.stderr = old_stderr\n",
    "\n",
    "def train_and_predict_model(features_df, model_type, test_size_fraction, svr_c, svr_gamma,\n",
    "                            lgbm_n_estimators, lgbm_learning_rate, lgbm_max_depth, lgbm_verbose,\n",
    "                            xgb_n_estimators, xgb_learning_rate, xgb_max_depth, xgb_n_jobs):\n",
    "    \"\"\"\n",
    "    Trains the specified model for x and y coordinates for each user independently and makes predictions.\n",
    "    \"\"\"\n",
    "    all_actual_data = []\n",
    "    all_predicted_data = []\n",
    "\n",
    "    unique_uids = features_df['uid'].unique()\n",
    "    print(f\"  Training models for {len(unique_uids)} unique users...\")\n",
    "\n",
    "    for uid in unique_uids:\n",
    "        user_features_df = features_df[features_df['uid'] == uid].copy()\n",
    "\n",
    "        # Define features (X) and targets (y) for the current user\n",
    "        feature_columns = [col for col in user_features_df.columns if col not in ['uid', 'd', 't', 'x', 'y']]\n",
    "        X_user = user_features_df[feature_columns]\n",
    "        y_x_user = user_features_df['x']\n",
    "        y_y_user = user_features_df['y']\n",
    "\n",
    "        historical_data_user = user_features_df[user_features_df['d'] <= 60]\n",
    "        prediction_period_data_user = user_features_df[user_features_df['d'] >= 61]\n",
    "\n",
    "        if prediction_period_data_user.empty:\n",
    "            continue\n",
    "\n",
    "        # Ensure enough samples for splitting, especially for small user trajectories\n",
    "        if len(prediction_period_data_user) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Ensure enough samples for stratification if needed, though for single user, stratify is not applied\n",
    "        if len(prediction_period_data_user['uid'].unique()) < 2:\n",
    "            train_pred_period_user, test_pred_period_user = train_test_split(\n",
    "                prediction_period_data_user, test_size=test_size_fraction, random_state=42\n",
    "            )\n",
    "        else: # This case is technically not hit for single user, but kept for robustness\n",
    "            train_pred_period_user, test_pred_period_user = train_test_split(\n",
    "                prediction_period_data_user, test_size=test_size_fraction, random_state=42, stratify=prediction_period_data_user['uid']\n",
    "            )\n",
    "\n",
    "        # Combine historical data with the training fraction of the prediction period data for this user\n",
    "        X_train_combined_user = pd.concat([historical_data_user[feature_columns], train_pred_period_user[feature_columns]], ignore_index=True)\n",
    "        y_x_train_combined_user = pd.concat([historical_data_user['x'], train_pred_period_user['x']], ignore_index=True)\n",
    "        y_y_train_combined_user = pd.concat([historical_data_user['y'], train_pred_period_user['y']], ignore_index=True)\n",
    "\n",
    "        X_test_user = test_pred_period_user[feature_columns]\n",
    "        y_x_test_user = test_pred_period_user['x']\n",
    "        y_y_test_user = test_pred_period_user['y']\n",
    "\n",
    "        # Skip if training data is empty after split (e.g., if all data is in test_pred_period_user)\n",
    "        if X_train_combined_user.empty or X_test_user.empty:\n",
    "            continue\n",
    "\n",
    "        # Scale features for the current user\n",
    "        scaler_X_user = StandardScaler()\n",
    "        X_train_scaled_user = scaler_X_user.fit_transform(X_train_combined_user)\n",
    "        X_test_scaled_user = scaler_X_user.transform(X_test_user)\n",
    "\n",
    "        # Initialize models based on MODEL_TYPE\n",
    "        if model_type == 'SVR':\n",
    "            model_x_user = SVR(kernel='rbf', C=svr_c, gamma=svr_gamma)\n",
    "            model_y_user = SVR(kernel='rbf', C=svr_c, gamma=svr_gamma)\n",
    "        elif model_type == 'LightGBM':\n",
    "            model_x_user = lgb.LGBMRegressor(n_estimators=lgbm_n_estimators, learning_rate=lgbm_learning_rate, max_depth=lgbm_max_depth, random_state=42, verbose=lgbm_verbose)\n",
    "            model_y_user = lgb.LGBMRegressor(n_estimators=lgbm_n_estimators, learning_rate=lgbm_learning_rate, max_depth=lgbm_max_depth, random_state=42, verbose=lgbm_verbose)\n",
    "        elif model_type == 'XGBoost':\n",
    "            model_x_user = xgb.XGBRegressor(n_estimators=xgb_n_estimators, learning_rate=xgb_learning_rate, max_depth=xgb_max_depth, n_jobs=xgb_n_jobs, random_state=42)\n",
    "            model_y_user = xgb.XGBRegressor(n_estimators=xgb_n_estimators, learning_rate=xgb_learning_rate, max_depth=xgb_max_depth, n_jobs=xgb_n_jobs, random_state=42)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown MODEL_TYPE: {model_type}. Choose 'SVR', 'LightGBM', or 'XGBoost'.\")\n",
    "\n",
    "        # Train models for the current user\n",
    "        # Suppress all output during fit for cleaner console\n",
    "        with suppress_stdout_stderr():\n",
    "            model_x_user.fit(X_train_scaled_user, y_x_train_combined_user)\n",
    "            model_y_user.fit(X_train_scaled_user, y_y_train_combined_user)\n",
    "\n",
    "        # Predict on the test set for the current user\n",
    "        y_x_pred_user = model_x_user.predict(X_test_scaled_user)\n",
    "        y_y_pred_user = model_y_user.predict(X_test_scaled_user)\n",
    "\n",
    "        # Collect actual and predicted data for the current user\n",
    "        all_actual_data.append(pd.DataFrame({\n",
    "            'uid': test_pred_period_user['uid'],\n",
    "            'd': test_pred_period_user['d'],\n",
    "            't': test_pred_period_user['t'],\n",
    "            'x_actual': y_x_test_user,\n",
    "            'y_actual': y_y_test_user\n",
    "        }).reset_index(drop=True))\n",
    "\n",
    "        all_predicted_data.append(pd.DataFrame({\n",
    "            'uid': test_pred_period_user['uid'],\n",
    "            'd': test_pred_period_user['d'],\n",
    "            't': test_pred_period_user['t'],\n",
    "            'x_predicted': y_x_pred_user,\n",
    "            'y_predicted': y_y_pred_user\n",
    "        }).reset_index(drop=True))\n",
    "        \n",
    "        # Explicitly delete models and data for current user to free memory\n",
    "        del model_x_user, model_y_user, X_user, y_x_user, y_y_user, historical_data_user, \\\n",
    "            prediction_period_data_user, train_pred_period_user, test_pred_period_user, \\\n",
    "            X_train_combined_user, y_x_train_combined_user, y_y_train_combined_user, \\\n",
    "            X_test_user, y_x_test_user, y_y_test_user, scaler_X_user, X_train_scaled_user, X_test_scaled_user, \\\n",
    "            y_x_pred_user, y_y_pred_user\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    # Concatenate all collected data from all users\n",
    "    final_actual_data = pd.concat(all_actual_data, ignore_index=True) if all_actual_data else pd.DataFrame()\n",
    "    final_predicted_data = pd.concat(all_predicted_data, ignore_index=True) if all_predicted_data else pd.DataFrame()\n",
    "\n",
    "    return final_actual_data, final_predicted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d290f26d",
   "metadata": {},
   "source": [
    "## Main Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe0495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Main Execution Function ----\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate data loading, feature extraction, model training,\n",
    "    prediction, and evaluation using GEO-BLEU and DTW, processing data city by city.\n",
    "    \"\"\"\n",
    "    # Filter out specific LightGBM warnings that might still appear\n",
    "    warnings.filterwarnings('ignore', category=UserWarning, module='lightgbm')\n",
    "    \n",
    "    # Attempt to import geobleu functions. If unsuccessful, print error and return.\n",
    "    try:\n",
    "        from geobleu import calc_geobleu_single, calc_dtw_single\n",
    "    except ImportError as e:\n",
    "        print(f\"Error: Failed to import 'geobleu' functions. {e}\")\n",
    "        print(\"Please ensure the 'geobleu' library is installed correctly in your active Python environment.\")\n",
    "        print(\"Refer to the installation instructions at the top of this script.\")\n",
    "        return # Exit main function gracefully\n",
    "\n",
    "    # Get input files based on user configuration\n",
    "    input_csv_files = get_input_files(DATA_DIR, CITIES)\n",
    "\n",
    "    if not input_csv_files:\n",
    "        print(\"No input CSV files found based on the provided DATA_DIR and CITIES. Exiting.\")\n",
    "        return\n",
    "\n",
    "    extractor = FeatureExtractor(holidays=HOLIDAYS, interpolation_max_gap_hours=INTERPOLATION_MAX_GAP_HOURS)\n",
    "\n",
    "    # Lists to store scores from each city\n",
    "    all_geobleu_scores = []\n",
    "    all_dtw_scores = []\n",
    "    all_predicted_data_overall = [] # To optionally save all predicted data from all cities\n",
    "\n",
    "    for input_csv in input_csv_files:\n",
    "        print(f\"\\n--- Processing data from: {input_csv} ---\")\n",
    "        try:\n",
    "            df = load_data(input_csv)\n",
    "\n",
    "            # Apply sampling if configured\n",
    "            if SAMPLE_FRACTION_PER_CITY < 1.0 and not df.empty:\n",
    "                original_rows = len(df)\n",
    "                # Sample by UID to keep user trajectories intact\n",
    "                unique_uids_to_sample = df['uid'].unique()\n",
    "                sampled_uids = np.random.choice(unique_uids_to_sample, size=int(len(unique_uids_to_sample) * SAMPLE_FRACTION_PER_CITY), replace=False)\n",
    "                df = df[df['uid'].isin(sampled_uids)].reset_index(drop=True)\n",
    "                print(f\"  Sampled {len(df)} rows ({SAMPLE_FRACTION_PER_CITY*100:.1f}%) from original {original_rows} rows by UID.\")\n",
    "            \n",
    "            # Explicitly clear memory before heavy processing for current city\n",
    "            gc.collect()\n",
    "\n",
    "            features_df = extractor.extract(df)\n",
    "            \n",
    "            # Clear original DataFrame to free memory\n",
    "            del df\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "            if features_df.empty:\n",
    "                print(f\"No features extracted for {input_csv}. Skipping model training for this file.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"--- Starting {MODEL_TYPE} Model Training and Prediction for {os.path.basename(input_csv)} ---\")\n",
    "            actual_test_data_city, predicted_test_data_city = train_and_predict_model(\n",
    "                features_df, MODEL_TYPE, TEST_SIZE_FRACTION, SVR_C, SVR_GAMMA,\n",
    "                LGBM_N_ESTIMATORS, LGBM_LEARNING_RATE, LGBM_MAX_DEPTH, LGBM_VERBOSE, # Pass verbose here\n",
    "                XGB_N_ESTIMATORS, XGB_LEARNING_RATE, XGB_MAX_DEPTH, XGB_N_JOBS\n",
    "            )\n",
    "            \n",
    "            # Clear features_df to free memory\n",
    "            del features_df\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "            if not actual_test_data_city.empty and not predicted_test_data_city.empty:\n",
    "                # Prepare trajectories for GEO-BLEU and DTW calculation for the current city\n",
    "                true_trajectories_list = []\n",
    "                pred_trajectories_list = []\n",
    "\n",
    "                # Merge actual and predicted data to ensure alignment by uid, d, t\n",
    "                merged_test_data = pd.merge(\n",
    "                    actual_test_data_city,\n",
    "                    predicted_test_data_city,\n",
    "                    on=['uid', 'd', 't'],\n",
    "                    how='inner' # Use inner join to only keep common (uid, d, t) points\n",
    "                ).sort_values(['uid', 'd', 't']).reset_index(drop=True)\n",
    "\n",
    "                unique_uids_in_test = merged_test_data['uid'].unique()\n",
    "\n",
    "                for uid in unique_uids_in_test:\n",
    "                    user_data = merged_test_data[merged_test_data['uid'] == uid]\n",
    "\n",
    "                    # Constructing trajectories as (uid, d, t, x, y) as required by geobleu library\n",
    "                    # Ensure coordinates are integers for grid-based calculations.\n",
    "                    true_traj = [(int(row['uid']), int(row['d']), int(row['t']), int(row['x_actual']), int(row['y_actual']))\n",
    "                                 for index, row in user_data.iterrows()]\n",
    "                    pred_traj = [(int(row['uid']), int(row['d']), int(row['t']), int(row['x_predicted']), int(row['y_predicted']))\n",
    "                                 for index, row in user_data.iterrows()]\n",
    "\n",
    "                    if true_traj and pred_traj: # Only add if trajectories are not empty\n",
    "                        true_trajectories_list.append(true_traj)\n",
    "                        pred_trajectories_list.append(pred_traj)\n",
    "                \n",
    "                if true_trajectories_list and pred_trajectories_list:\n",
    "                    current_geobleu_scores_per_traj = []\n",
    "                    current_dtw_scores_per_traj = []\n",
    "                    \n",
    "                    # Iterate through each paired trajectory and calculate individual scores\n",
    "                    for i in range(len(pred_trajectories_list)):\n",
    "                        pred_traj_single = pred_trajectories_list[i]\n",
    "                        true_traj_single = true_trajectories_list[i]\n",
    "\n",
    "                        if not pred_traj_single or not true_traj_single:\n",
    "                            continue\n",
    "\n",
    "                        current_geobleu_scores_per_traj.append(calc_geobleu_single(pred_traj_single, true_traj_single))\n",
    "                        current_dtw_scores_per_traj.append(calc_dtw_single(pred_traj_single, true_traj_single))\n",
    "                    \n",
    "                    if current_geobleu_scores_per_traj:\n",
    "                        all_geobleu_scores.append(np.mean(current_geobleu_scores_per_traj))\n",
    "                        print(f\"  GEO-BLEU Score for {os.path.basename(input_csv)}: {all_geobleu_scores[-1]:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"  No valid GEO-BLEU scores for {os.path.basename(input_csv)}.\")\n",
    "\n",
    "                    if current_dtw_scores_per_traj:\n",
    "                        all_dtw_scores.append(np.mean(current_dtw_scores_per_traj))\n",
    "                        print(f\"  DTW Score for {os.path.basename(input_csv)}: {all_dtw_scores[-1]:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"  No valid DTW scores for {os.path.basename(input_csv)}.\")\n",
    "\n",
    "                    all_predicted_data_overall.append(predicted_test_data_city)\n",
    "                else:\n",
    "                    print(f\"No valid trajectories for metrics calculation in {os.path.basename(input_csv)} after splitting.\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Model training or prediction resulted in empty data for {os.path.basename(input_csv)}. Skipping evaluation.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {input_csv}: {e}\")\n",
    "\n",
    "    # Final overall average scores\n",
    "    print(\"\\n==================================================\")\n",
    "    print(\"Overall Evaluation Results\")\n",
    "    print(\"==================================================\")\n",
    "    if all_geobleu_scores:\n",
    "        overall_avg_geobleu = np.mean(all_geobleu_scores)\n",
    "        print(f\"Overall Average GEO-BLEU Score: {overall_avg_geobleu:.4f}\")\n",
    "    else:\n",
    "        print(\"No GEO-BLEU scores calculated across all files.\")\n",
    "\n",
    "    if all_dtw_scores:\n",
    "        overall_avg_dtw = np.mean(all_dtw_scores)\n",
    "        print(f\"Overall Average DTW Score: {overall_avg_dtw:.4f}\")\n",
    "    else:\n",
    "        print(\"No DTW scores calculated across all files.\")\n",
    "\n",
    "    # Save all predicted data if any was generated\n",
    "    if all_predicted_data_overall:\n",
    "        final_predicted_df = pd.concat(all_predicted_data_overall, ignore_index=True)\n",
    "        # Construct the output file path for Kaggle\n",
    "        if len(CITIES) == 1 and CITIES[0] != \"\":\n",
    "            predicted_output_file_name = f\"predicted_mobility_{MODEL_TYPE}_{CITIES[0]}_sampled{int(SAMPLE_FRACTION_PER_CITY*100)}.csv\"\n",
    "        elif len(CITIES) == 1 and CITIES[0] == \"\": # Case for single 'data.csv'\n",
    "            predicted_output_file_name = f\"predicted_mobility_{MODEL_TYPE}_sampled{int(SAMPLE_FRACTION_PER_CITY*100)}.csv\"\n",
    "        else:\n",
    "            predicted_output_file_name = f\"combined_predicted_mobility_{MODEL_TYPE}_sampled{int(SAMPLE_FRACTION_PER_CITY*100)}.csv\"\n",
    "        \n",
    "        predicted_output_path = os.path.join(\"/kaggle/working\", predicted_output_file_name)\n",
    "        final_predicted_df.to_csv(predicted_output_path, index=False)\n",
    "        print(f\"\\nAll predicted mobility data saved to {predicted_output_path}\")\n",
    "    else:\n",
    "        print(\"\\nNo predicted mobility data generated across all files.\")\n",
    "\n",
    "    print(\"\\nAll processing, training, prediction, and evaluation complete.\")\n",
    "\n",
    "# Entry point for the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7587200,
     "sourceId": 12055345,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 38808.820494,
   "end_time": "2025-07-25T16:12:15.010426",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-25T05:25:26.189932",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
