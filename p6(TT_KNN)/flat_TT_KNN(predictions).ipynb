{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5988980",
   "metadata": {},
   "source": [
    "ALGORITHM: Flat TT-KNN for Location Prediction\n",
    "\n",
    "INPUT: Historical trajectories, test sequences with masked locations\n",
    "OUTPUT: Predicted locations for masked entries\n",
    "\n",
    "PHASE 1: Training\n",
    "FOR each user:\n",
    "    1. Filter locations by frequency (≥ τ visits)\n",
    "    2. Convert (day, time) → flat time segments\n",
    "    3. Build transition table: segment → location → [next_locations]\n",
    "    4. Store only transitions within M future segments\n",
    "\n",
    "PHASE 2: Prediction\n",
    "FOR each masked location:\n",
    "    1. Get current location and time segment\n",
    "    2. Look up possible next locations in future segments (1 to M)\n",
    "    3. Find K nearest neighbors by Euclidean distance\n",
    "    4. Return closest different location, or current if none found\n",
    "\n",
    "PHASE 3: Evaluation\n",
    "1. Sample fraction of unmasked test data\n",
    "2. Mask their locations and predict\n",
    "3. Calculate GEO-BLEU score against ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668460f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Building wheel for geobleu (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary package\n",
    "%pip install -q git+https://github.com/yahoojapan/geobleu.git tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing as mp\n",
    "from geobleu import calc_geobleu_single\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b20bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat TT-KNN configuration\n",
    "# TTKNN_VALUES = {\n",
    "#     \"TAU\": 5,\n",
    "#     \"DELTA\": 30,   # 30 minutes\n",
    "#     \"M\": 2,        # future segments\n",
    "#     \"K\": 2,        # nearest neighbors\n",
    "#     \"SAMPLE_FRAC\": 0.1  # fraction of unmasked test data to simulate prediction\n",
    "# }\n",
    "\n",
    "# Dataset setup\n",
    "DATA_DIR = \"/kaggle/input/humob-data/15313913\"\n",
    "CITIES = [\"A\"]  # Change to [\"A\", \"B\", \"C\", \"D\"] for all\n",
    "COLUMNS = [\"uid\", \"d\", \"t\", \"x\", \"y\"]\n",
    "DTYPES = {\"uid\": \"int32\", \"d\": \"int8\", \"t\": \"int8\", \"x\": \"int16\", \"y\": \"int16\"}\n",
    "TRAIN_DAY_MAX = 60\n",
    "TEST_DAY_MIN = 61\n",
    "TEST_DAY_MAX = 75  # Added upper bound for test period\n",
    "MASK_VALUE = 999\n",
    "CHUNK_SIZE = 500_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6dcc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(loc1, loc2):\n",
    "    return np.sqrt((loc1[0] - loc2[0])**2 + (loc1[1] - loc2[1])**2)\n",
    "\n",
    "def manhattan_distance(loc1, loc2):\n",
    "    return abs(loc1[0] - loc2[0]) + abs(loc1[1] - loc2[1])\n",
    "\n",
    "def chebyshev_distance(loc1, loc2):\n",
    "    return max(abs(loc1[0] - loc2[0]), abs(loc1[1] - loc2[1]))\n",
    "\n",
    "def calculate_distance(loc1, loc2, distance_type='euclidean'):\n",
    "    if distance_type == 'euclidean':\n",
    "        return euclidean_distance(loc1, loc2)\n",
    "    elif distance_type == 'manhattan':\n",
    "        return manhattan_distance(loc1, loc2)\n",
    "    elif distance_type == 'chebyshev':\n",
    "        return chebyshev_distance(loc1, loc2)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown distance type: {distance_type}\")\n",
    "\n",
    "def to_flat_segment(d, t, delta=30):\n",
    "    segments_per_day = (24 * 60) // delta\n",
    "    return d * segments_per_day + (t * 60) // delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_flat_TT_index(trajectory, tau=5, delta=30):\n",
    "    location_counts = Counter((x, y) for _, _, x, y in trajectory)\n",
    "    traj_filtered = [(d, t, x, y) for (d, t, x, y) in trajectory if location_counts[(x, y)] >= tau]\n",
    "    \n",
    "    seg_traj = [(to_flat_segment(d, t, delta), (x, y)) for d, t, x, y in traj_filtered]\n",
    "    seg_traj.sort()\n",
    "    \n",
    "    TT_index = defaultdict(lambda: defaultdict(list))\n",
    "    # Also track frequency of transitions\n",
    "    TT_freq = defaultdict(lambda: defaultdict(Counter))\n",
    "    \n",
    "    for i in range(len(seg_traj) - 1):\n",
    "        seg1, loc1 = seg_traj[i]\n",
    "        seg2, loc2 = seg_traj[i + 1]\n",
    "        if 0 < seg2 - seg1 <= 3:  # allow up to 3-segment jumps (1.5 hours for Δ=30min)\n",
    "            TT_index[seg1][loc1].append(loc2)\n",
    "            TT_freq[seg1][loc1][loc2] += 1\n",
    "\n",
    "    return TT_index, TT_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d35276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved prediction function with better frequency weighting\n",
    "def predict_next_location_flat(TT_index, TT_freq, d, t, current_loc, M=2, K=2, delta=30, distance_type='euclidean', freq_weight=0.3):\n",
    "    curr_seg = to_flat_segment(d, t, delta)\n",
    "    candidates = []\n",
    "    candidate_freqs = []\n",
    "\n",
    "    # Look ahead in future segments\n",
    "    for i in range(1, M + 1):\n",
    "        future_seg = curr_seg + i\n",
    "        if future_seg in TT_index and current_loc in TT_index[future_seg]:\n",
    "            locs = TT_index[future_seg][current_loc]\n",
    "            freqs = [TT_freq[future_seg][current_loc][loc] for loc in locs]\n",
    "            candidates.extend(locs)\n",
    "            candidate_freqs.extend(freqs)\n",
    "\n",
    "    if not candidates:\n",
    "        return current_loc\n",
    "\n",
    "    # Create unique candidates with aggregated frequencies\n",
    "    unique_candidates = {}\n",
    "    for loc, freq in zip(candidates, candidate_freqs):\n",
    "        if loc in unique_candidates:\n",
    "            unique_candidates[loc] += freq\n",
    "        else:\n",
    "            unique_candidates[loc] = freq\n",
    "\n",
    "    # Calculate weighted scores\n",
    "    scored_candidates = []\n",
    "    max_freq = max(unique_candidates.values()) if unique_candidates else 1\n",
    "    min_distance = float('inf')\n",
    "    max_distance = 0\n",
    "    \n",
    "    # First pass: calculate distance range for normalization\n",
    "    distances = {}\n",
    "    for loc in unique_candidates:\n",
    "        dist = calculate_distance(current_loc, loc, distance_type)\n",
    "        distances[loc] = dist\n",
    "        min_distance = min(min_distance, dist)\n",
    "        max_distance = max(max_distance, dist)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    distance_range = max_distance - min_distance if max_distance > min_distance else 1\n",
    "    \n",
    "    for loc, freq in unique_candidates.items():\n",
    "        distance = distances[loc]\n",
    "        \n",
    "        # Normalize both distance and frequency to [0, 1]\n",
    "        norm_distance = (distance - min_distance) / distance_range if distance_range > 0 else 0\n",
    "        norm_freq = freq / max_freq\n",
    "        \n",
    "        # Combined score: balance between distance (lower is better) and frequency (higher is better)\n",
    "        # Use exponential weighting for frequency to give more preference to frequent locations\n",
    "        freq_bonus = norm_freq ** freq_weight\n",
    "        weighted_score = norm_distance / freq_bonus  # Lower score is better\n",
    "        \n",
    "        scored_candidates.append((weighted_score, loc))\n",
    "\n",
    "    # Sort by weighted score and return best different location\n",
    "    scored_candidates.sort()\n",
    "    \n",
    "    for _, loc in scored_candidates[:K]:\n",
    "        if loc != current_loc:\n",
    "            return loc\n",
    "    return current_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatTTKNNModel:\n",
    "    def __init__(self, tau=5, delta=30, M=2, K=2, distance_type='euclidean', freq_weight=0.3):\n",
    "        self.tau = tau\n",
    "        self.delta = delta\n",
    "        self.M = M\n",
    "        self.K = K\n",
    "        self.distance_type = distance_type\n",
    "        self.freq_weight = freq_weight\n",
    "        self.index = {}\n",
    "        self.freq_index = {}\n",
    "\n",
    "    def fit(self, user_trajectories):\n",
    "        for uid, traj in tqdm(user_trajectories.items(), desc=\"Building TT indices\"):\n",
    "            formatted = [(d, t, x, y) for (x, y), (d, t) in traj]\n",
    "            self.index[uid], self.freq_index[uid] = build_flat_TT_index(formatted, self.tau, self.delta)\n",
    "\n",
    "    def predict(self, uid, d, t, current_loc):\n",
    "        if uid not in self.index:\n",
    "            return current_loc\n",
    "        return predict_next_location_flat(\n",
    "            self.index[uid], \n",
    "            self.freq_index[uid], \n",
    "            d, t, current_loc, \n",
    "            self.M, self.K, self.delta, \n",
    "            self.distance_type, \n",
    "            self.freq_weight\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d9838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(city, validation_indices=None):\n",
    "    \"\"\"Load training data from days 1-60 and unmasked data from days 61-75 (excluding validation samples)\"\"\"\n",
    "    path = f\"{DATA_DIR}/city_{city}_challengedata.csv\"\n",
    "    user_trajs = defaultdict(list)\n",
    "    \n",
    "    # Track validation indices to exclude from training\n",
    "    validation_set = set()\n",
    "    if validation_indices is not None:\n",
    "        validation_set = set(validation_indices)\n",
    "    \n",
    "    chunk_counter = 0\n",
    "    for chunk in pd.read_csv(path, usecols=COLUMNS, dtype=DTYPES, chunksize=CHUNK_SIZE):\n",
    "        # Training data from days 1-60\n",
    "        train_early = chunk[chunk[\"d\"] <= TRAIN_DAY_MAX]\n",
    "        \n",
    "        # Additional training data from days 61-75 (unmasked, not in validation)\n",
    "        test_period = chunk[(chunk[\"d\"] >= TEST_DAY_MIN) & (chunk[\"d\"] <= TEST_DAY_MAX) & (chunk[\"x\"] != MASK_VALUE)]\n",
    "        \n",
    "        # Filter out validation samples from test period data\n",
    "        if not test_period.empty and validation_indices is not None:\n",
    "            # Calculate global indices for this chunk\n",
    "            chunk_start_idx = chunk_counter * CHUNK_SIZE\n",
    "            chunk_indices = set(range(chunk_start_idx, chunk_start_idx + len(chunk)))\n",
    "            test_period_global_indices = set(test_period.index + chunk_start_idx)\n",
    "            \n",
    "            # Keep only test period data that's not in validation\n",
    "            valid_test_indices = test_period_global_indices - validation_set\n",
    "            if valid_test_indices:\n",
    "                # Convert back to local chunk indices\n",
    "                local_valid_indices = [idx - chunk_start_idx for idx in valid_test_indices if idx - chunk_start_idx < len(chunk)]\n",
    "                if local_valid_indices:\n",
    "                    train_additional = chunk.iloc[local_valid_indices]\n",
    "                else:\n",
    "                    train_additional = pd.DataFrame()\n",
    "            else:\n",
    "                train_additional = pd.DataFrame()\n",
    "        else:\n",
    "            train_additional = test_period\n",
    "        \n",
    "        # Combine training data\n",
    "        combined_train = pd.concat([train_early, train_additional], ignore_index=True)\n",
    "        \n",
    "        # Process trajectories\n",
    "        for uid, group in combined_train.groupby(\"uid\"):\n",
    "            locs = list(zip(group[\"x\"], group[\"y\"]))\n",
    "            times = list(zip(group[\"d\"], group[\"t\"]))\n",
    "            user_trajs[uid].extend(zip(locs, times))\n",
    "        \n",
    "        chunk_counter += 1\n",
    "    \n",
    "    print(f\"Training data loaded: {sum(len(traj) for traj in user_trajs.values())} total points\")\n",
    "    return dict(user_trajs)\n",
    "\n",
    "def load_validation_data(city, sample_frac=0.1, seed=42):\n",
    "    \"\"\"Load validation data: sample from days 61-75 unmasked data\"\"\"\n",
    "    path = f\"{DATA_DIR}/city_{city}_challengedata.csv\"\n",
    "    test_parts = []\n",
    "    all_indices = []\n",
    "    \n",
    "    chunk_counter = 0\n",
    "    for chunk in pd.read_csv(path, usecols=COLUMNS, dtype=DTYPES, chunksize=CHUNK_SIZE):\n",
    "        mask = (chunk[\"d\"] >= TEST_DAY_MIN) & (chunk[\"d\"] <= TEST_DAY_MAX) & (chunk[\"x\"] != MASK_VALUE)\n",
    "        valid_chunk = chunk[mask].copy()\n",
    "        \n",
    "        if not valid_chunk.empty:\n",
    "            # Store global indices\n",
    "            global_indices = valid_chunk.index + chunk_counter * CHUNK_SIZE\n",
    "            valid_chunk['global_idx'] = global_indices\n",
    "            test_parts.append(valid_chunk)\n",
    "            all_indices.extend(global_indices)\n",
    "        \n",
    "        chunk_counter += 1\n",
    "    \n",
    "    if not test_parts:\n",
    "        print(\"No unmasked test data found!\")\n",
    "        return pd.DataFrame(), pd.DataFrame(), []\n",
    "    \n",
    "    test_df = pd.concat(test_parts, ignore_index=True)\n",
    "    print(f\"Unmasked test data (days {TEST_DAY_MIN}-{TEST_DAY_MAX}): {len(test_df)} rows\")\n",
    "\n",
    "    # Sample for validation\n",
    "    np.random.seed(seed)\n",
    "    sampled_indices = np.random.choice(len(test_df), size=int(len(test_df) * sample_frac), replace=False)\n",
    "    \n",
    "    validation_df = test_df.iloc[sampled_indices].copy()\n",
    "    validation_global_indices = validation_df['global_idx'].tolist()\n",
    "    \n",
    "    # Create ground truth for validation\n",
    "    validation_gt = validation_df[[\"uid\", \"d\", \"t\", \"x_orig\", \"y_orig\"]].copy()\n",
    "    validation_gt = validation_gt.rename(columns={\"x\": \"x_orig\", \"y\": \"y_orig\"})\n",
    "    \n",
    "    # Mask the validation samples\n",
    "    validation_df[\"x\"] = MASK_VALUE\n",
    "    validation_df[\"y\"] = MASK_VALUE\n",
    "    \n",
    "    # Create remaining training data (unmasked test data not used for validation)\n",
    "    remaining_indices = set(range(len(test_df))) - set(sampled_indices)\n",
    "    remaining_df = test_df.iloc[list(remaining_indices)].copy()\n",
    "    \n",
    "    # Combine remaining data as additional test data for prediction\n",
    "    full_test_df = pd.concat([remaining_df, validation_df], ignore_index=True)\n",
    "    full_test_df = full_test_df.sort_values([\"uid\", \"d\", \"t\"]).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Validation samples: {len(validation_df)} rows\")\n",
    "    print(f\"Additional training from test period: {len(remaining_df)} rows\")\n",
    "    \n",
    "    return full_test_df, validation_gt[[\"uid\", \"d\", \"t\", \"x_orig\", \"y_orig\"]], validation_global_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bdfd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(model, test_df, user_trajs):\n",
    "    pred_df = test_df[[\"uid\", \"d\", \"t\"]].copy()\n",
    "    pred_df[\"x_pred\"] = 0\n",
    "    pred_df[\"y_pred\"] = 0\n",
    "\n",
    "    for uid, group in tqdm(test_df.groupby(\"uid\"), desc=\"Predicting\"):\n",
    "        if uid in user_trajs and user_trajs[uid]:\n",
    "            last_known = user_trajs[uid][-1][0]\n",
    "        else:\n",
    "            user_known = group[group[\"x\"] != MASK_VALUE]\n",
    "            if not user_known.empty:\n",
    "                last_known = tuple(user_known[[\"x\", \"y\"]].iloc[0])\n",
    "            else:\n",
    "                last_known = (0, 0)\n",
    "\n",
    "        current_loc = last_known\n",
    "        preds = []\n",
    "\n",
    "        for _, row in group.iterrows():\n",
    "            if row[\"x\"] == MASK_VALUE:\n",
    "                pred = model.predict(uid, row[\"d\"], row[\"t\"], current_loc)\n",
    "            else:\n",
    "                pred = (row[\"x\"], row[\"y\"])\n",
    "            preds.append(pred)\n",
    "            current_loc = pred\n",
    "\n",
    "        idxs = group.index\n",
    "        pred_df.loc[idxs, \"x_pred\"] = [p[0] for p in preds]\n",
    "        pred_df.loc[idxs, \"y_pred\"] = [p[1] for p in preds]\n",
    "\n",
    "    return pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aca49b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated configuration with improved parameters\n",
    "TTKNN_VALUES = {\n",
    "    \"TAU\": 0,      # Reduced threshold for more locations\n",
    "    \"DELTA\": 30,   # 30 minutes\n",
    "    \"M\": 3,        # Look ahead 3 segments (1.5 hours)\n",
    "    \"K\": 2,        # Consider more candidates\n",
    "}\n",
    "\n",
    "# Use only the first configuration to save memory\n",
    "DISTANCE_TYPE = 'euclidean'\n",
    "FREQ_WEIGHT = 0.5\n",
    "\n",
    "for city in CITIES:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"CITY {city} EVALUATION\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # Path to the data file\n",
    "    path = f\"{DATA_DIR}/city_{city}_challengedata.csv\"\n",
    "    \n",
    "    # Process by reading the entire dataset once and organizing by user\n",
    "    print(\"Loading and organizing data by user...\")\n",
    "    \n",
    "    # Store all unmasked (training) and masked (test) data by user\n",
    "    user_train_data = defaultdict(list)\n",
    "    user_test_data = defaultdict(list)\n",
    "    \n",
    "    # Read data in chunks to manage memory\n",
    "    chunk_count = 0\n",
    "    for chunk in pd.read_csv(path, usecols=COLUMNS, dtype=DTYPES, chunksize=CHUNK_SIZE):\n",
    "        chunk_count += 1\n",
    "        print(f\"Processing chunk {chunk_count}...\")\n",
    "        \n",
    "        # Split based ONLY on masked status (regardless of day)\n",
    "        # Training data: All unmasked data (x != MASK_VALUE) from any day\n",
    "        # Test data: All masked data (x == MASK_VALUE) from any day\n",
    "        train_chunk = chunk[chunk[\"x\"] != MASK_VALUE]\n",
    "        test_chunk = chunk[chunk[\"x\"] == MASK_VALUE]\n",
    "        \n",
    "        # Process training data and collect by user\n",
    "        for uid, group in train_chunk.groupby(\"uid\"):\n",
    "            locs = list(zip(group[\"x\"], group[\"y\"]))\n",
    "            times = list(zip(group[\"d\"], group[\"t\"]))\n",
    "            user_train_data[uid].extend(zip(locs, times))\n",
    "        \n",
    "        # Collect test data by user (ONLY masked data)\n",
    "        for uid, group in test_chunk.groupby(\"uid\"):\n",
    "            user_test_data[uid].append(group)\n",
    "    \n",
    "    print(f\"Training data loaded: {sum(len(traj) for traj in user_train_data.values())} points from all days\")\n",
    "    print(f\"Test data collected for {len(user_test_data)} users from all days\")\n",
    "    \n",
    "    # Initialize empty list for all predictions\n",
    "    all_predictions = []\n",
    "    \n",
    "    # Improved user-by-user processing to build models and predict on-the-fly\n",
    "    print(\"Building models and making predictions user-by-user...\")\n",
    "    \n",
    "    # Process each user with test data to predict\n",
    "    user_count = 0\n",
    "    total_users = len(user_test_data)\n",
    "    \n",
    "    for uid in tqdm(user_test_data.keys(), desc=\"Processing users\"):\n",
    "        user_count += 1\n",
    "        \n",
    "        # Skip users with no test data (shouldn't happen since we're iterating over test data keys)\n",
    "        if len(user_test_data[uid]) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Combine all test data chunks for this user\n",
    "        test_df = pd.concat(user_test_data[uid], ignore_index=True)\n",
    "        \n",
    "        # EXPLICIT CHECK: Make sure we're only working with masked data\n",
    "        test_df = test_df[test_df[\"x\"] == MASK_VALUE]\n",
    "        \n",
    "        # Skip if there's no masked data for this user\n",
    "        if test_df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Build model for this user if they have training data\n",
    "        if uid in user_train_data and user_train_data[uid]:\n",
    "            # Format training data and build indices for this user\n",
    "            formatted = [(d, t, x, y) for (x, y), (d, t) in user_train_data[uid]]\n",
    "            user_index, user_freq_index = build_flat_TT_index(\n",
    "                formatted,\n",
    "                tau=TTKNN_VALUES[\"TAU\"],\n",
    "                delta=TTKNN_VALUES[\"DELTA\"]\n",
    "            )\n",
    "            \n",
    "            # Get last known location\n",
    "            last_known = user_train_data[uid][-1][0]\n",
    "        else:\n",
    "            # No training data, initialize empty indices and default location\n",
    "            user_index = defaultdict(lambda: defaultdict(list))\n",
    "            user_freq_index = defaultdict(lambda: defaultdict(Counter))\n",
    "            last_known = (0, 0)\n",
    "        \n",
    "        # Make predictions for this user\n",
    "        current_loc = last_known\n",
    "        \n",
    "        # Sort test data by day and time to ensure sequential prediction\n",
    "        test_rows_sorted = test_df.sort_values([\"d\", \"t\"])\n",
    "        \n",
    "        # Predict each masked location\n",
    "        for _, row in test_rows_sorted.iterrows():\n",
    "            d, t = row[\"d\"], row[\"t\"]\n",
    "            \n",
    "            # Predict using this user's model\n",
    "            pred = predict_next_location_flat(\n",
    "                user_index, \n",
    "                user_freq_index,\n",
    "                d, t, current_loc,\n",
    "                M=TTKNN_VALUES[\"M\"],\n",
    "                K=TTKNN_VALUES[\"K\"],\n",
    "                delta=TTKNN_VALUES[\"DELTA\"],\n",
    "                distance_type=DISTANCE_TYPE,\n",
    "                freq_weight=FREQ_WEIGHT\n",
    "            )\n",
    "            \n",
    "            # Add prediction to results (saving ONLY masked data predictions)\n",
    "            all_predictions.append({\n",
    "                \"uid\": uid,\n",
    "                \"d\": d,\n",
    "                \"t\": t,\n",
    "                \"x\": pred[0],\n",
    "                \"y\": pred[1]\n",
    "            })\n",
    "            \n",
    "            # Update current location for next prediction\n",
    "            current_loc = pred\n",
    "        \n",
    "        # Report progress periodically\n",
    "        if user_count % 100 == 0 or user_count == total_users:\n",
    "            print(f\"Processed {user_count}/{total_users} users, {len(all_predictions)} predictions so far\")\n",
    "        \n",
    "        # Clear memory for this user\n",
    "        del test_df, user_index, user_freq_index\n",
    "    \n",
    "    # Convert all predictions to dataframe\n",
    "    if all_predictions:\n",
    "        pred_df = pd.DataFrame(all_predictions)\n",
    "        print(f\"Total predictions: {len(pred_df)}\")\n",
    "        \n",
    "        # Save predictions in same format as input\n",
    "        output_file = f\"city_{city}_ttknn_predictions.csv\"\n",
    "        pred_df.to_csv(output_file, index=False)\n",
    "        print(f\"Predictions saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No predictions made!\")\n",
    "\n",
    "# All metric calculation code is commented out"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
