{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12055345,"sourceType":"datasetVersion","datasetId":7587200}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"eb573507","cell_type":"code","source":"%pip install -q git+https://github.com/yahoojapan/geobleu.git tqdm\n","metadata":{"execution":{"iopub.status.busy":"2025-06-12T12:32:18.774107Z","iopub.execute_input":"2025-06-12T12:32:18.774392Z","iopub.status.idle":"2025-06-12T12:32:27.747587Z","shell.execute_reply.started":"2025-06-12T12:32:18.774370Z","shell.execute_reply":"2025-06-12T12:32:27.746181Z"},"trusted":true},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for geobleu (setup.py) ... \u001b[?25l\u001b[?25hdone\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"id":"21b1edce","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport multiprocessing as mp\nfrom geobleu import calc_geobleu_single\nfrom collections import Counter, defaultdict\nfrom tqdm.auto import tqdm\nimport os\n\n# Ensure floats print with 5 decimals\npd.set_option(\"display.float_format\", \"{:.5f}\".format)\n\n# Constants\nDATA_DIR = \"/kaggle/input/humob-data/15313913\"\n# CITIES = [\"A\", \"B\", \"C\", \"D\"]\nCITIES = [\"B\", \"C\", \"D\"]\nCOLUMNS = [\"uid\", \"d\", \"t\", \"x\", \"y\"]\nDTYPES = {\n    \"uid\": \"int32\",\n    \"d\": \"int8\",\n    \"t\": \"int8\",\n    \"x\": \"int16\",\n    \"y\": \"int16\",\n}\nTRAIN_DAY_MAX = 60\nTEST_DAY_MIN = 61\nMASK_VALUE = 999\nCHUNK_SIZE = 500_000  # adjust as needed for memory/time\n\n# Set random seed for reproducible sampling\nnp.random.seed(42)\n","metadata":{"execution":{"iopub.status.busy":"2025-06-12T12:32:27.749946Z","iopub.execute_input":"2025-06-12T12:32:27.750327Z","iopub.status.idle":"2025-06-12T12:32:28.922867Z","shell.execute_reply.started":"2025-06-12T12:32:27.750290Z","shell.execute_reply":"2025-06-12T12:32:28.922039Z"},"trusted":true},"outputs":[],"execution_count":2},{"id":"db8fb1fa","cell_type":"code","source":"def _geobleu_for_group(group):\n    \"\"\"\n    Given a DataFrame for one uid (with columns 'd','t','x_pred','y_pred','x_gt','y_gt'),\n    compute and return its GEO-BLEU score.\n    \"\"\"\n    pred_seq = list(zip(group['d'], group['t'], group['x_pred'], group['y_pred']))\n    true_seq = list(zip(group['d'], group['t'], group['x_gt'], group['y_gt']))\n    return calc_geobleu_single(pred_seq, true_seq)\n\ndef evaluate_geobleu_parallel(pred_df: pd.DataFrame, gt_df: pd.DataFrame) -> float:\n    \"\"\"\n    - pred_df:  DataFrame with columns ['uid','d','t','x_pred','y_pred']\n    - gt_df:    DataFrame with columns ['uid','d','t','x_gt','y_gt']\n\n    Merges on ['uid','d','t'], then uses multiprocessing + tqdm to compute\n    GEO-BLEU per user in parallel. Returns the average GEO-BLEU over all users.\n    \"\"\"\n    merged = pd.merge(pred_df, gt_df, on=['uid', 'd', 't'], how='inner')\n    if merged.empty:\n        return 0.0\n\n    # Rename ground-truth x,y for readability\n    merged = merged.rename(columns={'x': 'x_gt', 'y': 'y_gt'})\n\n    # Split into list of DataFrames by uid\n    grouped = [grp for _, grp in merged.groupby('uid')]\n    num_users = len(grouped)\n    if num_users == 0:\n        return 0.0\n\n    print(f\"    ▶ Evaluating GEO-BLEU on {num_users} users...\")\n\n    # Use imap_unordered + tqdm for a progress bar\n    with mp.Pool(processes=max(1, mp.cpu_count() - 1)) as pool:\n        results = []\n        for score in tqdm(pool.imap_unordered(_geobleu_for_group, grouped),\n                          total=num_users,\n                          desc=\"      GEO-BLEU\"):\n            results.append(score)\n    return float(np.mean(results)) if results else 0.0\n","metadata":{"execution":{"iopub.status.busy":"2025-06-12T12:32:28.923725Z","iopub.execute_input":"2025-06-12T12:32:28.924208Z","iopub.status.idle":"2025-06-12T12:32:28.935201Z","shell.execute_reply.started":"2025-06-12T12:32:28.924173Z","shell.execute_reply":"2025-06-12T12:32:28.934250Z"},"trusted":true},"outputs":[],"execution_count":3},{"id":"150d62d7","cell_type":"code","source":"def compute_train_aggregates(city_code: str):\n    \"\"\"\n    Reads city_{city_code}_challengedata.csv in chunks (days 1–60) and computes:\n      - global mean (gm_x, gm_y)\n      - global mode (gmod_x, gmod_y)\n      - per_user_mean_df: DataFrame indexed by uid, columns ['x','y']\n      - per_user_mode_df: DataFrame indexed by uid, columns ['x','y']\n      - per_user_unigram_dict: Dictionary uid → Counter((x,y) → frequency)\n      - per_user_bigram_dict: Dictionary uid → Counter(((x1,y1), (x2,y2)) → frequency)\n    \"\"\"\n    print(f\">>> Computing train aggregates for City {city_code} ...\")\n\n    # Accumulators for global mean\n    total_x = 0\n    total_y = 0\n    total_count = 0\n\n    # 200×200 array for global mode counts\n    global_mode_counts = np.zeros((200, 200), dtype=np.int64)\n\n    # Per-user accumulators\n    per_user_sums = defaultdict(lambda: [0, 0, 0])   # uid → [sum_x, sum_y, count]\n    per_user_modes = defaultdict(Counter)           # uid → Counter((x,y) → freq)\n    per_user_unigrams = defaultdict(Counter)        # uid → Counter((x,y) → freq) for unigram model\n    per_user_bigrams = defaultdict(Counter)         # uid → Counter(((x1,y1), (x2,y2)) → freq)\n\n    path = os.path.join(DATA_DIR, f\"city_{city_code}_challengedata.csv\")\n\n    # Read the file in chunks\n    for chunk in tqdm(pd.read_csv(path, usecols=COLUMNS, dtype=DTYPES, chunksize=CHUNK_SIZE),\n                      desc=f\"Loading chunks (City {city_code})\"):\n        # Filter training portion (days 1–60)\n        train_chunk = chunk[chunk[\"d\"] <= TRAIN_DAY_MAX]\n        if train_chunk.empty:\n            continue\n\n        xs = train_chunk[\"x\"].to_numpy(dtype=np.int64)\n        ys = train_chunk[\"y\"].to_numpy(dtype=np.int64)\n\n        # Update global mean accumulators\n        total_x += xs.sum()\n        total_y += ys.sum()\n        total_count += len(train_chunk)\n\n        # Update global mode counts (zero-based indexing)\n        xi = xs - 1\n        yi = ys - 1\n        np.add.at(global_mode_counts, (xi, yi), 1)\n\n        # Update per-user sums, modes, unigrams, and bigrams\n        for uid, sub in train_chunk.groupby(\"uid\"):\n            arr_x = sub[\"x\"].to_numpy(dtype=np.int64)\n            arr_y = sub[\"y\"].to_numpy(dtype=np.int64)\n            per_user_sums[uid][0] += arr_x.sum()\n            per_user_sums[uid][1] += arr_y.sum()\n            per_user_sums[uid][2] += len(sub)\n\n            coords = list(zip(sub[\"x\"], sub[\"y\"]))\n            per_user_modes[uid].update(coords)\n            per_user_unigrams[uid].update(coords)\n            \n            # Build bigrams within each user's trajectory (ordered by d, t)\n            user_sub = sub.sort_values(['d', 't'])\n            user_coords = list(zip(user_sub[\"x\"], user_sub[\"y\"]))\n            if len(user_coords) > 1:\n                bigrams = [(user_coords[i], user_coords[i+1]) for i in range(len(user_coords)-1)]\n                per_user_bigrams[uid].update(bigrams)\n\n        del train_chunk  # free memory\n\n    # Compute global mean (rounded)\n    gm_x = int(round(total_x / total_count))\n    gm_y = int(round(total_y / total_count))\n\n    # Compute global mode from the 200×200 matrix\n    flat_idx = np.argmax(global_mode_counts)\n    gmod_x = (flat_idx // 200) + 1\n    gmod_y = (flat_idx % 200) + 1\n\n    # Build per-user mean DataFrame\n    user_mean_records = []\n    for uid, (sx, sy, cnt) in per_user_sums.items():\n        user_mean_records.append((uid, int(round(sx / cnt)), int(round(sy / cnt))))\n    per_user_mean_df = (\n        pd.DataFrame(user_mean_records, columns=[\"uid\", \"x\", \"y\"])\n          .set_index(\"uid\")\n          .astype(\"int16\")\n    )\n\n    # Build per-user mode DataFrame\n    user_mode_records = []\n    for uid, counter in per_user_modes.items():\n        (mx, my), _ = counter.most_common(1)[0]\n        user_mode_records.append((uid, int(mx), int(my)))\n    per_user_mode_df = (\n        pd.DataFrame(user_mode_records, columns=[\"uid\", \"x\", \"y\"])\n          .set_index(\"uid\")\n          .astype(\"int16\")\n    )\n\n    print(f\"Train aggregates done: GM=({gm_x},{gm_y}), GMODE=({gmod_x},{gmod_y}), \"\n          f\"{len(per_user_mean_df)} users' means, {len(per_user_mode_df)} users' modes, \"\n          f\"{len(per_user_unigrams)} users' unigrams, {len(per_user_bigrams)} users' bigrams.\")\n    return (gm_x, gm_y), (gmod_x, gmod_y), per_user_mean_df, per_user_mode_df, dict(per_user_unigrams), dict(per_user_bigrams)\n","metadata":{"execution":{"iopub.status.busy":"2025-06-12T12:32:28.937659Z","iopub.execute_input":"2025-06-12T12:32:28.938015Z","iopub.status.idle":"2025-06-12T12:32:28.973751Z","shell.execute_reply.started":"2025-06-12T12:32:28.937988Z","shell.execute_reply":"2025-06-12T12:32:28.972881Z"},"trusted":true},"outputs":[],"execution_count":4},{"id":"49898ad9","cell_type":"code","source":"def build_test_dataframe(city_code: str) -> pd.DataFrame:\n    \"\"\"\n    Reads city_{city_code}_challengedata.csv in chunks and collects only the rows\n    where d ≥ 61 and x,y != 999. Returns a DataFrame [uid,d,t,x,y].\n    \"\"\"\n    print(f\">>> Building test DataFrame for City {city_code} ...\")\n    path = os.path.join(DATA_DIR, f\"city_{city_code}_challengedata.csv\")\n    test_parts = []\n\n    for chunk in tqdm(pd.read_csv(path, usecols=COLUMNS, dtype=DTYPES, chunksize=CHUNK_SIZE),\n                      desc=f\"Loading test chunks (City {city_code})\"):\n        mask = (chunk[\"d\"] >= TEST_DAY_MIN) & (chunk[\"x\"] != MASK_VALUE) & (chunk[\"y\"] != MASK_VALUE)\n        sub = chunk.loc[mask, [\"uid\", \"d\", \"t\", \"x\", \"y\"]]\n        if not sub.empty:\n            test_parts.append(sub.copy())\n        del chunk\n\n    if test_parts:\n        test_df = pd.concat(test_parts, ignore_index=True)\n    else:\n        test_df = pd.DataFrame(columns=[\"uid\", \"d\", \"t\", \"x\", \"y\"]).astype(DTYPES)\n\n    print(f\"Test DataFrame built: shape = {test_df.shape}\")\n    return test_df\n","metadata":{"execution":{"iopub.status.busy":"2025-06-12T12:32:29.921955Z","iopub.execute_input":"2025-06-12T12:32:29.922314Z","iopub.status.idle":"2025-06-12T12:32:29.929417Z","shell.execute_reply.started":"2025-06-12T12:32:29.922289Z","shell.execute_reply":"2025-06-12T12:32:29.928560Z"},"trusted":true},"outputs":[],"execution_count":5},{"id":"95c986ae","cell_type":"code","source":"def generate_unigram_predictions(test_df: pd.DataFrame, per_user_unigram_dict: dict, \n                                gm_x: int, gm_y: int) -> pd.DataFrame:\n    \"\"\"\n    Generate predictions using unigram model for each user.\n    For each test point, sample from the user's location probability distribution.\n    Fallback to global mean for unseen users.\n    \"\"\"\n    print(f\"Generating Unigram predictions ...\")\n    \n    pred_unigram = test_df[[\"uid\", \"d\", \"t\"]].copy()\n    pred_unigram[\"x_pred\"] = 0\n    pred_unigram[\"y_pred\"] = 0\n    \n    # Group by user for efficient processing\n    for uid, group in tqdm(test_df.groupby(\"uid\"), desc=\"Unigram sampling\"):\n        if uid in per_user_unigram_dict:\n            # Get user's location distribution\n            location_counter = per_user_unigram_dict[uid]\n            locations = list(location_counter.keys())\n            frequencies = list(location_counter.values())\n            \n            # Convert frequencies to probabilities\n            total_freq = sum(frequencies)\n            probabilities = [f / total_freq for f in frequencies]\n            \n            # Sample locations for all test points of this user\n            num_samples = len(group)\n            sampled_indices = np.random.choice(len(locations), size=num_samples, p=probabilities)\n            sampled_locations = [locations[i] for i in sampled_indices]\n            \n            # Update predictions for this user\n            mask = pred_unigram[\"uid\"] == uid\n            pred_unigram.loc[mask, \"x_pred\"] = [loc[0] for loc in sampled_locations]\n            pred_unigram.loc[mask, \"y_pred\"] = [loc[1] for loc in sampled_locations]\n        else:\n            # Fallback to global mean for unseen users\n            mask = pred_unigram[\"uid\"] == uid\n            pred_unigram.loc[mask, \"x_pred\"] = gm_x\n            pred_unigram.loc[mask, \"y_pred\"] = gm_y\n    \n    return pred_unigram.astype({\"x_pred\": \"int16\", \"y_pred\": \"int16\"})\n","metadata":{"execution":{"iopub.status.busy":"2025-06-12T12:32:34.414254Z","iopub.execute_input":"2025-06-12T12:32:34.415192Z","iopub.status.idle":"2025-06-12T12:32:34.427231Z","shell.execute_reply.started":"2025-06-12T12:32:34.415148Z","shell.execute_reply":"2025-06-12T12:32:34.426088Z"},"trusted":true},"outputs":[],"execution_count":6},{"id":"12a9ed5c","cell_type":"code","source":"def top_p_sampling(probabilities, top_p=0.7):\n    \"\"\"\n    Apply top-p (nucleus) sampling to probability distribution.\n    Returns indices and renormalized probabilities.\n    \"\"\"\n    # Sort probabilities in descending order\n    sorted_indices = np.argsort(probabilities)[::-1]\n    sorted_probs = np.array(probabilities)[sorted_indices]\n    \n    # Calculate cumulative probabilities\n    cumulative_probs = np.cumsum(sorted_probs)\n    \n    # Find cutoff point where cumulative probability exceeds top_p\n    cutoff_idx = np.searchsorted(cumulative_probs, top_p) + 1\n    cutoff_idx = min(cutoff_idx, len(sorted_probs))\n    \n    # Select top-p subset\n    selected_indices = sorted_indices[:cutoff_idx]\n    selected_probs = sorted_probs[:cutoff_idx]\n    \n    # Renormalize probabilities\n    selected_probs = selected_probs / selected_probs.sum()\n    \n    return selected_indices, selected_probs\n\ndef generate_bigram_predictions(test_df: pd.DataFrame, per_user_bigram_dict: dict,\n                               per_user_unigram_dict: dict, gm_x: int, gm_y: int,\n                               top_p=None) -> pd.DataFrame:\n    \"\"\"\n    Generate predictions using bigram model for each user.\n    For each test point, use the previous location to predict the next location.\n    Apply top-p sampling if specified.\n    Fallback to unigram model if no bigram history, then to global mean.\n    \"\"\"\n    model_name = f\"Bigram Model (top_p={top_p})\" if top_p else \"Bigram Model\"\n    print(f\"Generating {model_name} predictions ...\")\n    \n    pred_bigram = test_df[[\"uid\", \"d\", \"t\"]].copy()\n    pred_bigram[\"x_pred\"] = 0\n    pred_bigram[\"y_pred\"] = 0\n    \n    # Process each user separately to maintain sequence order\n    for uid, group in tqdm(test_df.groupby(\"uid\"), desc=f\"{model_name} sampling\"):\n        # Sort test points by day and time to maintain sequence\n        user_test = group.sort_values(['d', 't']).copy()\n        \n        if uid in per_user_bigram_dict and per_user_bigram_dict[uid]:\n            bigram_counter = per_user_bigram_dict[uid]\n            \n            # Get the last location from training data as starting context\n            # Use the most frequent location as initial context\n            if uid in per_user_unigram_dict:\n                unigram_counter = per_user_unigram_dict[uid]\n                prev_location = unigram_counter.most_common(1)[0][0]\n            else:\n                prev_location = (gm_x, gm_y)\n            \n            predictions = []\n            \n            for idx, row in user_test.iterrows():\n                # Find all bigrams that start with prev_location\n                next_locations = {}\n                for (loc1, loc2), freq in bigram_counter.items():\n                    if loc1 == prev_location:\n                        next_locations[loc2] = freq\n                \n                if next_locations:\n                    # Sample from next locations\n                    locations = list(next_locations.keys())\n                    frequencies = list(next_locations.values())\n                    total_freq = sum(frequencies)\n                    probabilities = [f / total_freq for f in frequencies]\n                    \n                    if top_p is not None:\n                        # Apply top-p sampling\n                        selected_indices, selected_probs = top_p_sampling(probabilities, top_p)\n                        selected_locations = [locations[i] for i in selected_indices]\n                        sampled_location = np.random.choice(len(selected_locations), p=selected_probs)\n                        next_location = selected_locations[sampled_location]\n                    else:\n                        # Regular sampling\n                        sampled_idx = np.random.choice(len(locations), p=probabilities)\n                        next_location = locations[sampled_idx]\n                    \n                    predictions.append(next_location)\n                    prev_location = next_location\n                else:\n                    # Fallback to unigram model\n                    if uid in per_user_unigram_dict:\n                        unigram_counter = per_user_unigram_dict[uid]\n                        locations = list(unigram_counter.keys())\n                        frequencies = list(unigram_counter.values())\n                        total_freq = sum(frequencies)\n                        probabilities = [f / total_freq for f in frequencies]\n                        sampled_idx = np.random.choice(len(locations), p=probabilities)\n                        next_location = locations[sampled_idx]\n                    else:\n                        next_location = (gm_x, gm_y)\n                    \n                    predictions.append(next_location)\n                    prev_location = next_location\n            \n            # Update predictions for this user\n            user_indices = user_test.index\n            pred_bigram.loc[user_indices, \"x_pred\"] = [pred[0] for pred in predictions]\n            pred_bigram.loc[user_indices, \"y_pred\"] = [pred[1] for pred in predictions]\n            \n        else:\n            # Fallback to unigram model for users without bigram data\n            if uid in per_user_unigram_dict:\n                location_counter = per_user_unigram_dict[uid]\n                locations = list(location_counter.keys())\n                frequencies = list(location_counter.values())\n                total_freq = sum(frequencies)\n                probabilities = [f / total_freq for f in frequencies]\n                \n                num_samples = len(user_test)\n                sampled_indices = np.random.choice(len(locations), size=num_samples, p=probabilities)\n                sampled_locations = [locations[i] for i in sampled_indices]\n                \n                user_indices = user_test.index\n                pred_bigram.loc[user_indices, \"x_pred\"] = [loc[0] for loc in sampled_locations]\n                pred_bigram.loc[user_indices, \"y_pred\"] = [loc[1] for loc in sampled_locations]\n            else:\n                # Final fallback to global mean\n                mask = pred_bigram[\"uid\"] == uid\n                pred_bigram.loc[mask, \"x_pred\"] = gm_x\n                pred_bigram.loc[mask, \"y_pred\"] = gm_y\n    \n    return pred_bigram.astype({\"x_pred\": \"int16\", \"y_pred\": \"int16\"})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T12:32:37.221723Z","iopub.execute_input":"2025-06-12T12:32:37.222197Z","iopub.status.idle":"2025-06-12T12:32:37.244241Z","shell.execute_reply.started":"2025-06-12T12:32:37.222160Z","shell.execute_reply":"2025-06-12T12:32:37.243235Z"}},"outputs":[],"execution_count":7},{"id":"32639f4f","cell_type":"code","source":"def process_city(city_code: str) -> dict:\n    \"\"\"\n    1. Compute train aggregates\n    2. Build test_df\n    3. Prepare gt_df\n    4. Build each baseline's pred_df (showing progress)\n    5. Evaluate GEO-BLEU (with tqdm inside evaluate_geobleu_parallel)\n    Returns dict: {baseline_name: GEO-BLEU score}.\n    \"\"\"\n    print(f\"\\n>>> Starting City {city_code}\")\n\n    # 1) Train aggregates\n    (gm_x, gm_y), (gmod_x, gmod_y), per_user_mean_df, per_user_mode_df, per_user_unigram_dict, per_user_bigram_dict = compute_train_aggregates(city_code)\n\n    # 2) Test DataFrame\n    test_df = build_test_dataframe(city_code)\n\n    # 3) Ground-truth DataFrame\n    gt_df = test_df.rename(columns={\"x\": \"x_gt\", \"y\": \"y_gt\"})[[\"uid\", \"d\", \"t\", \"x_gt\", \"y_gt\"]]\n\n    results = {}\n\n    # 4a) Global Mean Prediction\n    print(f\"City {city_code} -> Global Mean prediction ...\")\n    pred_gm = test_df[[\"uid\", \"d\", \"t\"]].copy()\n    pred_gm[\"x_pred\"] = gm_x\n    pred_gm[\"y_pred\"] = gm_y\n    score_gm = evaluate_geobleu_parallel(pred_gm, gt_df)\n    results[\"Global Mean\"] = round(score_gm, 5)\n    print(f\"Global Mean GEO-BLEU = {results['Global Mean']}\")\n\n    # 4b) Global Mode Prediction\n    print(f\"City {city_code} -> Global Mode prediction ...\")\n    pred_gmod = test_df[[\"uid\", \"d\", \"t\"]].copy()\n    pred_gmod[\"x_pred\"] = gmod_x\n    pred_gmod[\"y_pred\"] = gmod_y\n    score_gmod = evaluate_geobleu_parallel(pred_gmod, gt_df)\n    results[\"Global Mode\"] = round(score_gmod, 5)\n    print(f\"Global Mode GEO-BLEU = {results['Global Mode']}\")\n\n    # 4c) Per-User Mean Prediction\n    print(f\"City {city_code} -> Per-User Mean prediction ...\")\n    pred_pum = test_df[[\"uid\", \"d\", \"t\"]].copy()\n    pred_pum = pred_pum.join(per_user_mean_df, on=\"uid\", how=\"left\", rsuffix=\"_tmp\")\n    pred_pum = pred_pum.rename(columns={\"x\": \"x_pred\", \"y\": \"y_pred\"})\n    # Fallback for unseen users\n    pred_pum[\"x_pred\"] = pred_pum[\"x_pred\"].fillna(gm_x).astype(\"int16\")\n    pred_pum[\"y_pred\"] = pred_pum[\"y_pred\"].fillna(gm_y).astype(\"int16\")\n    score_pum = evaluate_geobleu_parallel(pred_pum, gt_df)\n    results[\"Per-User Mean\"] = round(score_pum, 5)\n    print(f\"Per-User Mean GEO-BLEU = {results['Per-User Mean']}\")\n\n    # 4d) Per-User Mode Prediction\n    print(f\"City {city_code} -> Per-User Mode prediction ...\")\n    pred_pumod = test_df[[\"uid\", \"d\", \"t\"]].copy()\n    pred_pumod = pred_pumod.join(per_user_mode_df, on=\"uid\", how=\"left\", rsuffix=\"_tmp\")\n    pred_pumod = pred_pumod.rename(columns={\"x\": \"x_pred\", \"y\": \"y_pred\"})\n    # Fallback for unseen users\n    pred_pumod[\"x_pred\"] = pred_pumod[\"x_pred\"].fillna(gmod_x).astype(\"int16\")\n    pred_pumod[\"y_pred\"] = pred_pumod[\"y_pred\"].fillna(gmod_y).astype(\"int16\")\n    score_pumod = evaluate_geobleu_parallel(pred_pumod, gt_df)\n    results[\"Per-User Mode\"] = round(score_pumod, 5)\n    print(f\"Per-User Mode GEO-BLEU = {results['Per-User Mode']}\")\n\n    # 4e) Unigram Model Prediction\n    print(f\"City {city_code} -> Unigram Model prediction ...\")\n    pred_unigram = generate_unigram_predictions(test_df, per_user_unigram_dict, gm_x, gm_y)\n    score_unigram = evaluate_geobleu_parallel(pred_unigram, gt_df)\n    results[\"Unigram Model\"] = round(score_unigram, 5)\n    print(f\"Unigram Model GEO-BLEU = {results['Unigram Model']}\")\n\n    # 4f) Bigram Model Prediction\n    print(f\"City {city_code} -> Bigram Model prediction ...\")\n    pred_bigram = generate_bigram_predictions(test_df, per_user_bigram_dict, per_user_unigram_dict, gm_x, gm_y)\n    score_bigram = evaluate_geobleu_parallel(pred_bigram, gt_df)\n    results[\"Bigram Model\"] = round(score_bigram, 5)\n    print(f\"Bigram Model GEO-BLEU = {results['Bigram Model']}\")\n\n    # 4g) Bigram Model with top_p=0.7 Prediction\n    print(f\"City {city_code} -> Bigram Model (top_p=0.7) prediction ...\")\n    pred_bigram_top_p = generate_bigram_predictions(test_df, per_user_bigram_dict, per_user_unigram_dict, gm_x, gm_y, top_p=0.7)\n    score_bigram_top_p = evaluate_geobleu_parallel(pred_bigram_top_p, gt_df)\n    results[\"Bigram Model (top_p=0.7)\"] = round(score_bigram_top_p, 5)\n    print(f\"Bigram Model (top_p=0.7) GEO-BLEU = {results['Bigram Model (top_p=0.7)']}\")\n\n    print(f\"<<< Finished City {city_code} with results: {results}\\n\")\n    return results\n","metadata":{"execution":{"iopub.status.busy":"2025-06-12T12:32:44.121584Z","iopub.execute_input":"2025-06-12T12:32:44.121890Z","iopub.status.idle":"2025-06-12T12:32:44.136138Z","shell.execute_reply.started":"2025-06-12T12:32:44.121864Z","shell.execute_reply":"2025-06-12T12:32:44.135213Z"},"trusted":true},"outputs":[],"execution_count":8},{"id":"6d2dccd9-5e58-4bdc-8f19-47d491c763b0","cell_type":"code","source":"def analyze_data_coverage(city_code: str) -> dict:\n    \"\"\"\n    Analyze masked vs unmasked entries in the dataset for a given city.\n    Returns statistics about data coverage across training and test periods.\n    \"\"\"\n    print(f\">>> Analyzing data coverage for City {city_code} ...\")\n    \n    path = os.path.join(DATA_DIR, f\"city_{city_code}_challengedata.csv\")\n    \n    # Initialize counters\n    stats = {\n        'train_total': 0,\n        'train_masked': 0,\n        'train_unmasked': 0,\n        'test_total': 0,\n        'test_masked': 0,\n        'test_unmasked': 0,\n        'unique_users': set(),\n        'train_users': set(),\n        'test_users': set()\n    }\n    \n    # Process data in chunks\n    for chunk in tqdm(pd.read_csv(path, usecols=COLUMNS, dtype=DTYPES, chunksize=CHUNK_SIZE),\n                      desc=f\"Analyzing coverage (City {city_code})\"):\n        \n        # Split into train and test\n        train_chunk = chunk[chunk[\"d\"] <= TRAIN_DAY_MAX]\n        test_chunk = chunk[chunk[\"d\"] >= TEST_DAY_MIN]\n        \n        # Update unique users\n        stats['unique_users'].update(chunk['uid'].unique())\n        \n        # Training data analysis\n        if not train_chunk.empty:\n            stats['train_total'] += len(train_chunk)\n            masked_train = (train_chunk[\"x\"] == MASK_VALUE) | (train_chunk[\"y\"] == MASK_VALUE)\n            stats['train_masked'] += masked_train.sum()\n            stats['train_unmasked'] += (~masked_train).sum()\n            stats['train_users'].update(train_chunk['uid'].unique())\n        \n        # Test data analysis\n        if not test_chunk.empty:\n            stats['test_total'] += len(test_chunk)\n            masked_test = (test_chunk[\"x\"] == MASK_VALUE) | (test_chunk[\"y\"] == MASK_VALUE)\n            stats['test_masked'] += masked_test.sum()\n            stats['test_unmasked'] += (~masked_test).sum()\n            stats['test_users'].update(test_chunk['uid'].unique())\n        \n        del chunk, train_chunk, test_chunk\n    \n    # Convert sets to counts\n    stats['unique_users'] = len(stats['unique_users'])\n    stats['train_users'] = len(stats['train_users'])\n    stats['test_users'] = len(stats['test_users'])\n    \n    # Calculate percentages\n    if stats['train_total'] > 0:\n        stats['train_masked_pct'] = (stats['train_masked'] / stats['train_total']) * 100\n        stats['train_unmasked_pct'] = (stats['train_unmasked'] / stats['train_total']) * 100\n    else:\n        stats['train_masked_pct'] = stats['train_unmasked_pct'] = 0\n    \n    if stats['test_total'] > 0:\n        stats['test_masked_pct'] = (stats['test_masked'] / stats['test_total']) * 100\n        stats['test_unmasked_pct'] = (stats['test_unmasked'] / stats['test_total']) * 100\n    else:\n        stats['test_masked_pct'] = stats['test_unmasked_pct'] = 0\n    \n    return stats\n\ndef print_coverage_summary(stats: dict, city_code: str):\n    \"\"\"Print a formatted summary of data coverage statistics.\"\"\"\n    print(f\"\\n=== Data Coverage Summary for City {city_code} ===\")\n    print(f\"Overall Statistics:\")\n    print(f\"   Total unique users: {stats['unique_users']:,}\")\n    print(f\"   Users in training: {stats['train_users']:,}\")\n    print(f\"   Users in testing: {stats['test_users']:,}\")\n    \n    print(f\"\\nTraining Period (Days 1-{TRAIN_DAY_MAX}):\")\n    print(f\"   Total entries: {stats['train_total']:,}\")\n    print(f\"   Unmasked entries: {stats['train_unmasked']:,} ({stats['train_unmasked_pct']:.2f}%)\")\n    print(f\"   Masked entries: {stats['train_masked']:,} ({stats['train_masked_pct']:.2f}%)\")\n    \n    print(f\"\\nTest Period (Days {TEST_DAY_MIN}+):\")\n    print(f\"   Total entries: {stats['test_total']:,}\")\n    print(f\"   Unmasked entries: {stats['test_unmasked']:,} ({stats['test_unmasked_pct']:.2f}%)\")\n    print(f\"   Masked entries: {stats['test_masked']:,} ({stats['test_masked_pct']:.2f}%)\")\n    \n    if stats['test_unmasked'] > 0:\n        print(f\"\\nEvaluation will be performed on {stats['test_unmasked']:,} test entries\")\n    else:\n        print(f\"\\nNo unmasked test entries found!\")\n\n# Run coverage analysis for all cities\nprint(\"=\" * 60)\nprint(\"DATA COVERAGE ANALYSIS\")\nprint(\"=\" * 60)\n\nall_coverage_stats = {}\nfor city in [\"A\", \"B\", \"C\", \"D\"]:\n    coverage_stats = analyze_data_coverage(city)\n    all_coverage_stats[city] = coverage_stats\n    print_coverage_summary(coverage_stats, city)\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2025-06-12T10:05:21.446526Z","iopub.execute_input":"2025-06-12T10:05:21.446839Z","iopub.status.idle":"2025-06-12T10:06:42.545633Z","shell.execute_reply.started":"2025-06-12T10:05:21.446818Z","shell.execute_reply":"2025-06-12T10:06:42.544528Z"},"trusted":true},"outputs":[{"name":"stdout","text":"============================================================\nDATA COVERAGE ANALYSIS\n============================================================\n>>> Analyzing data coverage for City A ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Analyzing coverage (City A): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bc626d3d37247ceaf9a32f8ebab286a"}},"metadata":{}},{"name":"stdout","text":"\n=== Data Coverage Summary for City A ===\nOverall Statistics:\n   Total unique users: 150,000\n   Users in training: 150,000\n   Users in testing: 150,000\n\nTraining Period (Days 1-60):\n   Total entries: 67,862,502\n   Unmasked entries: 67,862,502 (100.00%)\n   Masked entries: 0 (0.00%)\n\nTest Period (Days 61+):\n   Total entries: 19,179,916\n   Unmasked entries: 18,859,525 (98.33%)\n   Masked entries: 320,391 (1.67%)\n\nEvaluation will be performed on 18,859,525 test entries\n\n>>> Analyzing data coverage for City B ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Analyzing coverage (City B): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"639a3016b11a4511b51b584dc70f8b80"}},"metadata":{}},{"name":"stdout","text":"\n=== Data Coverage Summary for City B ===\nOverall Statistics:\n   Total unique users: 30,000\n   Users in training: 30,000\n   Users in testing: 30,000\n\nTraining Period (Days 1-60):\n   Total entries: 14,194,433\n   Unmasked entries: 14,194,433 (100.00%)\n   Masked entries: 0 (0.00%)\n\nTest Period (Days 61+):\n   Total entries: 4,002,560\n   Unmasked entries: 3,627,062 (90.62%)\n   Masked entries: 375,498 (9.38%)\n\nEvaluation will be performed on 3,627,062 test entries\n\n>>> Analyzing data coverage for City C ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Analyzing coverage (City C): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcd0bd44b71b4af89a90baeffd25ce46"}},"metadata":{}},{"name":"stdout","text":"\n=== Data Coverage Summary for City C ===\nOverall Statistics:\n   Total unique users: 25,000\n   Users in training: 25,000\n   Users in testing: 25,000\n\nTraining Period (Days 1-60):\n   Total entries: 11,226,812\n   Unmasked entries: 11,226,812 (100.00%)\n   Masked entries: 0 (0.00%)\n\nTest Period (Days 61+):\n   Total entries: 3,248,335\n   Unmasked entries: 2,953,708 (90.93%)\n   Masked entries: 294,627 (9.07%)\n\nEvaluation will be performed on 2,953,708 test entries\n\n>>> Analyzing data coverage for City D ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Analyzing coverage (City D): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f186d676ba0049e18c029b9c35b30464"}},"metadata":{}},{"name":"stdout","text":"\n=== Data Coverage Summary for City D ===\nOverall Statistics:\n   Total unique users: 20,000\n   Users in training: 20,000\n   Users in testing: 20,000\n\nTraining Period (Days 1-60):\n   Total entries: 9,358,783\n   Unmasked entries: 9,358,783 (100.00%)\n   Masked entries: 0 (0.00%)\n\nTest Period (Days 61+):\n   Total entries: 2,671,295\n   Unmasked entries: 2,361,882 (88.42%)\n   Masked entries: 309,413 (11.58%)\n\nEvaluation will be performed on 2,361,882 test entries\n\n","output_type":"stream"}],"execution_count":9},{"id":"99f60ca3-2bac-44a9-8333-2504917c34d8","cell_type":"code","source":"methods = [\"Global Mean\", \"Global Mode\", \"Per-User Mean\", \"Per-User Mode\", \"Unigram Model\", \"Bigram Model\", \"Bigram Model (top_p=0.7)\"]\nall_scores = {method: [] for method in methods}\n\nfor city in CITIES:\n    city_scores = process_city(city)\n    for method in methods:\n        all_scores[method].append(city_scores[method])\n\ndf_results = pd.DataFrame(\n    all_scores,\n    index=[f\"City {c}\" for c in CITIES]\n).T\ndf_results[\"Average\"] = df_results.mean(axis=1)\n\nprint(\"\\n=== Final GEO-BLEU Scores ===\")\ndisplay(df_results)\n","metadata":{"execution":{"iopub.status.busy":"2025-06-12T06:26:44.153100Z","iopub.execute_input":"2025-06-12T06:26:44.153424Z","execution_failed":"2025-06-12T09:59:43.269Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n>>> Starting City B\n>>> Computing train aggregates for City B ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading chunks (City B): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dabd0653d727457c850ed2008a4f8723"}},"metadata":{}},{"name":"stdout","text":"Train aggregates done: GM=(104,64), GMODE=(90,49), 30000 users' means, 30000 users' modes, 30000 users' unigrams, 30000 users' bigrams.\n>>> Building test DataFrame for City B ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading test chunks (City B): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eb65607c9c34e85a957df47353b81d5"}},"metadata":{}},{"name":"stdout","text":"Test DataFrame built: shape = (3627062, 5)\nCity B -> Global Mean prediction ...\n    ▶ Evaluating GEO-BLEU on 27000 users...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      GEO-BLEU:   0%|          | 0/27000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4e05977079644cc85e1c3901dedc31b"}},"metadata":{}},{"name":"stdout","text":"Global Mean GEO-BLEU = 0.00025\nCity B -> Global Mode prediction ...\n    ▶ Evaluating GEO-BLEU on 27000 users...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      GEO-BLEU:   0%|          | 0/27000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"802555aa2e07406bb518157a7f235c80"}},"metadata":{}},{"name":"stdout","text":"Global Mode GEO-BLEU = 0.00419\nCity B -> Per-User Mean prediction ...\n    ▶ Evaluating GEO-BLEU on 27000 users...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      GEO-BLEU:   0%|          | 0/27000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b96b2557e2e845fc8d3840e9643e8120"}},"metadata":{}},{"name":"stdout","text":"Per-User Mean GEO-BLEU = 0.01778\nCity B -> Per-User Mode prediction ...\n    ▶ Evaluating GEO-BLEU on 27000 users...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      GEO-BLEU:   0%|          | 0/27000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10dd9e7b3dc242c4ae17e023f128f070"}},"metadata":{}},{"name":"stdout","text":"Per-User Mode GEO-BLEU = 0.0842\nCity B -> Unigram Model prediction ...\nGenerating Unigram predictions ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unigram sampling:   0%|          | 0/27000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88b0a19c36c146feb6c2697397470134"}},"metadata":{}},{"name":"stdout","text":"    ▶ Evaluating GEO-BLEU on 27000 users...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      GEO-BLEU:   0%|          | 0/27000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cbe6f7a8a2c4e29805dd69222f240b1"}},"metadata":{}},{"name":"stdout","text":"Unigram Model GEO-BLEU = 0.04105\nCity B -> Bigram Model prediction ...\nGenerating Bigram Model predictions ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Bigram Model sampling:   0%|          | 0/27000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"576dca6a9dd145568742351caf17101b"}},"metadata":{}},{"name":"stdout","text":"    ▶ Evaluating GEO-BLEU on 27000 users...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      GEO-BLEU:   0%|          | 0/27000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f36f95ad89c34cd797aa637ec16ff28b"}},"metadata":{}},{"name":"stdout","text":"Bigram Model GEO-BLEU = 0.05974\nCity B -> Bigram Model (top_p=0.7) prediction ...\nGenerating Bigram Model (top_p=0.7) predictions ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Bigram Model (top_p=0.7) sampling:   0%|          | 0/27000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f40c4617be294cf4b9ab67418920a219"}},"metadata":{}}],"execution_count":null},{"id":"e944ad53-2193-4928-9268-5e7b7d4f3dd6","cell_type":"code","source":"methods = [\"Bigram Model (top_p=0.7)\"]\nall_scores = {method: [] for method in methods}\n\nfor city in [\"B\"]:\n    city_scores = process_city(city)\n    for method in methods:\n        all_scores[method].append(city_scores[method])\n\ndf_results = pd.DataFrame(\n    all_scores,\n    index=[f\"City {c}\" for c in CITIES]\n).T\ndf_results[\"Average\"] = df_results.mean(axis=1)\n\nprint(\"\\n=== Final GEO-BLEU Scores ===\")\ndisplay(df_results)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:17:18.943Z"}},"outputs":[],"execution_count":null},{"id":"f913adf3-5934-427f-95a8-e732a2271821","cell_type":"code","source":"methods = [\"Global Mean\", \"Global Mode\", \"Per-User Mean\", \"Per-User Mode\", \"Unigram Model\", \"Bigram Model\", \"Bigram Model (top_p=0.7)\"]\nall_scores = {method: [] for method in methods}\n\nfor city in [\"C\"]:\n    city_scores = process_city(city)\n    for method in methods:\n        all_scores[method].append(city_scores[method])\n\ndf_results = pd.DataFrame(\n    all_scores,\n    index=[f\"City {c}\" for c in CITIES]\n).T\ndf_results[\"Average\"] = df_results.mean(axis=1)\n\nprint(\"\\n=== Final GEO-BLEU Scores ===\")\ndisplay(df_results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T10:06:42.546960Z","iopub.execute_input":"2025-06-12T10:06:42.547346Z","execution_failed":"2025-06-12T12:17:18.942Z"}},"outputs":[{"name":"stdout","text":"\n>>> Starting City C\n>>> Computing train aggregates for City C ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading chunks (City C): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a94d91c2a7b4cc084937027bd841645"}},"metadata":{}},{"name":"stdout","text":"Train aggregates done: GM=(96,139), GMODE=(103,133), 25000 users' means, 25000 users' modes, 25000 users' unigrams, 25000 users' bigrams.\n>>> Building test DataFrame for City C ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading test chunks (City C): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23c42970cdef477fb64e3cf6ab54daac"}},"metadata":{}},{"name":"stdout","text":"Test DataFrame built: shape = (2953708, 5)\nCity C -> Global Mean prediction ...\n    ▶ Evaluating GEO-BLEU on 22000 users...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      GEO-BLEU:   0%|          | 0/22000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b33e53bf3c9413c93e496347dd7fd47"}},"metadata":{}},{"name":"stdout","text":"Global Mean GEO-BLEU = 0.00107\nCity C -> Global Mode prediction ...\n    ▶ Evaluating GEO-BLEU on 22000 users...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      GEO-BLEU:   0%|          | 0/22000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbc86bc5bc094bb5b0d8f4b1ab414282"}},"metadata":{}},{"name":"stdout","text":"Global Mode GEO-BLEU = 0.00493\nCity C -> Per-User Mean prediction ...\n    ▶ Evaluating GEO-BLEU on 22000 users...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      GEO-BLEU:   0%|          | 0/22000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd5ac6021cd04c25a5b10a9e018fc5f9"}},"metadata":{}},{"name":"stdout","text":"Per-User Mean GEO-BLEU = 0.01588\nCity C -> Per-User Mode prediction ...\n    ▶ Evaluating GEO-BLEU on 22000 users...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      GEO-BLEU:   0%|          | 0/22000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30fc4f050309441287857047a3606795"}},"metadata":{}},{"name":"stdout","text":"Per-User Mode GEO-BLEU = 0.08338\nCity C -> Unigram Model prediction ...\nGenerating Unigram predictions ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unigram sampling:   0%|          | 0/22000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62b8042f3eb145919abbbefd96671a8d"}},"metadata":{}},{"name":"stdout","text":"    ▶ Evaluating GEO-BLEU on 22000 users...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      GEO-BLEU:   0%|          | 0/22000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61d0f7dd0fbe4c858d9bbf78065f8c81"}},"metadata":{}}],"execution_count":null},{"id":"2eb887e9-df38-47b0-8602-72fa689645bd","cell_type":"code","source":"methods = [\"Unigram Model\", \"Bigram Model\", \"Bigram Model (top_p=0.7)\"]\nall_scores = {method: [] for method in methods}\n\nfor city in [\"C\"]:\n    city_scores = process_city(city)\n    for method in methods:\n        all_scores[method].append(city_scores[method])\n\ndf_results = pd.DataFrame(\n    all_scores,\n    index=[f\"City {c}\" for c in CITIES]\n).T\ndf_results[\"Average\"] = df_results.mean(axis=1)\n\nprint(\"\\n=== Final GEO-BLEU Scores ===\")\ndisplay(df_results)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"de617ebc-b2f3-48de-a617-4b613ef63633","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"704649de-6f06-431f-9307-41b98ae152ef","cell_type":"code","source":"methods = [\"Bigram Model\", \"Bigram Model (top_p=0.7)\"]\nall_scores = {method: [] for method in methods}\n\nfor city in [\"D\"]:\n    city_scores = process_city(city)\n    for method in methods:\n        all_scores[method].append(city_scores[method])\n\ndf_results = pd.DataFrame(\n    all_scores,\n    index=[f\"City {c}\" for c in CITIES]\n).T\ndf_results[\"Average\"] = df_results.mean(axis=1)\n\nprint(\"\\n=== Final GEO-BLEU Scores ===\")\ndisplay(df_results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T12:33:18.450299Z","iopub.execute_input":"2025-06-12T12:33:18.450576Z","execution_failed":"2025-06-12T13:14:41.500Z"}},"outputs":[{"name":"stdout","text":"\n>>> Starting City D\n>>> Computing train aggregates for City D ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading chunks (City D): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fbd8baf47cb4150a51989ec330f1d25"}},"metadata":{}},{"name":"stdout","text":"Train aggregates done: GM=(110,91), GMODE=(142,107), 20000 users' means, 20000 users' modes, 20000 users' unigrams, 20000 users' bigrams.\n>>> Building test DataFrame for City D ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading test chunks (City D): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be21121242b34ad8bac9d72c6fce5504"}},"metadata":{}},{"name":"stdout","text":"Test DataFrame built: shape = (2361882, 5)\nCity D -> Global Mean prediction ...\n    ▶ Evaluating GEO-BLEU on 17000 users...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      GEO-BLEU:   0%|          | 0/17000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9f9506630ce4beda715e8c94e9a467e"}},"metadata":{}}],"execution_count":null},{"id":"097ea944-7855-43c6-814e-b3c6dbbafebd","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}