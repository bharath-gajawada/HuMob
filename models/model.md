| Model Name          | Architecture                                                            | Paper Link                                                                 | Code/Model Link                                                     | Additional Info                                                                                                                                                             |
|---------------------|-------------------------------------------------------------------------|-----------------------------------------------------------------------------|----------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **ST-MoE-BERT (Top Performer)** | Spatial-Temporal Mixture-of-Experts with a BERT-like Transformer.          | [arXiv:2410.14099](https://arxiv.org/abs/2410.14099)                        | [GitHub](https://github.com/he-h/ST-MoE-BERT)                        | Developed for the SIGSPATIAL HuMob Challenge. The Mixture-of-Experts design handles diverse mobility patterns across cities, making it directly relevant to GIS Cup 2025.    |
| **LP-BERT (Top Performer)**     | Transformer with attention mechanism, based on BERT.                      | [HuMob2023 Final Report (PDF)](https://db.uclab.jp/download/?name=Humob2023_final.pdf) | *(No official winning code available)*                             | Used in a top-10 solution for a previous HuMob Challenge. Applies Transformer power to model complex spatio-temporal sequences.                                             |
| **DeepMove (Underrated)**       | Attentional Recurrent Network with GRU/LSTM.                             | [ACM Digital Library](https://dl.acm.org/doi/abs/10.1145/3178876.3186058)  | [GitHub](https://github.com/Logan-Lin/DeepMove-pytorch)             | A foundational model for mobility prediction. Captures both short-term sequential dependencies and long-term periodic patterns using attention.                             |
| **DisasterMobLLM (Underrated)**| LLM-based framework with RAG (Retrieval-Augmented Generation).           | [arXiv:2507.19737](https://arxiv.org/abs/2507.19737)                        | [GitHub](https://github.com/tsinghua-fib-lab/DisasterMobLLM)        | Uses LLMs to reason about mobility intentions and transfer knowledge across cities. Focused on disasters but adaptable for GIS Cup scenarios.                               |
| **UniMob (Underrated)**        | Diffusion Transformer with a multi-view mobility tokenizer.              | [UniMob Paper (PDF)](https://github.com/tsinghua-fib-lab/UniMob/blob/main/UniMob_arxiv.pdf) | [GitHub](https://github.com/tsinghua-fib-lab/UniMob)                | Unifies trajectory prediction and crowd flow forecasting. Effective for large datasets and modeling both individual and collective mobility patterns.                        |
| **WS-BiGNN (Underrated)**      | Bipartite Graph Neural Network (GNN) with weighted similarity.           | [arXiv:2202.12450](https://arxiv.org/abs/2202.12450)                        | *(No official public repository)*                                   | Re-frames mobility as a link prediction task between users and locations. Great for irregular mobility patterns. Requires GNN library (e.g., PyTorch Geometric) to implement. |
